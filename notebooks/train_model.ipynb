{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd555848",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch as T\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "from datasets.wcst import WCST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "389721a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "wcst = WCST(10)\n",
    "\n",
    "device = T.device('cuda' if T.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08945224",
   "metadata": {},
   "source": [
    "## Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "25ea2ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ba4f2c",
   "metadata": {},
   "source": [
    "### 1. Dataset Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e95fdcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e47ffd5",
   "metadata": {},
   "source": [
    "### 2. Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "365835c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_targets = T.load('../datasets/train_dataset.pt')\n",
    "train_dataset_loader = DataLoader(TensorDataset(train_data, train_targets), batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "validation_data, validation_targets = T.load('../datasets/validation_dataset.pt')\n",
    "validation_dataset_loader = DataLoader(TensorDataset(validation_data, validation_targets), batch_size=BATCH_SIZE)\n",
    "\n",
    "test_data, test_targets = T.load('../datasets/test_dataset.pt')\n",
    "test_dataset_loader  = DataLoader(TensorDataset(test_data, test_targets), batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f524267",
   "metadata": {},
   "source": [
    "## Transformer Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d3a8bc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.transformer import Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7739c3fe",
   "metadata": {},
   "source": [
    "### 1. Transformer Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "606c1008",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCABULARY_SIZE = 70 # Cards + Categories + 'SEP' + 'EOS'\n",
    "EMBEDDING_SIZE = 128\n",
    "N_ATTENTION_HEADS = 4\n",
    "N_BLOCKS = 3\n",
    "MAX_SEQUENCE_LENGTH = 10\n",
    "FF_DIMS = 256\n",
    "DROPOUT_PROB = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab9fc96",
   "metadata": {},
   "source": [
    "### 2. Transformer Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9f25ee98",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    VOCABULARY_SIZE, VOCABULARY_SIZE, EMBEDDING_SIZE, N_ATTENTION_HEADS,\n",
    "    N_BLOCKS, MAX_SEQUENCE_LENGTH, FF_DIMS, DROPOUT_PROB, device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7489333",
   "metadata": {},
   "source": [
    "## Training Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "42f3ebdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c5d23d",
   "metadata": {},
   "source": [
    "### 1. Train, Validate, Evaluate Model Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3a8ee8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    train_loader: DataLoader,\n",
    "    validation_loader: DataLoader,\n",
    "    model: Transformer,\n",
    "    criterion: nn.CrossEntropyLoss,\n",
    "    optimizer: optim.Optimizer,\n",
    "    max_epochs: int = 20,\n",
    "    device: str | T.device = \"cpu\",\n",
    "    patience: int = 3,\n",
    "):\n",
    "    best_val_loss = np.inf\n",
    "    patience_counter = 0\n",
    "\n",
    "    train_losses, train_accs, train_perplexities = [], [], []\n",
    "    val_losses, val_accs, val_perplexities = [], [], []\n",
    "    best_model_state = model.state_dict()\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{max_epochs}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "        # --- Training ---\n",
    "        model.train()\n",
    "        epoch_train_losses = []\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        for batch_idx, (X, target) in enumerate(train_loader):\n",
    "            encoder_inputs, target = X.to(device), target.to(device)\n",
    "\n",
    "            # decoder_inputs = all tokens except last (category)\n",
    "            decoder_inputs = target[:, :-1]  # e.g. [Trial] SEP\n",
    "            # decoder_targets = last token only (category)\n",
    "            decoder_targets = target[:, -1]  # shape [batch]\n",
    "\n",
    "            # Forward pass\n",
    "            logits = model(encoder_inputs, decoder_inputs)  # [batch, seq_len, vocab]\n",
    "            logits = logits[:, -1, :]  # only the final step prediction [batch, vocab]\n",
    "\n",
    "            loss = criterion(logits, decoder_targets)\n",
    "\n",
    "            # Backprop\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accuracy\n",
    "            preds = logits.argmax(dim=1)\n",
    "            total_correct += (preds == decoder_targets).sum().item()\n",
    "            total_samples += decoder_targets.size(0)\n",
    "\n",
    "            epoch_train_losses.append(loss.item())\n",
    "\n",
    "            if batch_idx % 100 == 0 or batch_idx == len(train_loader) - 1:\n",
    "                print(f\"Train Batch {batch_idx+1}/{len(train_loader)} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "        train_loss = np.mean(epoch_train_losses)\n",
    "        train_acc = total_correct / total_samples\n",
    "        train_perplexity = np.exp(train_loss)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        train_perplexities.append(train_perplexity)\n",
    "\n",
    "        print(f\"[Epoch {epoch+1}] Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Train Perplexity: {train_perplexity:.4f}\")\n",
    "\n",
    "        # --- Validation ---\n",
    "        model.eval()\n",
    "        val_batch_losses = []\n",
    "        val_correct = 0\n",
    "        val_samples = 0\n",
    "\n",
    "        with T.no_grad():\n",
    "            for X, target in validation_loader:\n",
    "                encoder_input, target = X.to(device), target.to(device)\n",
    "                decoder_input = target[:, :-1]\n",
    "                decoder_target = target[:, -1]\n",
    "\n",
    "                logits = model(encoder_input, decoder_input)\n",
    "                logits = logits[:, -1, :]  # only final prediction\n",
    "\n",
    "                loss = criterion(logits, decoder_target)\n",
    "\n",
    "                preds = logits.argmax(dim=1)\n",
    "                val_correct += (preds == decoder_target).sum().item()\n",
    "                val_samples += decoder_target.size(0)\n",
    "\n",
    "                val_batch_losses.append(loss.item())\n",
    "\n",
    "        val_loss = np.mean(val_batch_losses)\n",
    "        val_acc = val_correct / val_samples\n",
    "        val_perplexity = np.exp(val_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_acc)\n",
    "        val_perplexities.append(val_perplexity)\n",
    "\n",
    "        print(f\"[Epoch {epoch+1}] Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | Val Perplexity: {val_perplexity:.4f}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "        # --- Early Stopping ---\n",
    "        # if val_loss < best_val_loss:\n",
    "        #     best_val_loss = val_loss\n",
    "        #     patience_counter = 0\n",
    "        #     best_model_state = model.state_dict()\n",
    "        #     print(f\"Validation loss improved â€” saving model (Loss: {val_loss:.4f})\")\n",
    "        # else:\n",
    "        #     patience_counter += 1\n",
    "        #     print(f\"No improvement ({patience_counter}/{patience})\")\n",
    "        #     if patience_counter >= patience:\n",
    "        #         print(\"\\nEarly stopping triggered. Restoring best model.\")\n",
    "        #         model.load_state_dict(best_model_state)\n",
    "        #         break\n",
    "\n",
    "    print(\"\\nTraining complete\")\n",
    "\n",
    "    return {\n",
    "        \"train_losses\": train_losses,\n",
    "        \"train_accs\": train_accs,\n",
    "        \"train_perplexities\": train_perplexities,\n",
    "        \"val_losses\": val_losses,\n",
    "        \"val_accs\": val_accs,\n",
    "        \"val_perplexities\": val_perplexities,\n",
    "        \"best_val_loss\": best_val_loss,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "44864dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(test_loader: DataLoader, model: Transformer, criterion: nn.CrossEntropyLoss, device: str | T.device = \"cpu\"):\n",
    "\n",
    "    model.eval()\n",
    "    test_batch_losses = []\n",
    "    test_correct = 0\n",
    "    test_tokens = 0\n",
    "\n",
    "    with T.no_grad():\n",
    "        for X, target in test_loader:\n",
    "            encoder_input, target = X.to(device), target.to(device)\n",
    "            decoder_input = target[:, :-1]\n",
    "            decoder_target = target[:, -1]\n",
    "\n",
    "            logits = model(encoder_input, decoder_input)[:, -1, :]\n",
    "            loss = criterion(logits, decoder_target)\n",
    "\n",
    "            preds = logits.argmax(dim=1)\n",
    "            test_correct += (preds == decoder_target).sum().item()\n",
    "            test_tokens += decoder_target.size(0)\n",
    "\n",
    "            test_batch_losses.append(loss.item())\n",
    "\n",
    "    test_loss = np.mean(test_batch_losses)\n",
    "    test_acc = test_correct / test_tokens\n",
    "    test_perplexity = np.exp(test_loss)\n",
    "\n",
    "    return {\n",
    "        \"test_loss\": test_loss,\n",
    "        \"test_acc\": test_acc,\n",
    "        \"test_perplexity\": test_perplexity\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d863c67",
   "metadata": {},
   "source": [
    "### 2. Train Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "37f53def",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 3e-4\n",
    "BATCH_SIZE = 64\n",
    "WEIGHT_DECAY = 1e-2\n",
    "WARMUP_STEPS = 400\n",
    "LABEL_SMOOTHING = 0.1\n",
    "MAX_EPOCHS = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a5c8f99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/40\n",
      "----------------------------------------\n",
      "Train Batch 1/150 | Loss: 4.2049\n",
      "Train Batch 101/150 | Loss: 1.9811\n",
      "Train Batch 150/150 | Loss: 1.9683\n",
      "[Epoch 1] Train Loss: 2.0715 | Train Acc: 0.2492 | Train Perplexity: 7.9365\n",
      "[Epoch 1] Val Loss: 1.9718 | Val Acc: 0.2650 | Val Perplexity: 7.1833\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 2/40\n",
      "----------------------------------------\n",
      "Train Batch 1/150 | Loss: 1.9962\n",
      "Train Batch 101/150 | Loss: 1.9583\n",
      "Train Batch 150/150 | Loss: 1.9453\n",
      "[Epoch 2] Train Loss: 1.9744 | Train Acc: 0.2490 | Train Perplexity: 7.2022\n",
      "[Epoch 2] Val Loss: 1.9748 | Val Acc: 0.2453 | Val Perplexity: 7.2050\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 3/40\n",
      "----------------------------------------\n",
      "Train Batch 1/150 | Loss: 1.9598\n",
      "Train Batch 101/150 | Loss: 1.9765\n",
      "Train Batch 150/150 | Loss: 1.9745\n",
      "[Epoch 3] Train Loss: 1.9736 | Train Acc: 0.2580 | Train Perplexity: 7.1969\n",
      "[Epoch 3] Val Loss: 1.9643 | Val Acc: 0.2669 | Val Perplexity: 7.1300\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 4/40\n",
      "----------------------------------------\n",
      "Train Batch 1/150 | Loss: 1.9419\n",
      "Train Batch 101/150 | Loss: 1.9595\n",
      "Train Batch 150/150 | Loss: 1.9862\n",
      "[Epoch 4] Train Loss: 1.9723 | Train Acc: 0.2572 | Train Perplexity: 7.1871\n",
      "[Epoch 4] Val Loss: 1.9703 | Val Acc: 0.2456 | Val Perplexity: 7.1730\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 5/40\n",
      "----------------------------------------\n",
      "Train Batch 1/150 | Loss: 1.9587\n",
      "Train Batch 101/150 | Loss: 1.9774\n",
      "Train Batch 150/150 | Loss: 1.9544\n",
      "[Epoch 5] Train Loss: 1.9687 | Train Acc: 0.2623 | Train Perplexity: 7.1612\n",
      "[Epoch 5] Val Loss: 1.9668 | Val Acc: 0.2475 | Val Perplexity: 7.1476\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 6/40\n",
      "----------------------------------------\n",
      "Train Batch 1/150 | Loss: 1.9702\n",
      "Train Batch 101/150 | Loss: 1.9718\n",
      "Train Batch 150/150 | Loss: 1.9678\n",
      "[Epoch 6] Train Loss: 1.9707 | Train Acc: 0.2553 | Train Perplexity: 7.1756\n",
      "[Epoch 6] Val Loss: 1.9664 | Val Acc: 0.2512 | Val Perplexity: 7.1447\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 7/40\n",
      "----------------------------------------\n",
      "Train Batch 1/150 | Loss: 1.9676\n",
      "Train Batch 101/150 | Loss: 1.9596\n",
      "Train Batch 150/150 | Loss: 1.9730\n",
      "[Epoch 7] Train Loss: 1.9675 | Train Acc: 0.2674 | Train Perplexity: 7.1530\n",
      "[Epoch 7] Val Loss: 1.9730 | Val Acc: 0.2519 | Val Perplexity: 7.1921\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 8/40\n",
      "----------------------------------------\n",
      "Train Batch 1/150 | Loss: 1.9784\n",
      "Train Batch 101/150 | Loss: 1.9762\n",
      "Train Batch 150/150 | Loss: 1.9449\n",
      "[Epoch 8] Train Loss: 1.9651 | Train Acc: 0.2750 | Train Perplexity: 7.1354\n",
      "[Epoch 8] Val Loss: 1.9677 | Val Acc: 0.2569 | Val Perplexity: 7.1542\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 9/40\n",
      "----------------------------------------\n",
      "Train Batch 1/150 | Loss: 1.9542\n",
      "Train Batch 101/150 | Loss: 1.9436\n",
      "Train Batch 150/150 | Loss: 1.9704\n",
      "[Epoch 9] Train Loss: 1.9609 | Train Acc: 0.2804 | Train Perplexity: 7.1054\n",
      "[Epoch 9] Val Loss: 1.9859 | Val Acc: 0.2447 | Val Perplexity: 7.2856\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 10/40\n",
      "----------------------------------------\n",
      "Train Batch 1/150 | Loss: 1.9903\n",
      "Train Batch 101/150 | Loss: 1.9790\n",
      "Train Batch 150/150 | Loss: 1.9526\n",
      "[Epoch 10] Train Loss: 1.9615 | Train Acc: 0.2859 | Train Perplexity: 7.1099\n",
      "[Epoch 10] Val Loss: 1.9725 | Val Acc: 0.2591 | Val Perplexity: 7.1889\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 11/40\n",
      "----------------------------------------\n",
      "Train Batch 1/150 | Loss: 1.9520\n",
      "Train Batch 101/150 | Loss: 1.9531\n",
      "Train Batch 150/150 | Loss: 1.9482\n",
      "[Epoch 11] Train Loss: 1.9554 | Train Acc: 0.2926 | Train Perplexity: 7.0669\n",
      "[Epoch 11] Val Loss: 1.9765 | Val Acc: 0.2584 | Val Perplexity: 7.2177\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 12/40\n",
      "----------------------------------------\n",
      "Train Batch 1/150 | Loss: 1.9635\n",
      "Train Batch 101/150 | Loss: 1.9271\n",
      "Train Batch 150/150 | Loss: 1.9504\n",
      "[Epoch 12] Train Loss: 1.9536 | Train Acc: 0.2978 | Train Perplexity: 7.0538\n",
      "[Epoch 12] Val Loss: 1.9969 | Val Acc: 0.2569 | Val Perplexity: 7.3661\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 13/40\n",
      "----------------------------------------\n",
      "Train Batch 1/150 | Loss: 1.9795\n",
      "Train Batch 101/150 | Loss: 1.9611\n",
      "Train Batch 150/150 | Loss: 1.9743\n",
      "[Epoch 13] Train Loss: 1.9513 | Train Acc: 0.3052 | Train Perplexity: 7.0375\n",
      "[Epoch 13] Val Loss: 1.9846 | Val Acc: 0.2581 | Val Perplexity: 7.2764\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 14/40\n",
      "----------------------------------------\n",
      "Train Batch 1/150 | Loss: 1.9136\n",
      "Train Batch 101/150 | Loss: 1.9348\n",
      "Train Batch 150/150 | Loss: 1.9651\n",
      "[Epoch 14] Train Loss: 1.9444 | Train Acc: 0.3116 | Train Perplexity: 6.9896\n",
      "[Epoch 14] Val Loss: 1.9787 | Val Acc: 0.2534 | Val Perplexity: 7.2336\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 15/40\n",
      "----------------------------------------\n",
      "Train Batch 1/150 | Loss: 1.9307\n",
      "Train Batch 101/150 | Loss: 1.9124\n",
      "Train Batch 150/150 | Loss: 1.9825\n",
      "[Epoch 15] Train Loss: 1.9409 | Train Acc: 0.3155 | Train Perplexity: 6.9650\n",
      "[Epoch 15] Val Loss: 1.9958 | Val Acc: 0.2628 | Val Perplexity: 7.3578\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 16/40\n",
      "----------------------------------------\n",
      "Train Batch 1/150 | Loss: 1.9148\n",
      "Train Batch 101/150 | Loss: 1.9645\n",
      "Train Batch 150/150 | Loss: 1.9446\n",
      "[Epoch 16] Train Loss: 1.9335 | Train Acc: 0.3280 | Train Perplexity: 6.9138\n",
      "[Epoch 16] Val Loss: 1.9972 | Val Acc: 0.2625 | Val Perplexity: 7.3684\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 17/40\n",
      "----------------------------------------\n",
      "Train Batch 1/150 | Loss: 1.9558\n",
      "Train Batch 101/150 | Loss: 1.9147\n",
      "Train Batch 150/150 | Loss: 1.9512\n",
      "[Epoch 17] Train Loss: 1.9300 | Train Acc: 0.3396 | Train Perplexity: 6.8894\n",
      "[Epoch 17] Val Loss: 2.0075 | Val Acc: 0.2616 | Val Perplexity: 7.4447\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 18/40\n",
      "----------------------------------------\n",
      "Train Batch 1/150 | Loss: 1.9034\n",
      "Train Batch 101/150 | Loss: 1.9016\n",
      "Train Batch 150/150 | Loss: 1.9348\n",
      "[Epoch 18] Train Loss: 1.9223 | Train Acc: 0.3467 | Train Perplexity: 6.8366\n",
      "[Epoch 18] Val Loss: 2.0093 | Val Acc: 0.2559 | Val Perplexity: 7.4579\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 19/40\n",
      "----------------------------------------\n",
      "Train Batch 1/150 | Loss: 1.9017\n",
      "Train Batch 101/150 | Loss: 1.9635\n",
      "Train Batch 150/150 | Loss: 1.9237\n",
      "[Epoch 19] Train Loss: 1.9126 | Train Acc: 0.3626 | Train Perplexity: 6.7704\n",
      "[Epoch 19] Val Loss: 2.0141 | Val Acc: 0.2662 | Val Perplexity: 7.4943\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 20/40\n",
      "----------------------------------------\n",
      "Train Batch 1/150 | Loss: 1.9067\n",
      "Train Batch 101/150 | Loss: 1.8897\n",
      "Train Batch 150/150 | Loss: 1.8642\n",
      "[Epoch 20] Train Loss: 1.9057 | Train Acc: 0.3616 | Train Perplexity: 6.7238\n",
      "[Epoch 20] Val Loss: 2.0182 | Val Acc: 0.2634 | Val Perplexity: 7.5248\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 21/40\n",
      "----------------------------------------\n",
      "Train Batch 1/150 | Loss: 1.9105\n",
      "Train Batch 101/150 | Loss: 2.0252\n",
      "Train Batch 150/150 | Loss: 1.9060\n",
      "[Epoch 21] Train Loss: 1.8948 | Train Acc: 0.3775 | Train Perplexity: 6.6515\n",
      "[Epoch 21] Val Loss: 2.0478 | Val Acc: 0.2609 | Val Perplexity: 7.7511\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 22/40\n",
      "----------------------------------------\n",
      "Train Batch 1/150 | Loss: 1.8457\n",
      "Train Batch 101/150 | Loss: 1.8730\n",
      "Train Batch 150/150 | Loss: 1.8947\n",
      "[Epoch 22] Train Loss: 1.8801 | Train Acc: 0.3951 | Train Perplexity: 6.5542\n",
      "[Epoch 22] Val Loss: 2.0599 | Val Acc: 0.2687 | Val Perplexity: 7.8455\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 23/40\n",
      "----------------------------------------\n",
      "Train Batch 1/150 | Loss: 1.8944\n",
      "Train Batch 101/150 | Loss: 1.9107\n",
      "Train Batch 150/150 | Loss: 1.8700\n",
      "[Epoch 23] Train Loss: 1.8713 | Train Acc: 0.3983 | Train Perplexity: 6.4966\n",
      "[Epoch 23] Val Loss: 2.0495 | Val Acc: 0.2528 | Val Perplexity: 7.7643\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 24/40\n",
      "----------------------------------------\n",
      "Train Batch 1/150 | Loss: 1.8729\n",
      "Train Batch 101/150 | Loss: 1.8879\n",
      "Train Batch 150/150 | Loss: 1.8600\n",
      "[Epoch 24] Train Loss: 1.8544 | Train Acc: 0.4131 | Train Perplexity: 6.3878\n",
      "[Epoch 24] Val Loss: 2.0803 | Val Acc: 0.2697 | Val Perplexity: 8.0072\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 25/40\n",
      "----------------------------------------\n",
      "Train Batch 1/150 | Loss: 1.9204\n",
      "Train Batch 101/150 | Loss: 1.8957\n",
      "Train Batch 150/150 | Loss: 1.8062\n",
      "[Epoch 25] Train Loss: 1.8405 | Train Acc: 0.4275 | Train Perplexity: 6.2997\n",
      "[Epoch 25] Val Loss: 2.1440 | Val Acc: 0.2641 | Val Perplexity: 8.5332\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 26/40\n",
      "----------------------------------------\n",
      "Train Batch 1/150 | Loss: 1.8494\n",
      "Train Batch 101/150 | Loss: 1.8460\n",
      "Train Batch 150/150 | Loss: 1.7552\n",
      "[Epoch 26] Train Loss: 1.8174 | Train Acc: 0.4425 | Train Perplexity: 6.1557\n",
      "[Epoch 26] Val Loss: 2.0911 | Val Acc: 0.2750 | Val Perplexity: 8.0942\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 27/40\n",
      "----------------------------------------\n",
      "Train Batch 1/150 | Loss: 1.7923\n",
      "Train Batch 101/150 | Loss: 1.8267\n",
      "Train Batch 150/150 | Loss: 1.7122\n",
      "[Epoch 27] Train Loss: 1.7862 | Train Acc: 0.4634 | Train Perplexity: 5.9665\n",
      "[Epoch 27] Val Loss: 2.0710 | Val Acc: 0.2894 | Val Perplexity: 7.9325\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 28/40\n",
      "----------------------------------------\n",
      "Train Batch 1/150 | Loss: 1.7912\n",
      "Train Batch 101/150 | Loss: 1.6923\n",
      "Train Batch 150/150 | Loss: 1.7713\n",
      "[Epoch 28] Train Loss: 1.7527 | Train Acc: 0.4858 | Train Perplexity: 5.7704\n",
      "[Epoch 28] Val Loss: 1.9795 | Val Acc: 0.3594 | Val Perplexity: 7.2389\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 29/40\n",
      "----------------------------------------\n",
      "Train Batch 1/150 | Loss: 1.6736\n",
      "Train Batch 101/150 | Loss: 1.5706\n",
      "Train Batch 150/150 | Loss: 1.4846\n",
      "[Epoch 29] Train Loss: 1.6697 | Train Acc: 0.5392 | Train Perplexity: 5.3103\n",
      "[Epoch 29] Val Loss: 1.7101 | Val Acc: 0.5094 | Val Perplexity: 5.5298\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 30/40\n",
      "----------------------------------------\n",
      "Train Batch 1/150 | Loss: 1.5168\n",
      "Train Batch 101/150 | Loss: 1.4873\n",
      "Train Batch 150/150 | Loss: 1.4062\n",
      "[Epoch 30] Train Loss: 1.4867 | Train Acc: 0.6478 | Train Perplexity: 4.4225\n",
      "[Epoch 30] Val Loss: 1.2538 | Val Acc: 0.7706 | Val Perplexity: 3.5035\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 31/40\n",
      "----------------------------------------\n",
      "Train Batch 1/150 | Loss: 1.3255\n",
      "Train Batch 101/150 | Loss: 1.3229\n",
      "Train Batch 150/150 | Loss: 1.1698\n",
      "[Epoch 31] Train Loss: 1.2856 | Train Acc: 0.7558 | Train Perplexity: 3.6167\n",
      "[Epoch 31] Val Loss: 0.9634 | Val Acc: 0.9278 | Val Perplexity: 2.6205\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 32/40\n",
      "----------------------------------------\n",
      "Train Batch 1/150 | Loss: 1.0339\n",
      "Train Batch 101/150 | Loss: 1.1730\n",
      "Train Batch 150/150 | Loss: 1.1351\n",
      "[Epoch 32] Train Loss: 1.1405 | Train Acc: 0.8390 | Train Perplexity: 3.1282\n",
      "[Epoch 32] Val Loss: 0.8454 | Val Acc: 0.9762 | Val Perplexity: 2.3288\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 33/40\n",
      "----------------------------------------\n",
      "Train Batch 1/150 | Loss: 1.0332\n",
      "Train Batch 101/150 | Loss: 0.9966\n",
      "Train Batch 150/150 | Loss: 1.0033\n",
      "[Epoch 33] Train Loss: 1.0685 | Train Acc: 0.8673 | Train Perplexity: 2.9110\n",
      "[Epoch 33] Val Loss: 0.8079 | Val Acc: 0.9900 | Val Perplexity: 2.2431\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 34/40\n",
      "----------------------------------------\n",
      "Train Batch 1/150 | Loss: 1.1401\n",
      "Train Batch 101/150 | Loss: 0.8929\n",
      "Train Batch 150/150 | Loss: 0.8717\n",
      "[Epoch 34] Train Loss: 1.0158 | Train Acc: 0.8910 | Train Perplexity: 2.7616\n",
      "[Epoch 34] Val Loss: 0.7820 | Val Acc: 0.9959 | Val Perplexity: 2.1859\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 35/40\n",
      "----------------------------------------\n",
      "Train Batch 1/150 | Loss: 1.0015\n",
      "Train Batch 101/150 | Loss: 1.0109\n",
      "Train Batch 150/150 | Loss: 1.0362\n",
      "[Epoch 35] Train Loss: 0.9729 | Train Acc: 0.9102 | Train Perplexity: 2.6455\n",
      "[Epoch 35] Val Loss: 0.7661 | Val Acc: 0.9991 | Val Perplexity: 2.1514\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 36/40\n",
      "----------------------------------------\n",
      "Train Batch 1/150 | Loss: 0.9245\n",
      "Train Batch 101/150 | Loss: 0.9243\n",
      "Train Batch 150/150 | Loss: 0.9116\n",
      "[Epoch 36] Train Loss: 0.9421 | Train Acc: 0.9263 | Train Perplexity: 2.5653\n",
      "[Epoch 36] Val Loss: 0.7547 | Val Acc: 0.9997 | Val Perplexity: 2.1269\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 37/40\n",
      "----------------------------------------\n",
      "Train Batch 1/150 | Loss: 1.0335\n",
      "Train Batch 101/150 | Loss: 0.8712\n",
      "Train Batch 150/150 | Loss: 1.0010\n",
      "[Epoch 37] Train Loss: 0.9283 | Train Acc: 0.9310 | Train Perplexity: 2.5301\n",
      "[Epoch 37] Val Loss: 0.7638 | Val Acc: 0.9981 | Val Perplexity: 2.1465\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 38/40\n",
      "----------------------------------------\n",
      "Train Batch 1/150 | Loss: 0.8653\n",
      "Train Batch 101/150 | Loss: 1.0509\n",
      "Train Batch 150/150 | Loss: 0.9107\n",
      "[Epoch 38] Train Loss: 0.9230 | Train Acc: 0.9300 | Train Perplexity: 2.5169\n",
      "[Epoch 38] Val Loss: 0.7618 | Val Acc: 0.9994 | Val Perplexity: 2.1422\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 39/40\n",
      "----------------------------------------\n",
      "Train Batch 1/150 | Loss: 1.0821\n",
      "Train Batch 101/150 | Loss: 0.8515\n",
      "Train Batch 150/150 | Loss: 0.8568\n",
      "[Epoch 39] Train Loss: 0.8980 | Train Acc: 0.9400 | Train Perplexity: 2.4546\n",
      "[Epoch 39] Val Loss: 0.7495 | Val Acc: 1.0000 | Val Perplexity: 2.1159\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 40/40\n",
      "----------------------------------------\n",
      "Train Batch 1/150 | Loss: 0.8403\n",
      "Train Batch 101/150 | Loss: 0.9112\n",
      "Train Batch 150/150 | Loss: 0.8308\n",
      "[Epoch 40] Train Loss: 0.8867 | Train Acc: 0.9426 | Train Perplexity: 2.4270\n",
      "[Epoch 40] Val Loss: 0.7467 | Val Acc: 1.0000 | Val Perplexity: 2.1099\n",
      "----------------------------------------\n",
      "\n",
      "Training complete\n"
     ]
    }
   ],
   "source": [
    "criterion =  nn.CrossEntropyLoss(label_smoothing=LABEL_SMOOTHING)\n",
    "optimizer = optim.AdamW(transformer.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=WARMUP_STEPS)  \n",
    "\n",
    "results = train_model(\n",
    "    train_dataset_loader, validation_dataset_loader, transformer, criterion, \n",
    "    optimizer, max_epochs=MAX_EPOCHS, device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917a1fa4",
   "metadata": {},
   "source": [
    "### 3. Test Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7d144441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7467 | Train Acc: 1.0000 | Train Perplexity: 2.1100\n"
     ]
    }
   ],
   "source": [
    "results = test_model(test_dataset_loader, transformer, criterion, device)\n",
    "print(f\"Train Loss: {results[\"test_loss\"]:.4f} | Train Acc: {results[\"test_acc\"]:.4f} | Train Perplexity: {results[\"test_perplexity\"]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28dac7b",
   "metadata": {},
   "source": [
    "## Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6b590910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inference(model: Transformer, source_sequence, start_tokens):\n",
    "    model.eval()\n",
    "    generated = start_tokens\n",
    "    \n",
    "    with T.no_grad():\n",
    "        logits = model(source_sequence, generated)\n",
    "    \n",
    "    # Greedy Selection\n",
    "    next_token = T.argmax(logits[:, -1, :], dim=-1, keepdim=True)\n",
    "\n",
    "    generated = T.cat([generated, next_token], dim=1)\n",
    "\n",
    "    return generated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7f458678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Actual Trials\n",
      "[array(['green', 'star', '4'], dtype='<U6'), array(['red', 'circle', '4'], dtype='<U6'), array(['blue', 'cross', '3'], dtype='<U6'), array(['yellow', 'cross', '1'], dtype='<U6'), array(['yellow', 'circle', '1'], dtype='<U6'), 'SEP', 'C4', 'EOS', array(['yellow', 'circle', '4'], dtype='<U6'), 'SEP', 'C4']\n",
      "[array(['yellow', 'circle', '1'], dtype='<U6'), array(['blue', 'circle', '2'], dtype='<U6'), array(['green', 'star', '1'], dtype='<U6'), array(['red', 'star', '3'], dtype='<U6'), array(['green', 'star', '3'], dtype='<U6'), 'SEP', 'C3', 'EOS', array(['yellow', 'star', '2'], dtype='<U6'), 'SEP', 'C1']\n",
      "[array(['yellow', 'cross', '1'], dtype='<U6'), array(['red', 'square', '4'], dtype='<U6'), array(['green', 'circle', '3'], dtype='<U6'), array(['blue', 'square', '3'], dtype='<U6'), array(['green', 'star', '3'], dtype='<U6'), 'SEP', 'C3', 'EOS', array(['red', 'circle', '1'], dtype='<U6'), 'SEP', 'C2']\n",
      "[array(['blue', 'cross', '1'], dtype='<U6'), array(['green', 'circle', '2'], dtype='<U6'), array(['red', 'circle', '4'], dtype='<U6'), array(['yellow', 'star', '3'], dtype='<U6'), array(['yellow', 'circle', '4'], dtype='<U6'), 'SEP', 'C4', 'EOS', array(['yellow', 'star', '1'], dtype='<U6'), 'SEP', 'C4']\n",
      "[array(['yellow', 'cross', '2'], dtype='<U6'), array(['red', 'square', '2'], dtype='<U6'), array(['green', 'star', '2'], dtype='<U6'), array(['blue', 'circle', '1'], dtype='<U6'), array(['yellow', 'square', '1'], dtype='<U6'), 'SEP', 'C1', 'EOS', array(['yellow', 'star', '4'], dtype='<U6'), 'SEP', 'C1']\n",
      "[array(['red', 'circle', '4'], dtype='<U6'), array(['yellow', 'star', '4'], dtype='<U6'), array(['green', 'square', '1'], dtype='<U6'), array(['blue', 'circle', '2'], dtype='<U6'), array(['blue', 'square', '1'], dtype='<U6'), 'SEP', 'C4', 'EOS', array(['green', 'cross', '1'], dtype='<U6'), 'SEP', 'C3']\n",
      "[array(['green', 'square', '3'], dtype='<U6'), array(['red', 'square', '2'], dtype='<U6'), array(['yellow', 'star', '2'], dtype='<U6'), array(['blue', 'square', '4'], dtype='<U6'), array(['blue', 'square', '3'], dtype='<U6'), 'SEP', 'C4', 'EOS', array(['green', 'circle', '2'], dtype='<U6'), 'SEP', 'C1']\n",
      "[array(['yellow', 'circle', '1'], dtype='<U6'), array(['blue', 'star', '1'], dtype='<U6'), array(['red', 'star', '4'], dtype='<U6'), array(['green', 'square', '1'], dtype='<U6'), array(['blue', 'cross', '1'], dtype='<U6'), 'SEP', 'C2', 'EOS', array(['red', 'cross', '3'], dtype='<U6'), 'SEP', 'C3']\n",
      "[array(['green', 'star', '2'], dtype='<U6'), array(['red', 'star', '2'], dtype='<U6'), array(['yellow', 'square', '3'], dtype='<U6'), array(['blue', 'cross', '2'], dtype='<U6'), array(['blue', 'square', '4'], dtype='<U6'), 'SEP', 'C4', 'EOS', array(['red', 'square', '1'], dtype='<U6'), 'SEP', 'C2']\n",
      "[array(['blue', 'circle', '4'], dtype='<U6'), array(['red', 'star', '3'], dtype='<U6'), array(['green', 'square', '3'], dtype='<U6'), array(['yellow', 'cross', '4'], dtype='<U6'), array(['blue', 'circle', '1'], dtype='<U6'), 'SEP', 'C1', 'EOS', array(['yellow', 'square', '2'], dtype='<U6'), 'SEP', 'C4']\n",
      "Feature for Classification:  2 \n",
      "\n",
      "# Predicted Trials\n",
      "[array(['green', 'star', '4'], dtype='<U6'), array(['red', 'circle', '4'], dtype='<U6'), array(['blue', 'cross', '3'], dtype='<U6'), array(['yellow', 'cross', '1'], dtype='<U6'), array(['yellow', 'circle', '1'], dtype='<U6'), 'SEP', 'C4', 'EOS', array(['yellow', 'circle', '4'], dtype='<U6'), 'SEP', 'C4']\n",
      "[array(['yellow', 'circle', '1'], dtype='<U6'), array(['blue', 'circle', '2'], dtype='<U6'), array(['green', 'star', '1'], dtype='<U6'), array(['red', 'star', '3'], dtype='<U6'), array(['green', 'star', '3'], dtype='<U6'), 'SEP', 'C3', 'EOS', array(['yellow', 'star', '2'], dtype='<U6'), 'SEP', 'C1']\n",
      "[array(['yellow', 'cross', '1'], dtype='<U6'), array(['red', 'square', '4'], dtype='<U6'), array(['green', 'circle', '3'], dtype='<U6'), array(['blue', 'square', '3'], dtype='<U6'), array(['green', 'star', '3'], dtype='<U6'), 'SEP', 'C3', 'EOS', array(['red', 'circle', '1'], dtype='<U6'), 'SEP', 'C2']\n",
      "[array(['blue', 'cross', '1'], dtype='<U6'), array(['green', 'circle', '2'], dtype='<U6'), array(['red', 'circle', '4'], dtype='<U6'), array(['yellow', 'star', '3'], dtype='<U6'), array(['yellow', 'circle', '4'], dtype='<U6'), 'SEP', 'C4', 'EOS', array(['yellow', 'star', '1'], dtype='<U6'), 'SEP', 'C4']\n",
      "[array(['yellow', 'cross', '2'], dtype='<U6'), array(['red', 'square', '2'], dtype='<U6'), array(['green', 'star', '2'], dtype='<U6'), array(['blue', 'circle', '1'], dtype='<U6'), array(['yellow', 'square', '1'], dtype='<U6'), 'SEP', 'C1', 'EOS', array(['yellow', 'star', '4'], dtype='<U6'), 'SEP', 'C1']\n",
      "[array(['red', 'circle', '4'], dtype='<U6'), array(['yellow', 'star', '4'], dtype='<U6'), array(['green', 'square', '1'], dtype='<U6'), array(['blue', 'circle', '2'], dtype='<U6'), array(['blue', 'square', '1'], dtype='<U6'), 'SEP', 'C4', 'EOS', array(['green', 'cross', '1'], dtype='<U6'), 'SEP', 'C3']\n",
      "[array(['green', 'square', '3'], dtype='<U6'), array(['red', 'square', '2'], dtype='<U6'), array(['yellow', 'star', '2'], dtype='<U6'), array(['blue', 'square', '4'], dtype='<U6'), array(['blue', 'square', '3'], dtype='<U6'), 'SEP', 'C4', 'EOS', array(['green', 'circle', '2'], dtype='<U6'), 'SEP', 'C1']\n",
      "[array(['yellow', 'circle', '1'], dtype='<U6'), array(['blue', 'star', '1'], dtype='<U6'), array(['red', 'star', '4'], dtype='<U6'), array(['green', 'square', '1'], dtype='<U6'), array(['blue', 'cross', '1'], dtype='<U6'), 'SEP', 'C2', 'EOS', array(['red', 'cross', '3'], dtype='<U6'), 'SEP', 'C3']\n",
      "[array(['green', 'star', '2'], dtype='<U6'), array(['red', 'star', '2'], dtype='<U6'), array(['yellow', 'square', '3'], dtype='<U6'), array(['blue', 'cross', '2'], dtype='<U6'), array(['blue', 'square', '4'], dtype='<U6'), 'SEP', 'C4', 'EOS', array(['red', 'square', '1'], dtype='<U6'), 'SEP', 'C2']\n",
      "[array(['blue', 'circle', '4'], dtype='<U6'), array(['red', 'star', '3'], dtype='<U6'), array(['green', 'square', '3'], dtype='<U6'), array(['yellow', 'cross', '4'], dtype='<U6'), array(['blue', 'circle', '1'], dtype='<U6'), 'SEP', 'C1', 'EOS', array(['yellow', 'square', '2'], dtype='<U6'), 'SEP', 'C4']\n",
      "Feature for Classification:  2 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "x, target = test_data[:10].to(device), test_targets[:10].to(device)\n",
    "prediction = model_inference(transformer, x, target[:, : -1])\n",
    "\n",
    "print(\"# Actual Trials\")\n",
    "test_batch = [np.asarray(item.cpu()) for item in [x, target]]\n",
    "output = wcst.visualise_batch(test_batch)\n",
    "\n",
    "print(\"# Predicted Trials\")\n",
    "prediction_batch = [np.asarray(item.cpu()) for item in [x, prediction]]\n",
    "output = wcst.visualise_batch(prediction_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f73bf2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
