{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd555848",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch as T\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "389721a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "\n",
    "device = T.device('cuda' if T.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "\n",
    "T.manual_seed(SEED)\n",
    "\n",
    "if T.cuda.is_available():\n",
    "    T.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08945224",
   "metadata": {},
   "source": [
    "## Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25ea2ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ba4f2c",
   "metadata": {},
   "source": [
    "### 1. Dataset Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e95fdcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e47ffd5",
   "metadata": {},
   "source": [
    "### 2. Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d16c929e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path):\n",
    "    encoder_data, decoder_data, targets = T.load(path)\n",
    "    return TensorDataset(encoder_data, decoder_data, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "365835c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datasets = {\n",
    "    \"color\": load_dataset(\"../datasets/train_context_0.pt\"),\n",
    "    \"shape\": load_dataset(\"../datasets/train_context_1.pt\"),\n",
    "    \"quantity\": load_dataset(\"../datasets/train_context_2.pt\"),\n",
    "}\n",
    "\n",
    "train_loaders = {\n",
    "    ctx: DataLoader(ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    for ctx, ds in train_datasets.items()\n",
    "}\n",
    "\n",
    "validation_dataset = load_dataset('../datasets/validation_dataset.pt')\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "test_dataset = load_dataset('../datasets/test_dataset.pt')\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f524267",
   "metadata": {},
   "source": [
    "## Transformer Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3a8bc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.transformer import Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7739c3fe",
   "metadata": {},
   "source": [
    "### 1. Transformer Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "606c1008",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCABULARY_SIZE = 70        # 64 cards + 4 categories + SEP + EOS\n",
    "EMBEDDING_SIZE = 216        # larger embedding to capture card features\n",
    "N_ATTENTION_HEADS = 6       # more heads for better multi-feature attention\n",
    "N_BLOCKS = 3                # same depth as before\n",
    "MAX_SEQUENCE_LENGTH = 10    # longer max sequence to accommodate multiple past trials\n",
    "FF_DIMS = 256               # larger feedforward layer for better representation\n",
    "DROPOUT_PROB = 0.2        # reduce dropout slightly to retain signal in small batches\n",
    "CARD_DIMS = (4, 4, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab9fc96",
   "metadata": {},
   "source": [
    "### 2. Transformer Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f25ee98",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    VOCABULARY_SIZE, VOCABULARY_SIZE, CARD_DIMS, EMBEDDING_SIZE, N_ATTENTION_HEADS,\n",
    "    N_BLOCKS, MAX_SEQUENCE_LENGTH, FF_DIMS, DROPOUT_PROB, device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7489333",
   "metadata": {},
   "source": [
    "## Training Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42f3ebdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import torch as T\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn, optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c5d23d",
   "metadata": {},
   "source": [
    "### 1. Train, Validate, Evaluate Model Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a8ee8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    train_loader: DataLoader,\n",
    "    validation_loader: DataLoader,\n",
    "    model: Transformer,\n",
    "    criterion: nn.CrossEntropyLoss,\n",
    "    optimizer: optim.Optimizer,\n",
    "    max_epochs: int = 20,\n",
    "    device: str | T.device = \"cpu\",\n",
    "    patience: int = 3,\n",
    "):\n",
    "    best_val_loss = np.inf\n",
    "    patience_counter = 0\n",
    "\n",
    "    train_losses, train_accs, train_perplexities = [], [], []\n",
    "    val_losses, val_accs, val_perplexities = [], [], []\n",
    "    best_model_state = model.state_dict()\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{max_epochs}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "        # --- Training ---\n",
    "        model.train()\n",
    "        epoch_train_losses = []\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        for batch_idx, (encoder_input, decoder_input, target) in enumerate(train_loader):\n",
    "            encoder_input, decoder_input, target = encoder_input.to(device), decoder_input.to(device), target.view(-1).to(device)\n",
    "    \n",
    "            # Forward pass\n",
    "            logits = model(encoder_input, decoder_input)  # [batch, seq_len, vocab]\n",
    "            logits = logits[:, -1, :]  # only the final step prediction [batch, vocab]\n",
    "\n",
    "            loss = criterion(logits, target)\n",
    "\n",
    "            # Backprop\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accuracy\n",
    "            preds = logits.argmax(dim=1)\n",
    "            total_correct += (preds == target).sum().item()\n",
    "            total_samples += target.size(0)\n",
    "\n",
    "            epoch_train_losses.append(loss.item())\n",
    "\n",
    "            if batch_idx % 100 == 0 or batch_idx == len(train_loader) - 1:\n",
    "                print(f\"Train Batch {batch_idx+1}/{len(train_loader)} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "        train_loss = np.mean(epoch_train_losses)\n",
    "        train_acc = total_correct / total_samples\n",
    "        train_perplexity = np.exp(train_loss)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        train_perplexities.append(train_perplexity)\n",
    "\n",
    "        print(f\"[Epoch {epoch+1}] Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Train Perplexity: {train_perplexity:.4f}\")\n",
    "\n",
    "        # --- Validation ---\n",
    "        model.eval()\n",
    "        val_batch_losses = []\n",
    "        val_correct = 0\n",
    "        val_samples = 0\n",
    "\n",
    "        with T.no_grad():\n",
    "            for encoder_input, decoder_input, target in validation_loader:\n",
    "                encoder_input, decoder_input, target = encoder_input.to(device), decoder_input.to(device), target.view(-1).to(device)\n",
    "\n",
    "                logits = model(encoder_input, decoder_input) # [batch, seq_len, vocab]\n",
    "                logits = logits[:, -1, :]  # only the final step prediction [batch, vocab]\n",
    "\n",
    "                loss = criterion(logits, target)\n",
    "\n",
    "                preds = logits.argmax(dim=1)\n",
    "                val_correct += (preds == target).sum().item()\n",
    "                val_samples += target.size(0)\n",
    "\n",
    "                val_batch_losses.append(loss.item())\n",
    "\n",
    "        val_loss = np.mean(val_batch_losses)\n",
    "        val_acc = val_correct / val_samples\n",
    "        val_perplexity = np.exp(val_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_acc)\n",
    "        val_perplexities.append(val_perplexity)\n",
    "\n",
    "        print(f\"[Epoch {epoch+1}] Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | Val Perplexity: {val_perplexity:.4f}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "        # --- Early Stopping ---\n",
    "        # if val_loss < best_val_loss:\n",
    "        #     best_val_loss = val_loss\n",
    "        #     patience_counter = 0\n",
    "        #     best_model_state = model.state_dict()\n",
    "        #     print(f\"Validation loss improved — saving model (Loss: {val_loss:.4f})\")\n",
    "        # else:\n",
    "        #     patience_counter += 1\n",
    "        #     print(f\"No improvement ({patience_counter}/{patience})\")\n",
    "        #     if patience_counter >= patience:\n",
    "        #         print(\"\\nEarly stopping triggered. Restoring best model.\")\n",
    "        #         model.load_state_dict(best_model_state)\n",
    "        #         break\n",
    "\n",
    "    print(\"\\nTraining complete\")\n",
    "\n",
    "    return {\n",
    "        \"train_losses\": train_losses,\n",
    "        \"train_accs\": train_accs,\n",
    "        \"train_perplexities\": train_perplexities,\n",
    "        \"val_losses\": val_losses,\n",
    "        \"val_accs\": val_accs,\n",
    "        \"val_perplexities\": val_perplexities,\n",
    "        \"best_val_loss\": best_val_loss,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6af9014f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_round_robin(\n",
    "    train_loaders: dict[str, DataLoader], validation_loader: DataLoader, model: nn.Module,\n",
    "    criterion: nn.CrossEntropyLoss, optimizer: optim.Optimizer, max_epochs: int = 20,\n",
    "    device: str | T.device = \"cpu\", patience: int = 3,\n",
    "):\n",
    "    best_val_loss = np.inf\n",
    "    patience_counter = 0\n",
    "    best_model_state = model.state_dict()\n",
    "\n",
    "    history = {k: [] for k in [\n",
    "        \"train_losses\", \"train_accs\", \"train_perplexities\",\n",
    "        \"val_losses\", \"val_accs\", \"val_perplexities\"\n",
    "    ]}\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        print(f\"\\n[Epoch {epoch + 1}/{max_epochs}]\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "        model.train()\n",
    "        epoch_losses, total_correct, total_samples = [], 0, 0\n",
    "\n",
    "        # --- Build a round-robin iterator across all loaders ---\n",
    "        loaders_cycle = itertools.cycle(train_loaders.items())\n",
    "        active_iters = {ctx: iter(dl) for ctx, dl in train_loaders.items()}\n",
    "\n",
    "        # Find smallest loader length to roughly balance epoch size\n",
    "        min_len = min(len(dl) for dl in train_loaders.values())\n",
    "        total_batches = min_len * len(train_loaders)\n",
    "\n",
    "        for batch_idx in range(total_batches):\n",
    "            context, _ = next(loaders_cycle)\n",
    "            loader_iter = active_iters[context]\n",
    "\n",
    "            try:\n",
    "                encoder_input, decoder_input, target = next(loader_iter)\n",
    "            except StopIteration:\n",
    "                # Restart exhausted iterator\n",
    "                active_iters[context] = iter(train_loaders[context])\n",
    "                encoder_input, decoder_input, target = next(active_iters[context])\n",
    "\n",
    "            encoder_input, decoder_input, target = (\n",
    "                encoder_input.to(device),\n",
    "                decoder_input.to(device),\n",
    "                target.view(-1).to(device)\n",
    "            )\n",
    "\n",
    "            # Forward pass\n",
    "            logits = model(encoder_input, decoder_input)\n",
    "            logits = logits[:, -1, :]  # predict final token only\n",
    "            loss = criterion(logits, target)\n",
    "\n",
    "            # Backprop\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Metrics\n",
    "            preds = logits.argmax(dim=1)\n",
    "            total_correct += (preds == target).sum().item()\n",
    "            total_samples += target.size(0)\n",
    "            epoch_losses.append(loss.item())\n",
    "\n",
    "            if batch_idx % 50 == 0 or batch_idx == total_batches - 1:\n",
    "                print(f\"[{context}] Batch {batch_idx+1}/{total_batches} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "        # --- Epoch stats ---\n",
    "        train_loss = np.mean(epoch_losses)\n",
    "        train_acc = total_correct / total_samples\n",
    "        train_perplexity = np.exp(train_loss)\n",
    "\n",
    "        print(f\"[Epoch {epoch+1}] Train Loss: {train_loss:.4f} | Acc: {train_acc:.4f} | Perplexity: {train_perplexity:.4f}\")\n",
    "\n",
    "        # --- Validation ---\n",
    "        model.eval()\n",
    "        val_losses, val_correct, val_samples = [], 0, 0\n",
    "\n",
    "        with T.no_grad():\n",
    "            for encoder_input, decoder_input, target in validation_loader:\n",
    "                encoder_input, decoder_input, target = (\n",
    "                    encoder_input.to(device),\n",
    "                    decoder_input.to(device),\n",
    "                    target.view(-1).to(device)\n",
    "                )\n",
    "                logits = model(encoder_input, decoder_input)\n",
    "                logits = logits[:, -1, :]\n",
    "                loss = criterion(logits, target)\n",
    "                preds = logits.argmax(dim=1)\n",
    "                val_correct += (preds == target).sum().item()\n",
    "                val_samples += target.size(0)\n",
    "                val_losses.append(loss.item())\n",
    "\n",
    "        val_loss = np.mean(val_losses)\n",
    "        val_acc = val_correct / val_samples\n",
    "        val_perplexity = np.exp(val_loss)\n",
    "\n",
    "        print(f\"[Epoch {epoch+1}] Val Loss: {val_loss:.4f} | Acc: {val_acc:.4f} | Perplexity: {val_perplexity:.4f}\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "        # --- Logging ---\n",
    "        history[\"train_losses\"].append(train_loss)\n",
    "        history[\"train_accs\"].append(train_acc)\n",
    "        history[\"train_perplexities\"].append(train_perplexity)\n",
    "        history[\"val_losses\"].append(val_loss)\n",
    "        history[\"val_accs\"].append(val_acc)\n",
    "        history[\"val_perplexities\"].append(val_perplexity)\n",
    "\n",
    "        # --- Early stopping ---\n",
    "        # if val_loss < best_val_loss:\n",
    "        #     best_val_loss = val_loss\n",
    "        #     best_model_state = model.state_dict()\n",
    "        #     patience_counter = 0\n",
    "        # else:\n",
    "        #     patience_counter += 1\n",
    "        #     if patience_counter >= patience:\n",
    "        #         print(\"Early stopping — restoring best model.\")\n",
    "        #         model.load_state_dict(best_model_state)\n",
    "        #         break\n",
    "\n",
    "    print(\"\\n Training complete (Round-Robin Mode)\")\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44864dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(test_loader: DataLoader, model: Transformer, criterion: nn.CrossEntropyLoss, device: str | T.device = \"cpu\"):\n",
    "\n",
    "    model.eval()\n",
    "    test_batch_losses = []\n",
    "    test_correct = 0\n",
    "    test_tokens = 0\n",
    "\n",
    "    with T.no_grad():\n",
    "        for encoder_input, decoder_input, target in test_loader:\n",
    "            encoder_input, decoder_input, target = encoder_input.to(device), decoder_input.to(device), target.view(-1).to(device)\n",
    "            \n",
    "            logits = model(encoder_input, decoder_input)[:, -1, :]\n",
    "            loss = criterion(logits, target)\n",
    "\n",
    "            preds = logits.argmax(dim=1)\n",
    "            test_correct += (preds == target).sum().item()\n",
    "            test_tokens += target.size(0)\n",
    "\n",
    "            test_batch_losses.append(loss.item())\n",
    "\n",
    "    test_loss = np.mean(test_batch_losses)\n",
    "    test_acc = test_correct / test_tokens\n",
    "    test_perplexity = np.exp(test_loss)\n",
    "\n",
    "    return {\n",
    "        \"test_loss\": test_loss,\n",
    "        \"test_acc\": test_acc,\n",
    "        \"test_perplexity\": test_perplexity\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d863c67",
   "metadata": {},
   "source": [
    "### 2. Train Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "37f53def",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 3e-4\n",
    "BATCH_SIZE = 64\n",
    "WEIGHT_DECAY = 1e-2\n",
    "WARMUP_STEPS = 400\n",
    "LABEL_SMOOTHING = 0.1\n",
    "MAX_EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a5c8f99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 1/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 4.6329\n",
      "[quantity] Batch 51/300 | Loss: 1.9630\n",
      "[shape] Batch 101/300 | Loss: 1.9775\n",
      "[color] Batch 151/300 | Loss: 1.9750\n",
      "[quantity] Batch 201/300 | Loss: 1.9506\n",
      "[shape] Batch 251/300 | Loss: 1.9705\n",
      "[quantity] Batch 300/300 | Loss: 1.9675\n",
      "[Epoch 1] Train Loss: 2.0072 | Acc: 0.2449 | Perplexity: 7.4427\n",
      "[Epoch 1] Val Loss: 1.9667 | Acc: 0.2528 | Perplexity: 7.1468\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 2/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.9694\n",
      "[quantity] Batch 51/300 | Loss: 1.9635\n",
      "[shape] Batch 101/300 | Loss: 2.0081\n",
      "[color] Batch 151/300 | Loss: 1.9806\n",
      "[quantity] Batch 201/300 | Loss: 1.9820\n",
      "[shape] Batch 251/300 | Loss: 1.9797\n",
      "[quantity] Batch 300/300 | Loss: 1.9589\n",
      "[Epoch 2] Train Loss: 1.9763 | Acc: 0.2514 | Perplexity: 7.2162\n",
      "[Epoch 2] Val Loss: 1.9650 | Acc: 0.2528 | Perplexity: 7.1347\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 3/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.9631\n",
      "[quantity] Batch 51/300 | Loss: 1.9560\n",
      "[shape] Batch 101/300 | Loss: 1.9837\n",
      "[color] Batch 151/300 | Loss: 1.9748\n",
      "[quantity] Batch 201/300 | Loss: 1.9788\n",
      "[shape] Batch 251/300 | Loss: 1.9813\n",
      "[quantity] Batch 300/300 | Loss: 2.0212\n",
      "[Epoch 3] Train Loss: 1.9741 | Acc: 0.2491 | Perplexity: 7.2000\n",
      "[Epoch 3] Val Loss: 1.9893 | Acc: 0.2528 | Perplexity: 7.3107\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 4/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.9521\n",
      "[quantity] Batch 51/300 | Loss: 1.9635\n",
      "[shape] Batch 101/300 | Loss: 1.9711\n",
      "[color] Batch 151/300 | Loss: 1.9949\n",
      "[quantity] Batch 201/300 | Loss: 1.9521\n",
      "[shape] Batch 251/300 | Loss: 1.9687\n",
      "[quantity] Batch 300/300 | Loss: 1.9438\n",
      "[Epoch 4] Train Loss: 1.9705 | Acc: 0.2574 | Perplexity: 7.1745\n",
      "[Epoch 4] Val Loss: 1.9795 | Acc: 0.2377 | Perplexity: 7.2395\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 5/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 2.0117\n",
      "[quantity] Batch 51/300 | Loss: 1.9894\n",
      "[shape] Batch 101/300 | Loss: 1.9608\n",
      "[color] Batch 151/300 | Loss: 1.9871\n",
      "[quantity] Batch 201/300 | Loss: 1.9758\n",
      "[shape] Batch 251/300 | Loss: 1.9643\n",
      "[quantity] Batch 300/300 | Loss: 1.9476\n",
      "[Epoch 5] Train Loss: 1.9689 | Acc: 0.2528 | Perplexity: 7.1630\n",
      "[Epoch 5] Val Loss: 1.9675 | Acc: 0.2503 | Perplexity: 7.1530\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 6/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.9588\n",
      "[quantity] Batch 51/300 | Loss: 1.9836\n",
      "[shape] Batch 101/300 | Loss: 1.9766\n",
      "[color] Batch 151/300 | Loss: 1.9670\n",
      "[quantity] Batch 201/300 | Loss: 1.9632\n",
      "[shape] Batch 251/300 | Loss: 1.9658\n",
      "[quantity] Batch 300/300 | Loss: 1.9652\n",
      "[Epoch 6] Train Loss: 1.9677 | Acc: 0.2539 | Perplexity: 7.1543\n",
      "[Epoch 6] Val Loss: 1.9658 | Acc: 0.2505 | Perplexity: 7.1405\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 7/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.9583\n",
      "[quantity] Batch 51/300 | Loss: 1.9799\n",
      "[shape] Batch 101/300 | Loss: 1.9487\n",
      "[color] Batch 151/300 | Loss: 1.9660\n",
      "[quantity] Batch 201/300 | Loss: 1.9608\n",
      "[shape] Batch 251/300 | Loss: 1.9516\n",
      "[quantity] Batch 300/300 | Loss: 1.9618\n",
      "[Epoch 7] Train Loss: 1.9673 | Acc: 0.2599 | Perplexity: 7.1511\n",
      "[Epoch 7] Val Loss: 1.9659 | Acc: 0.2522 | Perplexity: 7.1414\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 8/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.9599\n",
      "[quantity] Batch 51/300 | Loss: 1.9482\n",
      "[shape] Batch 101/300 | Loss: 1.9536\n",
      "[color] Batch 151/300 | Loss: 1.9677\n",
      "[quantity] Batch 201/300 | Loss: 1.9857\n",
      "[shape] Batch 251/300 | Loss: 1.9593\n",
      "[quantity] Batch 300/300 | Loss: 1.9701\n",
      "[Epoch 8] Train Loss: 1.9645 | Acc: 0.2639 | Perplexity: 7.1311\n",
      "[Epoch 8] Val Loss: 1.9655 | Acc: 0.2578 | Perplexity: 7.1384\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 9/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.9659\n",
      "[quantity] Batch 51/300 | Loss: 1.9877\n",
      "[shape] Batch 101/300 | Loss: 1.9522\n",
      "[color] Batch 151/300 | Loss: 1.9600\n",
      "[quantity] Batch 201/300 | Loss: 1.9454\n",
      "[shape] Batch 251/300 | Loss: 1.9707\n",
      "[quantity] Batch 300/300 | Loss: 1.9629\n",
      "[Epoch 9] Train Loss: 1.9641 | Acc: 0.2655 | Perplexity: 7.1282\n",
      "[Epoch 9] Val Loss: 1.9668 | Acc: 0.2558 | Perplexity: 7.1480\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 10/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.9792\n",
      "[quantity] Batch 51/300 | Loss: 1.9669\n",
      "[shape] Batch 101/300 | Loss: 1.9600\n",
      "[color] Batch 151/300 | Loss: 1.9611\n",
      "[quantity] Batch 201/300 | Loss: 1.9577\n",
      "[shape] Batch 251/300 | Loss: 1.9680\n",
      "[quantity] Batch 300/300 | Loss: 1.9872\n",
      "[Epoch 10] Train Loss: 1.9629 | Acc: 0.2723 | Perplexity: 7.1201\n",
      "[Epoch 10] Val Loss: 1.9703 | Acc: 0.2519 | Perplexity: 7.1731\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 11/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.9503\n",
      "[quantity] Batch 51/300 | Loss: 1.9906\n",
      "[shape] Batch 101/300 | Loss: 1.9809\n",
      "[color] Batch 151/300 | Loss: 1.9537\n",
      "[quantity] Batch 201/300 | Loss: 1.9568\n",
      "[shape] Batch 251/300 | Loss: 1.9717\n",
      "[quantity] Batch 300/300 | Loss: 1.9552\n",
      "[Epoch 11] Train Loss: 1.9613 | Acc: 0.2751 | Perplexity: 7.1088\n",
      "[Epoch 11] Val Loss: 1.9733 | Acc: 0.2503 | Perplexity: 7.1940\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 12/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.9473\n",
      "[quantity] Batch 51/300 | Loss: 1.9573\n",
      "[shape] Batch 101/300 | Loss: 1.9550\n",
      "[color] Batch 151/300 | Loss: 1.9532\n",
      "[quantity] Batch 201/300 | Loss: 1.9605\n",
      "[shape] Batch 251/300 | Loss: 1.9318\n",
      "[quantity] Batch 300/300 | Loss: 1.9460\n",
      "[Epoch 12] Train Loss: 1.9607 | Acc: 0.2795 | Perplexity: 7.1046\n",
      "[Epoch 12] Val Loss: 1.9823 | Acc: 0.2445 | Perplexity: 7.2596\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 13/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.9546\n",
      "[quantity] Batch 51/300 | Loss: 1.9449\n",
      "[shape] Batch 101/300 | Loss: 1.9507\n",
      "[color] Batch 151/300 | Loss: 1.9782\n",
      "[quantity] Batch 201/300 | Loss: 1.9486\n",
      "[shape] Batch 251/300 | Loss: 1.9633\n",
      "[quantity] Batch 300/300 | Loss: 1.9562\n",
      "[Epoch 13] Train Loss: 1.9585 | Acc: 0.2815 | Perplexity: 7.0886\n",
      "[Epoch 13] Val Loss: 1.9721 | Acc: 0.2539 | Perplexity: 7.1860\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 14/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.9823\n",
      "[quantity] Batch 51/300 | Loss: 1.9615\n",
      "[shape] Batch 101/300 | Loss: 1.9727\n",
      "[color] Batch 151/300 | Loss: 1.9522\n",
      "[quantity] Batch 201/300 | Loss: 1.9521\n",
      "[shape] Batch 251/300 | Loss: 1.9524\n",
      "[quantity] Batch 300/300 | Loss: 1.9364\n",
      "[Epoch 14] Train Loss: 1.9561 | Acc: 0.2849 | Perplexity: 7.0718\n",
      "[Epoch 14] Val Loss: 1.9730 | Acc: 0.2558 | Perplexity: 7.1925\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 15/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.9397\n",
      "[quantity] Batch 51/300 | Loss: 1.9776\n",
      "[shape] Batch 101/300 | Loss: 1.9056\n",
      "[color] Batch 151/300 | Loss: 1.9829\n",
      "[quantity] Batch 201/300 | Loss: 1.9757\n",
      "[shape] Batch 251/300 | Loss: 1.9355\n",
      "[quantity] Batch 300/300 | Loss: 1.9472\n",
      "[Epoch 15] Train Loss: 1.9538 | Acc: 0.2892 | Perplexity: 7.0554\n",
      "[Epoch 15] Val Loss: 1.9711 | Acc: 0.2523 | Perplexity: 7.1786\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 16/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.9302\n",
      "[quantity] Batch 51/300 | Loss: 1.9282\n",
      "[shape] Batch 101/300 | Loss: 1.9585\n",
      "[color] Batch 151/300 | Loss: 1.9515\n",
      "[quantity] Batch 201/300 | Loss: 1.9367\n",
      "[shape] Batch 251/300 | Loss: 1.9426\n",
      "[quantity] Batch 300/300 | Loss: 1.9656\n",
      "[Epoch 16] Train Loss: 1.9498 | Acc: 0.3048 | Perplexity: 7.0276\n",
      "[Epoch 16] Val Loss: 1.9772 | Acc: 0.2603 | Perplexity: 7.2228\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 17/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.9228\n",
      "[quantity] Batch 51/300 | Loss: 1.9370\n",
      "[shape] Batch 101/300 | Loss: 1.9453\n",
      "[color] Batch 151/300 | Loss: 1.9498\n",
      "[quantity] Batch 201/300 | Loss: 1.9563\n",
      "[shape] Batch 251/300 | Loss: 1.9087\n",
      "[quantity] Batch 300/300 | Loss: 1.9377\n",
      "[Epoch 17] Train Loss: 1.9475 | Acc: 0.3044 | Perplexity: 7.0110\n",
      "[Epoch 17] Val Loss: 1.9806 | Acc: 0.2547 | Perplexity: 7.2468\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 18/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.9911\n",
      "[quantity] Batch 51/300 | Loss: 1.9237\n",
      "[shape] Batch 101/300 | Loss: 1.9368\n",
      "[color] Batch 151/300 | Loss: 1.9205\n",
      "[quantity] Batch 201/300 | Loss: 1.9670\n",
      "[shape] Batch 251/300 | Loss: 1.8923\n",
      "[quantity] Batch 300/300 | Loss: 1.9621\n",
      "[Epoch 18] Train Loss: 1.9418 | Acc: 0.3165 | Perplexity: 6.9713\n",
      "[Epoch 18] Val Loss: 1.9831 | Acc: 0.2552 | Perplexity: 7.2655\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 19/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.9334\n",
      "[quantity] Batch 51/300 | Loss: 1.9658\n",
      "[shape] Batch 101/300 | Loss: 1.9443\n",
      "[color] Batch 151/300 | Loss: 1.9226\n",
      "[quantity] Batch 201/300 | Loss: 1.9261\n",
      "[shape] Batch 251/300 | Loss: 1.8642\n",
      "[quantity] Batch 300/300 | Loss: 1.9333\n",
      "[Epoch 19] Train Loss: 1.9382 | Acc: 0.3184 | Perplexity: 6.9461\n",
      "[Epoch 19] Val Loss: 1.9881 | Acc: 0.2552 | Perplexity: 7.3020\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 20/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.9169\n",
      "[quantity] Batch 51/300 | Loss: 1.9538\n",
      "[shape] Batch 101/300 | Loss: 2.0103\n",
      "[color] Batch 151/300 | Loss: 1.9446\n",
      "[quantity] Batch 201/300 | Loss: 1.9442\n",
      "[shape] Batch 251/300 | Loss: 1.8838\n",
      "[quantity] Batch 300/300 | Loss: 1.9369\n",
      "[Epoch 20] Train Loss: 1.9339 | Acc: 0.3289 | Perplexity: 6.9165\n",
      "[Epoch 20] Val Loss: 1.9930 | Acc: 0.2559 | Perplexity: 7.3378\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 21/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.8919\n",
      "[quantity] Batch 51/300 | Loss: 1.9272\n",
      "[shape] Batch 101/300 | Loss: 1.9339\n",
      "[color] Batch 151/300 | Loss: 1.9549\n",
      "[quantity] Batch 201/300 | Loss: 1.9179\n",
      "[shape] Batch 251/300 | Loss: 1.9691\n",
      "[quantity] Batch 300/300 | Loss: 1.9432\n",
      "[Epoch 21] Train Loss: 1.9283 | Acc: 0.3353 | Perplexity: 6.8779\n",
      "[Epoch 21] Val Loss: 2.0012 | Acc: 0.2547 | Perplexity: 7.3981\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 22/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.9227\n",
      "[quantity] Batch 51/300 | Loss: 1.9312\n",
      "[shape] Batch 101/300 | Loss: 1.9454\n",
      "[color] Batch 151/300 | Loss: 1.9271\n",
      "[quantity] Batch 201/300 | Loss: 1.9316\n",
      "[shape] Batch 251/300 | Loss: 1.9638\n",
      "[quantity] Batch 300/300 | Loss: 1.8883\n",
      "[Epoch 22] Train Loss: 1.9239 | Acc: 0.3392 | Perplexity: 6.8473\n",
      "[Epoch 22] Val Loss: 2.0068 | Acc: 0.2542 | Perplexity: 7.4393\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 23/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.8756\n",
      "[quantity] Batch 51/300 | Loss: 1.8842\n",
      "[shape] Batch 101/300 | Loss: 1.9354\n",
      "[color] Batch 151/300 | Loss: 1.9388\n",
      "[quantity] Batch 201/300 | Loss: 1.9200\n",
      "[shape] Batch 251/300 | Loss: 1.9356\n",
      "[quantity] Batch 300/300 | Loss: 1.9273\n",
      "[Epoch 23] Train Loss: 1.9176 | Acc: 0.3456 | Perplexity: 6.8047\n",
      "[Epoch 23] Val Loss: 2.0190 | Acc: 0.2606 | Perplexity: 7.5311\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 24/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.8546\n",
      "[quantity] Batch 51/300 | Loss: 1.8982\n",
      "[shape] Batch 101/300 | Loss: 1.7993\n",
      "[color] Batch 151/300 | Loss: 1.8817\n",
      "[quantity] Batch 201/300 | Loss: 1.9155\n",
      "[shape] Batch 251/300 | Loss: 1.8820\n",
      "[quantity] Batch 300/300 | Loss: 1.9439\n",
      "[Epoch 24] Train Loss: 1.9118 | Acc: 0.3580 | Perplexity: 6.7650\n",
      "[Epoch 24] Val Loss: 2.0154 | Acc: 0.2614 | Perplexity: 7.5034\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 25/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.8505\n",
      "[quantity] Batch 51/300 | Loss: 1.9076\n",
      "[shape] Batch 101/300 | Loss: 1.9171\n",
      "[color] Batch 151/300 | Loss: 1.9128\n",
      "[quantity] Batch 201/300 | Loss: 1.9231\n",
      "[shape] Batch 251/300 | Loss: 1.9342\n",
      "[quantity] Batch 300/300 | Loss: 1.9066\n",
      "[Epoch 25] Train Loss: 1.9018 | Acc: 0.3656 | Perplexity: 6.6980\n",
      "[Epoch 25] Val Loss: 2.0239 | Acc: 0.2539 | Perplexity: 7.5679\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 26/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.9279\n",
      "[quantity] Batch 51/300 | Loss: 1.8401\n",
      "[shape] Batch 101/300 | Loss: 1.9155\n",
      "[color] Batch 151/300 | Loss: 1.8636\n",
      "[quantity] Batch 201/300 | Loss: 1.9625\n",
      "[shape] Batch 251/300 | Loss: 1.8311\n",
      "[quantity] Batch 300/300 | Loss: 1.8844\n",
      "[Epoch 26] Train Loss: 1.8967 | Acc: 0.3735 | Perplexity: 6.6636\n",
      "[Epoch 26] Val Loss: 2.0299 | Acc: 0.2555 | Perplexity: 7.6137\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 27/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.8820\n",
      "[quantity] Batch 51/300 | Loss: 1.9136\n",
      "[shape] Batch 101/300 | Loss: 1.8856\n",
      "[color] Batch 151/300 | Loss: 1.9082\n",
      "[quantity] Batch 201/300 | Loss: 1.8961\n",
      "[shape] Batch 251/300 | Loss: 1.9091\n",
      "[quantity] Batch 300/300 | Loss: 1.8769\n",
      "[Epoch 27] Train Loss: 1.8917 | Acc: 0.3740 | Perplexity: 6.6308\n",
      "[Epoch 27] Val Loss: 2.0454 | Acc: 0.2547 | Perplexity: 7.7326\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 28/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.8530\n",
      "[quantity] Batch 51/300 | Loss: 1.9148\n",
      "[shape] Batch 101/300 | Loss: 1.8316\n",
      "[color] Batch 151/300 | Loss: 1.8128\n",
      "[quantity] Batch 201/300 | Loss: 1.8875\n",
      "[shape] Batch 251/300 | Loss: 1.8916\n",
      "[quantity] Batch 300/300 | Loss: 1.9576\n",
      "[Epoch 28] Train Loss: 1.8831 | Acc: 0.3899 | Perplexity: 6.5736\n",
      "[Epoch 28] Val Loss: 2.0550 | Acc: 0.2492 | Perplexity: 7.8072\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 29/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.8318\n",
      "[quantity] Batch 51/300 | Loss: 1.8699\n",
      "[shape] Batch 101/300 | Loss: 1.7662\n",
      "[color] Batch 151/300 | Loss: 1.8814\n",
      "[quantity] Batch 201/300 | Loss: 1.9039\n",
      "[shape] Batch 251/300 | Loss: 1.8655\n",
      "[quantity] Batch 300/300 | Loss: 1.8586\n",
      "[Epoch 29] Train Loss: 1.8766 | Acc: 0.3923 | Perplexity: 6.5311\n",
      "[Epoch 29] Val Loss: 2.0347 | Acc: 0.2578 | Perplexity: 7.6499\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 30/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.8589\n",
      "[quantity] Batch 51/300 | Loss: 1.8951\n",
      "[shape] Batch 101/300 | Loss: 1.9471\n",
      "[color] Batch 151/300 | Loss: 1.8910\n",
      "[quantity] Batch 201/300 | Loss: 1.8180\n",
      "[shape] Batch 251/300 | Loss: 1.9677\n",
      "[quantity] Batch 300/300 | Loss: 1.9243\n",
      "[Epoch 30] Train Loss: 1.8704 | Acc: 0.4040 | Perplexity: 6.4908\n",
      "[Epoch 30] Val Loss: 2.0634 | Acc: 0.2545 | Perplexity: 7.8727\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 31/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.8547\n",
      "[quantity] Batch 51/300 | Loss: 1.8697\n",
      "[shape] Batch 101/300 | Loss: 1.8677\n",
      "[color] Batch 151/300 | Loss: 1.8253\n",
      "[quantity] Batch 201/300 | Loss: 1.9205\n",
      "[shape] Batch 251/300 | Loss: 1.8730\n",
      "[quantity] Batch 300/300 | Loss: 1.9281\n",
      "[Epoch 31] Train Loss: 1.8590 | Acc: 0.4088 | Perplexity: 6.4172\n",
      "[Epoch 31] Val Loss: 2.0829 | Acc: 0.2544 | Perplexity: 8.0274\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 32/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.7698\n",
      "[quantity] Batch 51/300 | Loss: 1.9498\n",
      "[shape] Batch 101/300 | Loss: 1.7868\n",
      "[color] Batch 151/300 | Loss: 1.8615\n",
      "[quantity] Batch 201/300 | Loss: 1.7571\n",
      "[shape] Batch 251/300 | Loss: 1.8230\n",
      "[quantity] Batch 300/300 | Loss: 1.8807\n",
      "[Epoch 32] Train Loss: 1.8523 | Acc: 0.4165 | Perplexity: 6.3743\n",
      "[Epoch 32] Val Loss: 2.1204 | Acc: 0.2516 | Perplexity: 8.3344\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 33/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.8868\n",
      "[quantity] Batch 51/300 | Loss: 1.8011\n",
      "[shape] Batch 101/300 | Loss: 1.8790\n",
      "[color] Batch 151/300 | Loss: 1.7593\n",
      "[quantity] Batch 201/300 | Loss: 1.8514\n",
      "[shape] Batch 251/300 | Loss: 1.8088\n",
      "[quantity] Batch 300/300 | Loss: 1.8595\n",
      "[Epoch 33] Train Loss: 1.8474 | Acc: 0.4192 | Perplexity: 6.3434\n",
      "[Epoch 33] Val Loss: 2.0596 | Acc: 0.2587 | Perplexity: 7.8425\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 34/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.7858\n",
      "[quantity] Batch 51/300 | Loss: 1.8655\n",
      "[shape] Batch 101/300 | Loss: 1.8564\n",
      "[color] Batch 151/300 | Loss: 1.8371\n",
      "[quantity] Batch 201/300 | Loss: 1.9576\n",
      "[shape] Batch 251/300 | Loss: 1.9417\n",
      "[quantity] Batch 300/300 | Loss: 1.7841\n",
      "[Epoch 34] Train Loss: 1.8358 | Acc: 0.4315 | Perplexity: 6.2703\n",
      "[Epoch 34] Val Loss: 2.1104 | Acc: 0.2564 | Perplexity: 8.2511\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 35/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.7493\n",
      "[quantity] Batch 51/300 | Loss: 1.7721\n",
      "[shape] Batch 101/300 | Loss: 1.9039\n",
      "[color] Batch 151/300 | Loss: 1.8788\n",
      "[quantity] Batch 201/300 | Loss: 1.9454\n",
      "[shape] Batch 251/300 | Loss: 1.8386\n",
      "[quantity] Batch 300/300 | Loss: 1.9114\n",
      "[Epoch 35] Train Loss: 1.8327 | Acc: 0.4321 | Perplexity: 6.2504\n",
      "[Epoch 35] Val Loss: 2.1110 | Acc: 0.2491 | Perplexity: 8.2562\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 36/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.8743\n",
      "[quantity] Batch 51/300 | Loss: 1.7988\n",
      "[shape] Batch 101/300 | Loss: 1.6958\n",
      "[color] Batch 151/300 | Loss: 1.7727\n",
      "[quantity] Batch 201/300 | Loss: 1.8359\n",
      "[shape] Batch 251/300 | Loss: 1.7714\n",
      "[quantity] Batch 300/300 | Loss: 1.7953\n",
      "[Epoch 36] Train Loss: 1.8208 | Acc: 0.4409 | Perplexity: 6.1767\n",
      "[Epoch 36] Val Loss: 2.1175 | Acc: 0.2591 | Perplexity: 8.3103\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 37/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.7989\n",
      "[quantity] Batch 51/300 | Loss: 1.8616\n",
      "[shape] Batch 101/300 | Loss: 1.8406\n",
      "[color] Batch 151/300 | Loss: 1.7621\n",
      "[quantity] Batch 201/300 | Loss: 1.7979\n",
      "[shape] Batch 251/300 | Loss: 1.8783\n",
      "[quantity] Batch 300/300 | Loss: 1.8341\n",
      "[Epoch 37] Train Loss: 1.8090 | Acc: 0.4496 | Perplexity: 6.1046\n",
      "[Epoch 37] Val Loss: 2.1438 | Acc: 0.2541 | Perplexity: 8.5315\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 38/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.7996\n",
      "[quantity] Batch 51/300 | Loss: 1.9021\n",
      "[shape] Batch 101/300 | Loss: 1.8605\n",
      "[color] Batch 151/300 | Loss: 1.8233\n",
      "[quantity] Batch 201/300 | Loss: 1.8006\n",
      "[shape] Batch 251/300 | Loss: 1.8073\n",
      "[quantity] Batch 300/300 | Loss: 1.8773\n",
      "[Epoch 38] Train Loss: 1.8051 | Acc: 0.4489 | Perplexity: 6.0803\n",
      "[Epoch 38] Val Loss: 2.1657 | Acc: 0.2491 | Perplexity: 8.7211\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 39/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.7183\n",
      "[quantity] Batch 51/300 | Loss: 1.7790\n",
      "[shape] Batch 101/300 | Loss: 1.7871\n",
      "[color] Batch 151/300 | Loss: 1.7667\n",
      "[quantity] Batch 201/300 | Loss: 1.8336\n",
      "[shape] Batch 251/300 | Loss: 1.9187\n",
      "[quantity] Batch 300/300 | Loss: 1.8971\n",
      "[Epoch 39] Train Loss: 1.7966 | Acc: 0.4620 | Perplexity: 6.0294\n",
      "[Epoch 39] Val Loss: 2.1290 | Acc: 0.2530 | Perplexity: 8.4067\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 40/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.7608\n",
      "[quantity] Batch 51/300 | Loss: 1.8213\n",
      "[shape] Batch 101/300 | Loss: 1.8397\n",
      "[color] Batch 151/300 | Loss: 1.8289\n",
      "[quantity] Batch 201/300 | Loss: 1.7203\n",
      "[shape] Batch 251/300 | Loss: 1.8965\n",
      "[quantity] Batch 300/300 | Loss: 1.7701\n",
      "[Epoch 40] Train Loss: 1.7904 | Acc: 0.4656 | Perplexity: 5.9916\n",
      "[Epoch 40] Val Loss: 2.1711 | Acc: 0.2492 | Perplexity: 8.7680\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 41/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.7201\n",
      "[quantity] Batch 51/300 | Loss: 1.7396\n",
      "[shape] Batch 101/300 | Loss: 1.7795\n",
      "[color] Batch 151/300 | Loss: 1.8965\n",
      "[quantity] Batch 201/300 | Loss: 1.6936\n",
      "[shape] Batch 251/300 | Loss: 1.8596\n",
      "[quantity] Batch 300/300 | Loss: 1.7649\n",
      "[Epoch 41] Train Loss: 1.7792 | Acc: 0.4708 | Perplexity: 5.9251\n",
      "[Epoch 41] Val Loss: 2.1990 | Acc: 0.2503 | Perplexity: 9.0156\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 42/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.7050\n",
      "[quantity] Batch 51/300 | Loss: 1.6745\n",
      "[shape] Batch 101/300 | Loss: 1.6414\n",
      "[color] Batch 151/300 | Loss: 1.8069\n",
      "[quantity] Batch 201/300 | Loss: 1.8161\n",
      "[shape] Batch 251/300 | Loss: 1.8211\n",
      "[quantity] Batch 300/300 | Loss: 1.8005\n",
      "[Epoch 42] Train Loss: 1.7658 | Acc: 0.4810 | Perplexity: 5.8463\n",
      "[Epoch 42] Val Loss: 2.1746 | Acc: 0.2620 | Perplexity: 8.7985\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 43/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.7275\n",
      "[quantity] Batch 51/300 | Loss: 1.8416\n",
      "[shape] Batch 101/300 | Loss: 1.8062\n",
      "[color] Batch 151/300 | Loss: 1.7975\n",
      "[quantity] Batch 201/300 | Loss: 1.8708\n",
      "[shape] Batch 251/300 | Loss: 1.7087\n",
      "[quantity] Batch 300/300 | Loss: 1.7768\n",
      "[Epoch 43] Train Loss: 1.7646 | Acc: 0.4785 | Perplexity: 5.8394\n",
      "[Epoch 43] Val Loss: 2.2345 | Acc: 0.2569 | Perplexity: 9.3417\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 44/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.5916\n",
      "[quantity] Batch 51/300 | Loss: 1.7413\n",
      "[shape] Batch 101/300 | Loss: 1.6444\n",
      "[color] Batch 151/300 | Loss: 1.8215\n",
      "[quantity] Batch 201/300 | Loss: 1.8114\n",
      "[shape] Batch 251/300 | Loss: 1.7633\n",
      "[quantity] Batch 300/300 | Loss: 1.7144\n",
      "[Epoch 44] Train Loss: 1.7560 | Acc: 0.4869 | Perplexity: 5.7891\n",
      "[Epoch 44] Val Loss: 2.2156 | Acc: 0.2505 | Perplexity: 9.1665\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 45/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.6941\n",
      "[quantity] Batch 51/300 | Loss: 1.7940\n",
      "[shape] Batch 101/300 | Loss: 1.7467\n",
      "[color] Batch 151/300 | Loss: 1.8119\n",
      "[quantity] Batch 201/300 | Loss: 1.7334\n",
      "[shape] Batch 251/300 | Loss: 1.8654\n",
      "[quantity] Batch 300/300 | Loss: 1.7084\n",
      "[Epoch 45] Train Loss: 1.7450 | Acc: 0.5005 | Perplexity: 5.7258\n",
      "[Epoch 45] Val Loss: 2.2863 | Acc: 0.2550 | Perplexity: 9.8388\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 46/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.6954\n",
      "[quantity] Batch 51/300 | Loss: 1.6833\n",
      "[shape] Batch 101/300 | Loss: 1.6132\n",
      "[color] Batch 151/300 | Loss: 1.8078\n",
      "[quantity] Batch 201/300 | Loss: 1.7750\n",
      "[shape] Batch 251/300 | Loss: 1.8867\n",
      "[quantity] Batch 300/300 | Loss: 1.7524\n",
      "[Epoch 46] Train Loss: 1.7313 | Acc: 0.5095 | Perplexity: 5.6481\n",
      "[Epoch 46] Val Loss: 2.2663 | Acc: 0.2553 | Perplexity: 9.6435\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 47/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.6343\n",
      "[quantity] Batch 51/300 | Loss: 1.7716\n",
      "[shape] Batch 101/300 | Loss: 1.7942\n",
      "[color] Batch 151/300 | Loss: 1.8016\n",
      "[quantity] Batch 201/300 | Loss: 1.7472\n",
      "[shape] Batch 251/300 | Loss: 1.8856\n",
      "[quantity] Batch 300/300 | Loss: 1.8458\n",
      "[Epoch 47] Train Loss: 1.7228 | Acc: 0.5161 | Perplexity: 5.6003\n",
      "[Epoch 47] Val Loss: 2.2624 | Acc: 0.2552 | Perplexity: 9.6058\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 48/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.8474\n",
      "[quantity] Batch 51/300 | Loss: 1.7181\n",
      "[shape] Batch 101/300 | Loss: 1.7032\n",
      "[color] Batch 151/300 | Loss: 1.7204\n",
      "[quantity] Batch 201/300 | Loss: 1.6767\n",
      "[shape] Batch 251/300 | Loss: 1.5738\n",
      "[quantity] Batch 300/300 | Loss: 1.7500\n",
      "[Epoch 48] Train Loss: 1.7167 | Acc: 0.5184 | Perplexity: 5.5661\n",
      "[Epoch 48] Val Loss: 2.2747 | Acc: 0.2545 | Perplexity: 9.7249\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 49/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.5802\n",
      "[quantity] Batch 51/300 | Loss: 1.7196\n",
      "[shape] Batch 101/300 | Loss: 1.7137\n",
      "[color] Batch 151/300 | Loss: 1.8239\n",
      "[quantity] Batch 201/300 | Loss: 1.7429\n",
      "[shape] Batch 251/300 | Loss: 1.7976\n",
      "[quantity] Batch 300/300 | Loss: 1.6508\n",
      "[Epoch 49] Train Loss: 1.7084 | Acc: 0.5239 | Perplexity: 5.5200\n",
      "[Epoch 49] Val Loss: 2.2755 | Acc: 0.2519 | Perplexity: 9.7324\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 50/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.7116\n",
      "[quantity] Batch 51/300 | Loss: 1.8014\n",
      "[shape] Batch 101/300 | Loss: 1.7276\n",
      "[color] Batch 151/300 | Loss: 1.6902\n",
      "[quantity] Batch 201/300 | Loss: 1.6897\n",
      "[shape] Batch 251/300 | Loss: 1.6001\n",
      "[quantity] Batch 300/300 | Loss: 1.6946\n",
      "[Epoch 50] Train Loss: 1.6929 | Acc: 0.5321 | Perplexity: 5.4350\n",
      "[Epoch 50] Val Loss: 2.3188 | Acc: 0.2572 | Perplexity: 10.1640\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 51/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.7206\n",
      "[quantity] Batch 51/300 | Loss: 1.7676\n",
      "[shape] Batch 101/300 | Loss: 1.6805\n",
      "[color] Batch 151/300 | Loss: 1.8037\n",
      "[quantity] Batch 201/300 | Loss: 1.6921\n",
      "[shape] Batch 251/300 | Loss: 1.6140\n",
      "[quantity] Batch 300/300 | Loss: 1.6725\n",
      "[Epoch 51] Train Loss: 1.6924 | Acc: 0.5323 | Perplexity: 5.4324\n",
      "[Epoch 51] Val Loss: 2.3190 | Acc: 0.2550 | Perplexity: 10.1655\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 52/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.6622\n",
      "[quantity] Batch 51/300 | Loss: 1.8241\n",
      "[shape] Batch 101/300 | Loss: 1.6467\n",
      "[color] Batch 151/300 | Loss: 1.7366\n",
      "[quantity] Batch 201/300 | Loss: 1.5507\n",
      "[shape] Batch 251/300 | Loss: 1.7693\n",
      "[quantity] Batch 300/300 | Loss: 1.6559\n",
      "[Epoch 52] Train Loss: 1.6800 | Acc: 0.5424 | Perplexity: 5.3658\n",
      "[Epoch 52] Val Loss: 2.3100 | Acc: 0.2548 | Perplexity: 10.0749\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 53/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.6870\n",
      "[quantity] Batch 51/300 | Loss: 1.6631\n",
      "[shape] Batch 101/300 | Loss: 1.6704\n",
      "[color] Batch 151/300 | Loss: 1.5010\n",
      "[quantity] Batch 201/300 | Loss: 1.7691\n",
      "[shape] Batch 251/300 | Loss: 1.6699\n",
      "[quantity] Batch 300/300 | Loss: 1.7669\n",
      "[Epoch 53] Train Loss: 1.6722 | Acc: 0.5447 | Perplexity: 5.3239\n",
      "[Epoch 53] Val Loss: 2.2882 | Acc: 0.2534 | Perplexity: 9.8569\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 54/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.5528\n",
      "[quantity] Batch 51/300 | Loss: 1.5881\n",
      "[shape] Batch 101/300 | Loss: 1.7436\n",
      "[color] Batch 151/300 | Loss: 1.5625\n",
      "[quantity] Batch 201/300 | Loss: 1.6107\n",
      "[shape] Batch 251/300 | Loss: 1.7583\n",
      "[quantity] Batch 300/300 | Loss: 1.6821\n",
      "[Epoch 54] Train Loss: 1.6610 | Acc: 0.5523 | Perplexity: 5.2648\n",
      "[Epoch 54] Val Loss: 2.3523 | Acc: 0.2600 | Perplexity: 10.5095\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 55/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.5714\n",
      "[quantity] Batch 51/300 | Loss: 1.6033\n",
      "[shape] Batch 101/300 | Loss: 1.6652\n",
      "[color] Batch 151/300 | Loss: 1.6394\n",
      "[quantity] Batch 201/300 | Loss: 1.7065\n",
      "[shape] Batch 251/300 | Loss: 1.5759\n",
      "[quantity] Batch 300/300 | Loss: 1.7279\n",
      "[Epoch 55] Train Loss: 1.6609 | Acc: 0.5531 | Perplexity: 5.2640\n",
      "[Epoch 55] Val Loss: 2.3891 | Acc: 0.2522 | Perplexity: 10.9041\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 56/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.6121\n",
      "[quantity] Batch 51/300 | Loss: 1.5638\n",
      "[shape] Batch 101/300 | Loss: 1.6015\n",
      "[color] Batch 151/300 | Loss: 1.6552\n",
      "[quantity] Batch 201/300 | Loss: 1.5574\n",
      "[shape] Batch 251/300 | Loss: 1.6628\n",
      "[quantity] Batch 300/300 | Loss: 1.6176\n",
      "[Epoch 56] Train Loss: 1.6458 | Acc: 0.5624 | Perplexity: 5.1851\n",
      "[Epoch 56] Val Loss: 2.3620 | Acc: 0.2473 | Perplexity: 10.6120\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 57/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.5047\n",
      "[quantity] Batch 51/300 | Loss: 1.5673\n",
      "[shape] Batch 101/300 | Loss: 1.6643\n",
      "[color] Batch 151/300 | Loss: 1.5998\n",
      "[quantity] Batch 201/300 | Loss: 1.8323\n",
      "[shape] Batch 251/300 | Loss: 1.6361\n",
      "[quantity] Batch 300/300 | Loss: 1.6614\n",
      "[Epoch 57] Train Loss: 1.6390 | Acc: 0.5658 | Perplexity: 5.1500\n",
      "[Epoch 57] Val Loss: 2.3788 | Acc: 0.2456 | Perplexity: 10.7915\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 58/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.4422\n",
      "[quantity] Batch 51/300 | Loss: 1.6512\n",
      "[shape] Batch 101/300 | Loss: 1.7527\n",
      "[color] Batch 151/300 | Loss: 1.6632\n",
      "[quantity] Batch 201/300 | Loss: 1.5882\n",
      "[shape] Batch 251/300 | Loss: 1.6694\n",
      "[quantity] Batch 300/300 | Loss: 1.6735\n",
      "[Epoch 58] Train Loss: 1.6190 | Acc: 0.5785 | Perplexity: 5.0481\n",
      "[Epoch 58] Val Loss: 2.4193 | Acc: 0.2487 | Perplexity: 11.2375\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 59/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.5502\n",
      "[quantity] Batch 51/300 | Loss: 1.7080\n",
      "[shape] Batch 101/300 | Loss: 1.6425\n",
      "[color] Batch 151/300 | Loss: 1.6670\n",
      "[quantity] Batch 201/300 | Loss: 1.7204\n",
      "[shape] Batch 251/300 | Loss: 1.6964\n",
      "[quantity] Batch 300/300 | Loss: 1.6540\n",
      "[Epoch 59] Train Loss: 1.6195 | Acc: 0.5797 | Perplexity: 5.0506\n",
      "[Epoch 59] Val Loss: 2.4606 | Acc: 0.2445 | Perplexity: 11.7120\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 60/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.5562\n",
      "[quantity] Batch 51/300 | Loss: 1.5505\n",
      "[shape] Batch 101/300 | Loss: 1.6891\n",
      "[color] Batch 151/300 | Loss: 1.7220\n",
      "[quantity] Batch 201/300 | Loss: 1.6335\n",
      "[shape] Batch 251/300 | Loss: 1.5977\n",
      "[quantity] Batch 300/300 | Loss: 1.5710\n",
      "[Epoch 60] Train Loss: 1.6113 | Acc: 0.5852 | Perplexity: 5.0093\n",
      "[Epoch 60] Val Loss: 2.4149 | Acc: 0.2587 | Perplexity: 11.1890\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 61/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.5370\n",
      "[quantity] Batch 51/300 | Loss: 1.6316\n",
      "[shape] Batch 101/300 | Loss: 1.6342\n",
      "[color] Batch 151/300 | Loss: 1.4431\n",
      "[quantity] Batch 201/300 | Loss: 1.7627\n",
      "[shape] Batch 251/300 | Loss: 1.6445\n",
      "[quantity] Batch 300/300 | Loss: 1.4938\n",
      "[Epoch 61] Train Loss: 1.6001 | Acc: 0.5923 | Perplexity: 4.9536\n",
      "[Epoch 61] Val Loss: 2.4942 | Acc: 0.2548 | Perplexity: 12.1126\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 62/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.5568\n",
      "[quantity] Batch 51/300 | Loss: 1.7181\n",
      "[shape] Batch 101/300 | Loss: 1.5724\n",
      "[color] Batch 151/300 | Loss: 1.5548\n",
      "[quantity] Batch 201/300 | Loss: 1.5722\n",
      "[shape] Batch 251/300 | Loss: 1.6005\n",
      "[quantity] Batch 300/300 | Loss: 1.5464\n",
      "[Epoch 62] Train Loss: 1.5946 | Acc: 0.5930 | Perplexity: 4.9264\n",
      "[Epoch 62] Val Loss: 2.4761 | Acc: 0.2619 | Perplexity: 11.8944\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 63/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.6434\n",
      "[quantity] Batch 51/300 | Loss: 1.6697\n",
      "[shape] Batch 101/300 | Loss: 1.6192\n",
      "[color] Batch 151/300 | Loss: 1.5977\n",
      "[quantity] Batch 201/300 | Loss: 1.6072\n",
      "[shape] Batch 251/300 | Loss: 1.5727\n",
      "[quantity] Batch 300/300 | Loss: 1.5590\n",
      "[Epoch 63] Train Loss: 1.5821 | Acc: 0.5972 | Perplexity: 4.8653\n",
      "[Epoch 63] Val Loss: 2.4870 | Acc: 0.2520 | Perplexity: 12.0251\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 64/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.6187\n",
      "[quantity] Batch 51/300 | Loss: 1.4356\n",
      "[shape] Batch 101/300 | Loss: 1.5693\n",
      "[color] Batch 151/300 | Loss: 1.5939\n",
      "[quantity] Batch 201/300 | Loss: 1.6548\n",
      "[shape] Batch 251/300 | Loss: 1.7403\n",
      "[quantity] Batch 300/300 | Loss: 1.6386\n",
      "[Epoch 64] Train Loss: 1.5780 | Acc: 0.6049 | Perplexity: 4.8450\n",
      "[Epoch 64] Val Loss: 2.5297 | Acc: 0.2522 | Perplexity: 12.5495\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 65/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.6406\n",
      "[quantity] Batch 51/300 | Loss: 1.6072\n",
      "[shape] Batch 101/300 | Loss: 1.5831\n",
      "[color] Batch 151/300 | Loss: 1.6496\n",
      "[quantity] Batch 201/300 | Loss: 1.4758\n",
      "[shape] Batch 251/300 | Loss: 1.5697\n",
      "[quantity] Batch 300/300 | Loss: 1.6198\n",
      "[Epoch 65] Train Loss: 1.5697 | Acc: 0.6101 | Perplexity: 4.8053\n",
      "[Epoch 65] Val Loss: 2.5224 | Acc: 0.2505 | Perplexity: 12.4587\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 66/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.5706\n",
      "[quantity] Batch 51/300 | Loss: 1.6131\n",
      "[shape] Batch 101/300 | Loss: 1.5102\n",
      "[color] Batch 151/300 | Loss: 1.5377\n",
      "[quantity] Batch 201/300 | Loss: 1.5255\n",
      "[shape] Batch 251/300 | Loss: 1.4957\n",
      "[quantity] Batch 300/300 | Loss: 1.7187\n",
      "[Epoch 66] Train Loss: 1.5569 | Acc: 0.6119 | Perplexity: 4.7442\n",
      "[Epoch 66] Val Loss: 2.5336 | Acc: 0.2608 | Perplexity: 12.5984\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 67/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.4142\n",
      "[quantity] Batch 51/300 | Loss: 1.5950\n",
      "[shape] Batch 101/300 | Loss: 1.6454\n",
      "[color] Batch 151/300 | Loss: 1.6029\n",
      "[quantity] Batch 201/300 | Loss: 1.5093\n",
      "[shape] Batch 251/300 | Loss: 1.5079\n",
      "[quantity] Batch 300/300 | Loss: 1.6916\n",
      "[Epoch 67] Train Loss: 1.5525 | Acc: 0.6171 | Perplexity: 4.7231\n",
      "[Epoch 67] Val Loss: 2.5573 | Acc: 0.2503 | Perplexity: 12.9003\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 68/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.4373\n",
      "[quantity] Batch 51/300 | Loss: 1.5458\n",
      "[shape] Batch 101/300 | Loss: 1.2963\n",
      "[color] Batch 151/300 | Loss: 1.5390\n",
      "[quantity] Batch 201/300 | Loss: 1.6562\n",
      "[shape] Batch 251/300 | Loss: 1.5770\n",
      "[quantity] Batch 300/300 | Loss: 1.5322\n",
      "[Epoch 68] Train Loss: 1.5375 | Acc: 0.6247 | Perplexity: 4.6529\n",
      "[Epoch 68] Val Loss: 2.5109 | Acc: 0.2533 | Perplexity: 12.3158\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 69/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.4061\n",
      "[quantity] Batch 51/300 | Loss: 1.3933\n",
      "[shape] Batch 101/300 | Loss: 1.4316\n",
      "[color] Batch 151/300 | Loss: 1.4735\n",
      "[quantity] Batch 201/300 | Loss: 1.6982\n",
      "[shape] Batch 251/300 | Loss: 1.5130\n",
      "[quantity] Batch 300/300 | Loss: 1.6031\n",
      "[Epoch 69] Train Loss: 1.5376 | Acc: 0.6243 | Perplexity: 4.6532\n",
      "[Epoch 69] Val Loss: 2.5437 | Acc: 0.2512 | Perplexity: 12.7262\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 70/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.4530\n",
      "[quantity] Batch 51/300 | Loss: 1.6231\n",
      "[shape] Batch 101/300 | Loss: 1.6160\n",
      "[color] Batch 151/300 | Loss: 1.5547\n",
      "[quantity] Batch 201/300 | Loss: 1.5356\n",
      "[shape] Batch 251/300 | Loss: 1.4547\n",
      "[quantity] Batch 300/300 | Loss: 1.5224\n",
      "[Epoch 70] Train Loss: 1.5235 | Acc: 0.6349 | Perplexity: 4.5881\n",
      "[Epoch 70] Val Loss: 2.6280 | Acc: 0.2492 | Perplexity: 13.8461\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 71/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.4786\n",
      "[quantity] Batch 51/300 | Loss: 1.5044\n",
      "[shape] Batch 101/300 | Loss: 1.5050\n",
      "[color] Batch 151/300 | Loss: 1.3946\n",
      "[quantity] Batch 201/300 | Loss: 1.6095\n",
      "[shape] Batch 251/300 | Loss: 1.5631\n",
      "[quantity] Batch 300/300 | Loss: 1.6604\n",
      "[Epoch 71] Train Loss: 1.5143 | Acc: 0.6383 | Perplexity: 4.5461\n",
      "[Epoch 71] Val Loss: 2.5699 | Acc: 0.2458 | Perplexity: 13.0651\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 72/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.4731\n",
      "[quantity] Batch 51/300 | Loss: 1.4242\n",
      "[shape] Batch 101/300 | Loss: 1.6854\n",
      "[color] Batch 151/300 | Loss: 1.3820\n",
      "[quantity] Batch 201/300 | Loss: 1.6099\n",
      "[shape] Batch 251/300 | Loss: 1.5309\n",
      "[quantity] Batch 300/300 | Loss: 1.2894\n",
      "[Epoch 72] Train Loss: 1.5016 | Acc: 0.6426 | Perplexity: 4.4888\n",
      "[Epoch 72] Val Loss: 2.6137 | Acc: 0.2523 | Perplexity: 13.6498\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 73/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.4437\n",
      "[quantity] Batch 51/300 | Loss: 1.5270\n",
      "[shape] Batch 101/300 | Loss: 1.4689\n",
      "[color] Batch 151/300 | Loss: 1.5963\n",
      "[quantity] Batch 201/300 | Loss: 1.5781\n",
      "[shape] Batch 251/300 | Loss: 1.6565\n",
      "[quantity] Batch 300/300 | Loss: 1.4490\n",
      "[Epoch 73] Train Loss: 1.4987 | Acc: 0.6473 | Perplexity: 4.4760\n",
      "[Epoch 73] Val Loss: 2.6818 | Acc: 0.2511 | Perplexity: 14.6119\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 74/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.4364\n",
      "[quantity] Batch 51/300 | Loss: 1.4497\n",
      "[shape] Batch 101/300 | Loss: 1.3989\n",
      "[color] Batch 151/300 | Loss: 1.3941\n",
      "[quantity] Batch 201/300 | Loss: 1.4622\n",
      "[shape] Batch 251/300 | Loss: 1.4578\n",
      "[quantity] Batch 300/300 | Loss: 1.4525\n",
      "[Epoch 74] Train Loss: 1.4987 | Acc: 0.6461 | Perplexity: 4.4758\n",
      "[Epoch 74] Val Loss: 2.5817 | Acc: 0.2442 | Perplexity: 13.2190\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 75/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.5211\n",
      "[quantity] Batch 51/300 | Loss: 1.4477\n",
      "[shape] Batch 101/300 | Loss: 1.2694\n",
      "[color] Batch 151/300 | Loss: 1.4229\n",
      "[quantity] Batch 201/300 | Loss: 1.5800\n",
      "[shape] Batch 251/300 | Loss: 1.5667\n",
      "[quantity] Batch 300/300 | Loss: 1.4598\n",
      "[Epoch 75] Train Loss: 1.4780 | Acc: 0.6617 | Perplexity: 4.3841\n",
      "[Epoch 75] Val Loss: 2.6607 | Acc: 0.2512 | Perplexity: 14.3065\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 76/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.4333\n",
      "[quantity] Batch 51/300 | Loss: 1.4926\n",
      "[shape] Batch 101/300 | Loss: 1.4803\n",
      "[color] Batch 151/300 | Loss: 1.4054\n",
      "[quantity] Batch 201/300 | Loss: 1.4721\n",
      "[shape] Batch 251/300 | Loss: 1.4726\n",
      "[quantity] Batch 300/300 | Loss: 1.5185\n",
      "[Epoch 76] Train Loss: 1.4721 | Acc: 0.6642 | Perplexity: 4.3585\n",
      "[Epoch 76] Val Loss: 2.6429 | Acc: 0.2472 | Perplexity: 14.0541\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 77/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.3254\n",
      "[quantity] Batch 51/300 | Loss: 1.3302\n",
      "[shape] Batch 101/300 | Loss: 1.3041\n",
      "[color] Batch 151/300 | Loss: 1.3843\n",
      "[quantity] Batch 201/300 | Loss: 1.4345\n",
      "[shape] Batch 251/300 | Loss: 1.5004\n",
      "[quantity] Batch 300/300 | Loss: 1.4957\n",
      "[Epoch 77] Train Loss: 1.4621 | Acc: 0.6642 | Perplexity: 4.3150\n",
      "[Epoch 77] Val Loss: 2.7118 | Acc: 0.2548 | Perplexity: 15.0570\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 78/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.2681\n",
      "[quantity] Batch 51/300 | Loss: 1.4036\n",
      "[shape] Batch 101/300 | Loss: 1.4771\n",
      "[color] Batch 151/300 | Loss: 1.5077\n",
      "[quantity] Batch 201/300 | Loss: 1.3580\n",
      "[shape] Batch 251/300 | Loss: 1.4733\n",
      "[quantity] Batch 300/300 | Loss: 1.4357\n",
      "[Epoch 78] Train Loss: 1.4474 | Acc: 0.6777 | Perplexity: 4.2519\n",
      "[Epoch 78] Val Loss: 2.7059 | Acc: 0.2477 | Perplexity: 14.9675\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 79/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.3597\n",
      "[quantity] Batch 51/300 | Loss: 1.3856\n",
      "[shape] Batch 101/300 | Loss: 1.3507\n",
      "[color] Batch 151/300 | Loss: 1.5046\n",
      "[quantity] Batch 201/300 | Loss: 1.2996\n",
      "[shape] Batch 251/300 | Loss: 1.3744\n",
      "[quantity] Batch 300/300 | Loss: 1.5314\n",
      "[Epoch 79] Train Loss: 1.4413 | Acc: 0.6770 | Perplexity: 4.2263\n",
      "[Epoch 79] Val Loss: 2.7223 | Acc: 0.2531 | Perplexity: 15.2155\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 80/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.5069\n",
      "[quantity] Batch 51/300 | Loss: 1.5490\n",
      "[shape] Batch 101/300 | Loss: 1.4819\n",
      "[color] Batch 151/300 | Loss: 1.4678\n",
      "[quantity] Batch 201/300 | Loss: 1.3210\n",
      "[shape] Batch 251/300 | Loss: 1.4643\n",
      "[quantity] Batch 300/300 | Loss: 1.4115\n",
      "[Epoch 80] Train Loss: 1.4394 | Acc: 0.6778 | Perplexity: 4.2180\n",
      "[Epoch 80] Val Loss: 2.7663 | Acc: 0.2478 | Perplexity: 15.9002\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 81/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.4543\n",
      "[quantity] Batch 51/300 | Loss: 1.4893\n",
      "[shape] Batch 101/300 | Loss: 1.4627\n",
      "[color] Batch 151/300 | Loss: 1.5630\n",
      "[quantity] Batch 201/300 | Loss: 1.5299\n",
      "[shape] Batch 251/300 | Loss: 1.4785\n",
      "[quantity] Batch 300/300 | Loss: 1.4008\n",
      "[Epoch 81] Train Loss: 1.4316 | Acc: 0.6817 | Perplexity: 4.1855\n",
      "[Epoch 81] Val Loss: 2.8017 | Acc: 0.2487 | Perplexity: 16.4732\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 82/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.3377\n",
      "[quantity] Batch 51/300 | Loss: 1.5448\n",
      "[shape] Batch 101/300 | Loss: 1.3616\n",
      "[color] Batch 151/300 | Loss: 1.3788\n",
      "[quantity] Batch 201/300 | Loss: 1.4699\n",
      "[shape] Batch 251/300 | Loss: 1.4375\n",
      "[quantity] Batch 300/300 | Loss: 1.3065\n",
      "[Epoch 82] Train Loss: 1.4242 | Acc: 0.6885 | Perplexity: 4.1546\n",
      "[Epoch 82] Val Loss: 2.7947 | Acc: 0.2492 | Perplexity: 16.3571\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 83/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.4152\n",
      "[quantity] Batch 51/300 | Loss: 1.4508\n",
      "[shape] Batch 101/300 | Loss: 1.4613\n",
      "[color] Batch 151/300 | Loss: 1.2605\n",
      "[quantity] Batch 201/300 | Loss: 1.4180\n",
      "[shape] Batch 251/300 | Loss: 1.5032\n",
      "[quantity] Batch 300/300 | Loss: 1.3430\n",
      "[Epoch 83] Train Loss: 1.4215 | Acc: 0.6930 | Perplexity: 4.1432\n",
      "[Epoch 83] Val Loss: 2.7840 | Acc: 0.2525 | Perplexity: 16.1842\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 84/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.4562\n",
      "[quantity] Batch 51/300 | Loss: 1.3478\n",
      "[shape] Batch 101/300 | Loss: 1.2877\n",
      "[color] Batch 151/300 | Loss: 1.5606\n",
      "[quantity] Batch 201/300 | Loss: 1.4529\n",
      "[shape] Batch 251/300 | Loss: 1.3453\n",
      "[quantity] Batch 300/300 | Loss: 1.4613\n",
      "[Epoch 84] Train Loss: 1.4082 | Acc: 0.6909 | Perplexity: 4.0885\n",
      "[Epoch 84] Val Loss: 2.8562 | Acc: 0.2523 | Perplexity: 17.3947\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 85/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.4035\n",
      "[quantity] Batch 51/300 | Loss: 1.3758\n",
      "[shape] Batch 101/300 | Loss: 1.3766\n",
      "[color] Batch 151/300 | Loss: 1.4302\n",
      "[quantity] Batch 201/300 | Loss: 1.4754\n",
      "[shape] Batch 251/300 | Loss: 1.4062\n",
      "[quantity] Batch 300/300 | Loss: 1.3735\n",
      "[Epoch 85] Train Loss: 1.4003 | Acc: 0.6997 | Perplexity: 4.0563\n",
      "[Epoch 85] Val Loss: 2.8431 | Acc: 0.2536 | Perplexity: 17.1696\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 86/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.4601\n",
      "[quantity] Batch 51/300 | Loss: 1.3960\n",
      "[shape] Batch 101/300 | Loss: 1.3648\n",
      "[color] Batch 151/300 | Loss: 1.4071\n",
      "[quantity] Batch 201/300 | Loss: 1.4042\n",
      "[shape] Batch 251/300 | Loss: 1.4041\n",
      "[quantity] Batch 300/300 | Loss: 1.2973\n",
      "[Epoch 86] Train Loss: 1.3918 | Acc: 0.7047 | Perplexity: 4.0221\n",
      "[Epoch 86] Val Loss: 2.8551 | Acc: 0.2534 | Perplexity: 17.3765\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 87/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.3637\n",
      "[quantity] Batch 51/300 | Loss: 1.2863\n",
      "[shape] Batch 101/300 | Loss: 1.3932\n",
      "[color] Batch 151/300 | Loss: 1.4359\n",
      "[quantity] Batch 201/300 | Loss: 1.2171\n",
      "[shape] Batch 251/300 | Loss: 1.3203\n",
      "[quantity] Batch 300/300 | Loss: 1.3901\n",
      "[Epoch 87] Train Loss: 1.3830 | Acc: 0.7071 | Perplexity: 3.9869\n",
      "[Epoch 87] Val Loss: 2.8497 | Acc: 0.2495 | Perplexity: 17.2828\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 88/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.2281\n",
      "[quantity] Batch 51/300 | Loss: 1.3891\n",
      "[shape] Batch 101/300 | Loss: 1.2883\n",
      "[color] Batch 151/300 | Loss: 1.4241\n",
      "[quantity] Batch 201/300 | Loss: 1.4372\n",
      "[shape] Batch 251/300 | Loss: 1.4705\n",
      "[quantity] Batch 300/300 | Loss: 1.4690\n",
      "[Epoch 88] Train Loss: 1.3764 | Acc: 0.7099 | Perplexity: 3.9605\n",
      "[Epoch 88] Val Loss: 2.8728 | Acc: 0.2487 | Perplexity: 17.6872\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 89/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.4440\n",
      "[quantity] Batch 51/300 | Loss: 1.3108\n",
      "[shape] Batch 101/300 | Loss: 1.1831\n",
      "[color] Batch 151/300 | Loss: 1.2963\n",
      "[quantity] Batch 201/300 | Loss: 1.3384\n",
      "[shape] Batch 251/300 | Loss: 1.4143\n",
      "[quantity] Batch 300/300 | Loss: 1.3927\n",
      "[Epoch 89] Train Loss: 1.3772 | Acc: 0.7104 | Perplexity: 3.9640\n",
      "[Epoch 89] Val Loss: 2.8977 | Acc: 0.2517 | Perplexity: 18.1332\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 90/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.3773\n",
      "[quantity] Batch 51/300 | Loss: 1.4037\n",
      "[shape] Batch 101/300 | Loss: 1.3550\n",
      "[color] Batch 151/300 | Loss: 1.2947\n",
      "[quantity] Batch 201/300 | Loss: 1.3033\n",
      "[shape] Batch 251/300 | Loss: 1.3131\n",
      "[quantity] Batch 300/300 | Loss: 1.3780\n",
      "[Epoch 90] Train Loss: 1.3646 | Acc: 0.7184 | Perplexity: 3.9143\n",
      "[Epoch 90] Val Loss: 2.8741 | Acc: 0.2467 | Perplexity: 17.7095\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 91/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.2227\n",
      "[quantity] Batch 51/300 | Loss: 1.3873\n",
      "[shape] Batch 101/300 | Loss: 1.3036\n",
      "[color] Batch 151/300 | Loss: 1.2586\n",
      "[quantity] Batch 201/300 | Loss: 1.2946\n",
      "[shape] Batch 251/300 | Loss: 1.1854\n",
      "[quantity] Batch 300/300 | Loss: 1.4342\n",
      "[Epoch 91] Train Loss: 1.3500 | Acc: 0.7273 | Perplexity: 3.8573\n",
      "[Epoch 91] Val Loss: 2.8223 | Acc: 0.2547 | Perplexity: 16.8155\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 92/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.3260\n",
      "[quantity] Batch 51/300 | Loss: 1.2514\n",
      "[shape] Batch 101/300 | Loss: 1.3293\n",
      "[color] Batch 151/300 | Loss: 1.3451\n",
      "[quantity] Batch 201/300 | Loss: 1.2747\n",
      "[shape] Batch 251/300 | Loss: 1.4473\n",
      "[quantity] Batch 300/300 | Loss: 1.2480\n",
      "[Epoch 92] Train Loss: 1.3498 | Acc: 0.7254 | Perplexity: 3.8568\n",
      "[Epoch 92] Val Loss: 2.9321 | Acc: 0.2505 | Perplexity: 18.7679\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 93/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.3055\n",
      "[quantity] Batch 51/300 | Loss: 1.2310\n",
      "[shape] Batch 101/300 | Loss: 1.3474\n",
      "[color] Batch 151/300 | Loss: 1.3781\n",
      "[quantity] Batch 201/300 | Loss: 1.2683\n",
      "[shape] Batch 251/300 | Loss: 1.1432\n",
      "[quantity] Batch 300/300 | Loss: 1.3993\n",
      "[Epoch 93] Train Loss: 1.3468 | Acc: 0.7274 | Perplexity: 3.8450\n",
      "[Epoch 93] Val Loss: 2.8116 | Acc: 0.2514 | Perplexity: 16.6367\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 94/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.1459\n",
      "[quantity] Batch 51/300 | Loss: 1.4072\n",
      "[shape] Batch 101/300 | Loss: 1.5003\n",
      "[color] Batch 151/300 | Loss: 1.2727\n",
      "[quantity] Batch 201/300 | Loss: 1.3670\n",
      "[shape] Batch 251/300 | Loss: 1.3469\n",
      "[quantity] Batch 300/300 | Loss: 1.4252\n",
      "[Epoch 94] Train Loss: 1.3312 | Acc: 0.7348 | Perplexity: 3.7855\n",
      "[Epoch 94] Val Loss: 2.9219 | Acc: 0.2519 | Perplexity: 18.5765\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 95/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.3586\n",
      "[quantity] Batch 51/300 | Loss: 1.2320\n",
      "[shape] Batch 101/300 | Loss: 1.5669\n",
      "[color] Batch 151/300 | Loss: 1.3630\n",
      "[quantity] Batch 201/300 | Loss: 1.4131\n",
      "[shape] Batch 251/300 | Loss: 1.3073\n",
      "[quantity] Batch 300/300 | Loss: 1.4112\n",
      "[Epoch 95] Train Loss: 1.3248 | Acc: 0.7356 | Perplexity: 3.7614\n",
      "[Epoch 95] Val Loss: 2.9264 | Acc: 0.2517 | Perplexity: 18.6596\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 96/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.3369\n",
      "[quantity] Batch 51/300 | Loss: 1.4330\n",
      "[shape] Batch 101/300 | Loss: 1.2242\n",
      "[color] Batch 151/300 | Loss: 1.3828\n",
      "[quantity] Batch 201/300 | Loss: 1.4618\n",
      "[shape] Batch 251/300 | Loss: 1.2478\n",
      "[quantity] Batch 300/300 | Loss: 1.2536\n",
      "[Epoch 96] Train Loss: 1.3269 | Acc: 0.7375 | Perplexity: 3.7695\n",
      "[Epoch 96] Val Loss: 2.9299 | Acc: 0.2514 | Perplexity: 18.7255\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 97/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.3062\n",
      "[quantity] Batch 51/300 | Loss: 1.2552\n",
      "[shape] Batch 101/300 | Loss: 1.3215\n",
      "[color] Batch 151/300 | Loss: 1.3796\n",
      "[quantity] Batch 201/300 | Loss: 1.5187\n",
      "[shape] Batch 251/300 | Loss: 1.2834\n",
      "[quantity] Batch 300/300 | Loss: 1.4773\n",
      "[Epoch 97] Train Loss: 1.3096 | Acc: 0.7441 | Perplexity: 3.7049\n",
      "[Epoch 97] Val Loss: 2.9222 | Acc: 0.2520 | Perplexity: 18.5818\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 98/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.1082\n",
      "[quantity] Batch 51/300 | Loss: 1.2290\n",
      "[shape] Batch 101/300 | Loss: 1.3002\n",
      "[color] Batch 151/300 | Loss: 1.3156\n",
      "[quantity] Batch 201/300 | Loss: 1.3759\n",
      "[shape] Batch 251/300 | Loss: 1.3730\n",
      "[quantity] Batch 300/300 | Loss: 1.2346\n",
      "[Epoch 98] Train Loss: 1.3001 | Acc: 0.7496 | Perplexity: 3.6695\n",
      "[Epoch 98] Val Loss: 3.0720 | Acc: 0.2489 | Perplexity: 21.5840\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 99/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.2905\n",
      "[quantity] Batch 51/300 | Loss: 1.3186\n",
      "[shape] Batch 101/300 | Loss: 1.2995\n",
      "[color] Batch 151/300 | Loss: 1.3146\n",
      "[quantity] Batch 201/300 | Loss: 1.2927\n",
      "[shape] Batch 251/300 | Loss: 1.4159\n",
      "[quantity] Batch 300/300 | Loss: 1.2829\n",
      "[Epoch 99] Train Loss: 1.2901 | Acc: 0.7541 | Perplexity: 3.6333\n",
      "[Epoch 99] Val Loss: 3.0789 | Acc: 0.2517 | Perplexity: 21.7339\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 100/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.3146\n",
      "[quantity] Batch 51/300 | Loss: 1.2523\n",
      "[shape] Batch 101/300 | Loss: 1.3978\n",
      "[color] Batch 151/300 | Loss: 1.3991\n",
      "[quantity] Batch 201/300 | Loss: 1.2483\n",
      "[shape] Batch 251/300 | Loss: 1.2807\n",
      "[quantity] Batch 300/300 | Loss: 1.3127\n",
      "[Epoch 100] Train Loss: 1.2969 | Acc: 0.7535 | Perplexity: 3.6581\n",
      "[Epoch 100] Val Loss: 3.0064 | Acc: 0.2508 | Perplexity: 20.2152\n",
      "------------------------------------------------------------\n",
      "\n",
      " Training complete (Round-Robin Mode)\n"
     ]
    }
   ],
   "source": [
    "criterion =  nn.CrossEntropyLoss(label_smoothing=LABEL_SMOOTHING)\n",
    "optimizer = optim.AdamW(transformer.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=WARMUP_STEPS)  \n",
    "\n",
    "results = train_model_round_robin(\n",
    "    train_loaders, validation_loader, transformer, criterion, \n",
    "    optimizer, max_epochs=MAX_EPOCHS, device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917a1fa4",
   "metadata": {},
   "source": [
    "### 3. Test Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7d144441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 3.0553 | Test Acc: 0.2467 | Test Perplexity: 21.2285\n"
     ]
    }
   ],
   "source": [
    "results = test_model(test_loader, transformer, criterion, device)\n",
    "print(f\"Test Loss: {results[\"test_loss\"]:.4f} | Test Acc: {results[\"test_acc\"]:.4f} | Test Perplexity: {results[\"test_perplexity\"]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28dac7b",
   "metadata": {},
   "source": [
    "## Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c3c7ecc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.wcst import WCST\n",
    "wcst = WCST(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b590910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inference(model: Transformer, source_sequence, start_tokens):\n",
    "    model.eval()\n",
    "    generated = start_tokens\n",
    "    \n",
    "    with T.no_grad():\n",
    "        logits = model(source_sequence, generated)\n",
    "    \n",
    "    # Greedy Selection\n",
    "    next_token = T.argmax(logits[:, -1, :], dim=-1, keepdim=True)\n",
    "\n",
    "    generated = T.cat([generated, next_token], dim=1)\n",
    "\n",
    "    return generated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7f458678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Actual Trials\n",
      "[array(['blue', 'cross', '1'], dtype='<U6'), array(['blue', 'cross', '3'], dtype='<U6'), array(['green', 'star', '4'], dtype='<U6'), array(['yellow', 'square', '2'], dtype='<U6'), array(['yellow', 'circle', '4'], dtype='<U6'), 'SEP', 'C3', 'EOS', array(['yellow', 'star', '2'], dtype='<U6'), 'SEP', 'C4']\n",
      "[array(['blue', 'circle', '4'], dtype='<U6'), array(['blue', 'star', '3'], dtype='<U6'), array(['yellow', 'circle', '2'], dtype='<U6'), array(['green', 'circle', '1'], dtype='<U6'), array(['green', 'star', '4'], dtype='<U6'), 'SEP', 'C1', 'EOS', array(['blue', 'star', '1'], dtype='<U6'), 'SEP', 'C4']\n",
      "[array(['blue', 'cross', '2'], dtype='<U6'), array(['yellow', 'circle', '1'], dtype='<U6'), array(['red', 'cross', '3'], dtype='<U6'), array(['green', 'circle', '4'], dtype='<U6'), array(['green', 'star', '3'], dtype='<U6'), 'SEP', 'C3', 'EOS', array(['blue', 'circle', '3'], dtype='<U6'), 'SEP', 'C3']\n",
      "[array(['yellow', 'circle', '1'], dtype='<U6'), array(['red', 'square', '3'], dtype='<U6'), array(['blue', 'square', '4'], dtype='<U6'), array(['green', 'circle', '2'], dtype='<U6'), array(['yellow', 'square', '2'], dtype='<U6'), 'SEP', 'C4', 'EOS', array(['yellow', 'square', '3'], dtype='<U6'), 'SEP', 'C2']\n",
      "[array(['blue', 'star', '2'], dtype='<U6'), array(['blue', 'circle', '4'], dtype='<U6'), array(['green', 'square', '3'], dtype='<U6'), array(['yellow', 'cross', '1'], dtype='<U6'), array(['yellow', 'star', '2'], dtype='<U6'), 'SEP', 'C1', 'EOS', array(['blue', 'star', '4'], dtype='<U6'), 'SEP', 'C2']\n",
      "[array(['blue', 'square', '1'], dtype='<U6'), array(['yellow', 'star', '4'], dtype='<U6'), array(['blue', 'star', '2'], dtype='<U6'), array(['yellow', 'cross', '3'], dtype='<U6'), array(['blue', 'circle', '2'], dtype='<U6'), 'SEP', 'C3', 'EOS', array(['yellow', 'circle', '3'], dtype='<U6'), 'SEP', 'C4']\n",
      "[array(['green', 'star', '4'], dtype='<U6'), array(['green', 'square', '3'], dtype='<U6'), array(['blue', 'star', '2'], dtype='<U6'), array(['green', 'circle', '1'], dtype='<U6'), array(['green', 'circle', '4'], dtype='<U6'), 'SEP', 'C1', 'EOS', array(['blue', 'cross', '1'], dtype='<U6'), 'SEP', 'C4']\n",
      "[array(['green', 'star', '3'], dtype='<U6'), array(['green', 'square', '2'], dtype='<U6'), array(['red', 'cross', '1'], dtype='<U6'), array(['yellow', 'cross', '4'], dtype='<U6'), array(['blue', 'star', '4'], dtype='<U6'), 'SEP', 'C4', 'EOS', array(['red', 'star', '1'], dtype='<U6'), 'SEP', 'C3']\n",
      "[array(['blue', 'star', '4'], dtype='<U6'), array(['green', 'circle', '1'], dtype='<U6'), array(['red', 'star', '3'], dtype='<U6'), array(['red', 'cross', '2'], dtype='<U6'), array(['green', 'circle', '3'], dtype='<U6'), 'SEP', 'C3', 'EOS', array(['yellow', 'circle', '1'], dtype='<U6'), 'SEP', 'C2']\n",
      "[array(['red', 'star', '4'], dtype='<U6'), array(['green', 'circle', '2'], dtype='<U6'), array(['green', 'square', '3'], dtype='<U6'), array(['red', 'square', '1'], dtype='<U6'), array(['red', 'circle', '1'], dtype='<U6'), 'SEP', 'C4', 'EOS', array(['yellow', 'cross', '2'], dtype='<U6'), 'SEP', 'C2']\n",
      "Feature for Classification:  0 \n",
      "\n",
      "# Predicted Trials\n",
      "[array(['blue', 'cross', '1'], dtype='<U6'), array(['blue', 'cross', '3'], dtype='<U6'), array(['green', 'star', '4'], dtype='<U6'), array(['yellow', 'square', '2'], dtype='<U6'), array(['yellow', 'circle', '4'], dtype='<U6'), 'SEP', 'C3', 'EOS', array(['yellow', 'star', '2'], dtype='<U6'), 'SEP', array(['yellow', 'star', '2'], dtype='<U6'), 'SEP', 'C1']\n",
      "[array(['blue', 'circle', '4'], dtype='<U6'), array(['blue', 'star', '3'], dtype='<U6'), array(['yellow', 'circle', '2'], dtype='<U6'), array(['green', 'circle', '1'], dtype='<U6'), array(['green', 'star', '4'], dtype='<U6'), 'SEP', 'C1', 'EOS', array(['blue', 'star', '1'], dtype='<U6'), 'SEP', array(['blue', 'star', '1'], dtype='<U6'), 'SEP', 'C2']\n",
      "[array(['blue', 'cross', '2'], dtype='<U6'), array(['yellow', 'circle', '1'], dtype='<U6'), array(['red', 'cross', '3'], dtype='<U6'), array(['green', 'circle', '4'], dtype='<U6'), array(['green', 'star', '3'], dtype='<U6'), 'SEP', 'C3', 'EOS', array(['blue', 'circle', '3'], dtype='<U6'), 'SEP', array(['blue', 'circle', '3'], dtype='<U6'), 'SEP', 'C3']\n",
      "[array(['yellow', 'circle', '1'], dtype='<U6'), array(['red', 'square', '3'], dtype='<U6'), array(['blue', 'square', '4'], dtype='<U6'), array(['green', 'circle', '2'], dtype='<U6'), array(['yellow', 'square', '2'], dtype='<U6'), 'SEP', 'C4', 'EOS', array(['yellow', 'square', '3'], dtype='<U6'), 'SEP', array(['yellow', 'square', '3'], dtype='<U6'), 'SEP', 'C2']\n",
      "[array(['blue', 'star', '2'], dtype='<U6'), array(['blue', 'circle', '4'], dtype='<U6'), array(['green', 'square', '3'], dtype='<U6'), array(['yellow', 'cross', '1'], dtype='<U6'), array(['yellow', 'star', '2'], dtype='<U6'), 'SEP', 'C1', 'EOS', array(['blue', 'star', '4'], dtype='<U6'), 'SEP', array(['blue', 'star', '4'], dtype='<U6'), 'SEP', 'C2']\n",
      "[array(['blue', 'square', '1'], dtype='<U6'), array(['yellow', 'star', '4'], dtype='<U6'), array(['blue', 'star', '2'], dtype='<U6'), array(['yellow', 'cross', '3'], dtype='<U6'), array(['blue', 'circle', '2'], dtype='<U6'), 'SEP', 'C3', 'EOS', array(['yellow', 'circle', '3'], dtype='<U6'), 'SEP', array(['yellow', 'circle', '3'], dtype='<U6'), 'SEP', 'C4']\n",
      "[array(['green', 'star', '4'], dtype='<U6'), array(['green', 'square', '3'], dtype='<U6'), array(['blue', 'star', '2'], dtype='<U6'), array(['green', 'circle', '1'], dtype='<U6'), array(['green', 'circle', '4'], dtype='<U6'), 'SEP', 'C1', 'EOS', array(['blue', 'cross', '1'], dtype='<U6'), 'SEP', array(['blue', 'cross', '1'], dtype='<U6'), 'SEP', 'C4']\n",
      "[array(['green', 'star', '3'], dtype='<U6'), array(['green', 'square', '2'], dtype='<U6'), array(['red', 'cross', '1'], dtype='<U6'), array(['yellow', 'cross', '4'], dtype='<U6'), array(['blue', 'star', '4'], dtype='<U6'), 'SEP', 'C4', 'EOS', array(['red', 'star', '1'], dtype='<U6'), 'SEP', array(['red', 'star', '1'], dtype='<U6'), 'SEP', 'C3']\n",
      "[array(['blue', 'star', '4'], dtype='<U6'), array(['green', 'circle', '1'], dtype='<U6'), array(['red', 'star', '3'], dtype='<U6'), array(['red', 'cross', '2'], dtype='<U6'), array(['green', 'circle', '3'], dtype='<U6'), 'SEP', 'C3', 'EOS', array(['yellow', 'circle', '1'], dtype='<U6'), 'SEP', array(['yellow', 'circle', '1'], dtype='<U6'), 'SEP', 'C2']\n",
      "[array(['red', 'star', '4'], dtype='<U6'), array(['green', 'circle', '2'], dtype='<U6'), array(['green', 'square', '3'], dtype='<U6'), array(['red', 'square', '1'], dtype='<U6'), array(['red', 'circle', '1'], dtype='<U6'), 'SEP', 'C4', 'EOS', array(['yellow', 'cross', '2'], dtype='<U6'), 'SEP', array(['yellow', 'cross', '2'], dtype='<U6'), 'SEP', 'C2']\n",
      "Feature for Classification:  0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoder_input, decoder_input, target = train_datasets[\"quantity\"][:10]\n",
    "encoder_input = encoder_input.to(device)\n",
    "decoder_input = decoder_input.to(device)\n",
    "target = target.to(device)\n",
    "\n",
    "prediction = model_inference(transformer, encoder_input, decoder_input)\n",
    "\n",
    "print(\"# Actual Trials\")\n",
    "test_batch = [np.asarray(item.cpu()) for item in [encoder_input, T.concatenate([decoder_input, target], dim=1)]]\n",
    "output = wcst.visualise_batch(test_batch)\n",
    "\n",
    "print(\"# Predicted Trials\")\n",
    "prediction_batch = [np.asarray(item.cpu()) for item in [encoder_input, T.concatenate([decoder_input, prediction], dim=1)]]\n",
    "output = wcst.visualise_batch(prediction_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f73bf2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
