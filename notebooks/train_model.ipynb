{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "dd555848",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch as T\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "\n",
    "from datasets.wcst import WCST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "389721a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "wcst = WCST(10)\n",
    "\n",
    "device = T.device('cuda' if T.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08945224",
   "metadata": {},
   "source": [
    "## Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "25ea2ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ba4f2c",
   "metadata": {},
   "source": [
    "### 1. Dataset Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e95fdcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e47ffd5",
   "metadata": {},
   "source": [
    "### 2. Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "365835c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_targets = T.load('../datasets/train_dataset.pt')\n",
    "train_dataset_loader = DataLoader(TensorDataset(train_data, train_targets), batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "validation_data, validation_targets = T.load('../datasets/validation_dataset.pt')\n",
    "validation_dataset_loader = DataLoader(TensorDataset(validation_data, validation_targets), batch_size=BATCH_SIZE)\n",
    "\n",
    "test_data, test_targets = T.load('../datasets/test_dataset.pt')\n",
    "test_dataset_loader  = DataLoader(TensorDataset(test_data, test_targets), batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f524267",
   "metadata": {},
   "source": [
    "## Transformer Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d3a8bc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.transformer import Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7739c3fe",
   "metadata": {},
   "source": [
    "### 1. Transformer Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "606c1008",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCABULARY_SIZE = 70 # Cards + Categories + 'SEP' + 'EOS'\n",
    "EMBEDDING_SIZE = 32\n",
    "N_ATTENTION_HEADS = 4\n",
    "N_BLOCKS = 1\n",
    "MAX_SEQUENCE_LENGTH = 10\n",
    "FF_DIMS = 64\n",
    "DROPOUT_PROB = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab9fc96",
   "metadata": {},
   "source": [
    "### 2. Transformer Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9f25ee98",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    VOCABULARY_SIZE, VOCABULARY_SIZE, EMBEDDING_SIZE, N_ATTENTION_HEADS,\n",
    "    N_BLOCKS, MAX_SEQUENCE_LENGTH, FF_DIMS, DROPOUT_PROB, device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7489333",
   "metadata": {},
   "source": [
    "## Training Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "42f3ebdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c5d23d",
   "metadata": {},
   "source": [
    "### 1. Train, Validate, Evaluate Model Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3a8ee8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    train_loader: DataLoader, validation_loader: DataLoader, model: Transformer, criterion: nn.CrossEntropyLoss, \n",
    "    optimizer: optim.Optimizer, max_epochs: int = 20, device: str | T.device = \"cpu\", patience: int = 3\n",
    "    ):\n",
    "    \n",
    "    best_val_loss = np.inf\n",
    "    patience_counter = 0\n",
    "\n",
    "    train_losses, train_accs, train_perplexities = [], [], []\n",
    "    val_losses, val_accs, val_perplexities = [], [], []\n",
    "    best_model_state = model.state_dict()\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{max_epochs}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "        # --- Training ---\n",
    "        model.train()\n",
    "        epoch_train_losses = []\n",
    "        total_correct = 0\n",
    "        total_tokens = 0\n",
    "\n",
    "        for batch_idx, (X, target) in enumerate(train_loader):\n",
    "            encoder_input, target = X.to(device), target.to(device)\n",
    "\n",
    "            # Decoder inputs/targets (shifted)\n",
    "            decoder_input = target[:, :-1]\n",
    "            decoder_target = target[:, 1:].reshape(-1)\n",
    "\n",
    "            # Forward pass\n",
    "            logits = model(encoder_input, decoder_input).reshape(-1, VOCABULARY_SIZE)\n",
    "            loss = criterion(logits, decoder_target)\n",
    "\n",
    "            # Backprop\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accuracy\n",
    "            preds = logits.argmax(dim=1)\n",
    "            total_correct += (preds == decoder_target).sum().item()\n",
    "            total_tokens += decoder_target.size(0)\n",
    "\n",
    "            epoch_train_losses.append(loss.item())\n",
    "\n",
    "            if batch_idx % 10 == 0 or batch_idx == len(train_loader) - 1:\n",
    "                print(f\"Train Batch {batch_idx+1}/{len(train_loader)} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "        train_loss = np.mean(epoch_train_losses)\n",
    "        train_acc = total_correct / total_tokens\n",
    "        train_perplexity = np.exp(train_loss)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        train_perplexities.append(train_perplexities)\n",
    "\n",
    "        print(f\"[Epoch {epoch+1}] Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Train Perplexity: {train_perplexity:.4f}\")\n",
    "\n",
    "        # --- Validation ---\n",
    "        model.eval()\n",
    "        val_batch_losses = []\n",
    "        test_correct = 0\n",
    "        test_tokens = 0\n",
    "\n",
    "        with T.no_grad():\n",
    "            for X, target in validation_loader:\n",
    "                encoder_input, target = X.to(device), target.to(device)\n",
    "                decoder_input = target[:, :-1]\n",
    "                decoder_target = target[:, 1:].reshape(-1)\n",
    "\n",
    "                logits = model(encoder_input, decoder_input).reshape(-1, VOCABULARY_SIZE)\n",
    "                loss = criterion(logits, decoder_target)\n",
    "\n",
    "                preds = logits.argmax(dim=1)\n",
    "                test_correct += (preds == decoder_target).sum().item()\n",
    "                test_tokens += decoder_target.size(0)\n",
    "\n",
    "                val_batch_losses.append(loss.item())\n",
    "\n",
    "        val_loss = np.mean(val_batch_losses)\n",
    "        val_acc = test_correct / test_tokens\n",
    "        val_perplexity = np.exp(val_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_acc)\n",
    "        val_perplexities.append(val_perplexities)\n",
    "\n",
    "        print(f\"[Epoch {epoch+1}] Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | Val Perplexity: {val_perplexity:.4f}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "        # --- Early Stopping ---\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            best_model_state = model.state_dict()\n",
    "            print(f\"Validation loss improved — saving model (Loss: {val_loss:.4f})\")\n",
    "        # else:\n",
    "        #     patience_counter += 1\n",
    "        #     print(f\"No improvement ({patience_counter}/{patience})\")\n",
    "        #     if patience_counter >= patience:\n",
    "        #         print(\"\\nEarly stopping triggered. Restoring best model.\")\n",
    "        #         model.load_state_dict(best_model_state)\n",
    "        #         break\n",
    "\n",
    "    print(\"\\nTraining complete\")\n",
    "\n",
    "    return {\n",
    "        \"train_losses\": train_losses,\n",
    "        \"train_accs\": train_accs,\n",
    "        \"train_perplexities\": train_perplexities,\n",
    "        \"val_losses\": val_losses,\n",
    "        \"val_accs\": val_accs,\n",
    "        \"val_perplexities\": val_perplexities,\n",
    "        \"best_val_loss\": best_val_loss,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "44864dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(test_loader: DataLoader, model: Transformer, criterion: nn.CrossEntropyLoss, device: str | T.device = \"cpu\"):\n",
    "\n",
    "    model.eval()\n",
    "    test_batch_losses = []\n",
    "    test_correct = 0\n",
    "    test_tokens = 0\n",
    "\n",
    "    with T.no_grad():\n",
    "        for X, target in test_loader:\n",
    "            encoder_input, target = X.to(device), target.to(device)\n",
    "            decoder_input = target[:, :-1]\n",
    "            decoder_target = target[:, 1:].reshape(-1)\n",
    "\n",
    "            logits = model(encoder_input, decoder_input).reshape(-1, VOCABULARY_SIZE)\n",
    "            loss = criterion(logits, decoder_target)\n",
    "\n",
    "            preds = logits.argmax(dim=1)\n",
    "            test_correct += (preds == decoder_target).sum().item()\n",
    "            test_tokens += decoder_target.size(0)\n",
    "\n",
    "            test_batch_losses.append(loss.item())\n",
    "\n",
    "    test_loss = np.mean(test_batch_losses)\n",
    "    test_acc = test_correct / test_tokens\n",
    "    test_perplexity = np.exp(test_loss)\n",
    "\n",
    "    return {\n",
    "        \"test_loss\": test_loss,\n",
    "        \"test_acc\": test_acc,\n",
    "        \"test_perplexity\": test_perplexity\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d863c67",
   "metadata": {},
   "source": [
    "### 2. Train Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "59271964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Hyperparameters\n",
    "MAX_EPOCHS = 40\n",
    "LEARNING_RATE = 1e-3\n",
    "BETAS = (0.9, 0.98)\n",
    "EPSILON = 1e-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a5c8f99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/40\n",
      "----------------------------------------\n",
      "Train Batch 1/60 | Loss: 0.5473\n",
      "Train Batch 11/60 | Loss: 0.5271\n",
      "Train Batch 21/60 | Loss: 0.5323\n",
      "Train Batch 31/60 | Loss: 0.5177\n",
      "Train Batch 41/60 | Loss: 0.6228\n",
      "Train Batch 51/60 | Loss: 0.5423\n",
      "Train Batch 60/60 | Loss: 0.4891\n",
      "[Epoch 1] Train Loss: 0.5446 | Train Acc: 0.7654 | Train Perplexity: 1.7240\n",
      "[Epoch 1] Val Loss: 0.8443 | Val Acc: 0.6367 | Val Perplexity: 2.3262\n",
      "----------------------------------------\n",
      "Validation loss improved — saving model (Loss: 0.8443)\n",
      "\n",
      "Epoch 2/40\n",
      "----------------------------------------\n",
      "Train Batch 1/60 | Loss: 0.5009\n",
      "Train Batch 11/60 | Loss: 0.5317\n",
      "Train Batch 21/60 | Loss: 0.5372\n",
      "Train Batch 31/60 | Loss: 0.5341\n",
      "Train Batch 41/60 | Loss: 0.4763\n",
      "Train Batch 51/60 | Loss: 0.4659\n",
      "Train Batch 60/60 | Loss: 0.5578\n",
      "[Epoch 2] Train Loss: 0.5359 | Train Acc: 0.7701 | Train Perplexity: 1.7090\n",
      "[Epoch 2] Val Loss: 0.8643 | Val Acc: 0.6344 | Val Perplexity: 2.3732\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 3/40\n",
      "----------------------------------------\n",
      "Train Batch 1/60 | Loss: 0.5363\n",
      "Train Batch 11/60 | Loss: 0.5850\n",
      "Train Batch 21/60 | Loss: 0.6072\n",
      "Train Batch 31/60 | Loss: 0.5792\n",
      "Train Batch 41/60 | Loss: 0.5127\n",
      "Train Batch 51/60 | Loss: 0.5867\n",
      "Train Batch 60/60 | Loss: 0.5997\n",
      "[Epoch 3] Train Loss: 0.5340 | Train Acc: 0.7758 | Train Perplexity: 1.7058\n",
      "[Epoch 3] Val Loss: 0.8715 | Val Acc: 0.6359 | Val Perplexity: 2.3906\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 4/40\n",
      "----------------------------------------\n",
      "Train Batch 1/60 | Loss: 0.5107\n",
      "Train Batch 11/60 | Loss: 0.5862\n",
      "Train Batch 21/60 | Loss: 0.4438\n",
      "Train Batch 31/60 | Loss: 0.4974\n",
      "Train Batch 41/60 | Loss: 0.5582\n",
      "Train Batch 51/60 | Loss: 0.5589\n",
      "Train Batch 60/60 | Loss: 0.5713\n",
      "[Epoch 4] Train Loss: 0.5353 | Train Acc: 0.7734 | Train Perplexity: 1.7080\n",
      "[Epoch 4] Val Loss: 0.8561 | Val Acc: 0.6375 | Val Perplexity: 2.3539\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 5/40\n",
      "----------------------------------------\n",
      "Train Batch 1/60 | Loss: 0.4997\n",
      "Train Batch 11/60 | Loss: 0.5076\n",
      "Train Batch 21/60 | Loss: 0.5073\n",
      "Train Batch 31/60 | Loss: 0.4457\n",
      "Train Batch 41/60 | Loss: 0.4915\n",
      "Train Batch 51/60 | Loss: 0.7481\n",
      "Train Batch 60/60 | Loss: 0.5023\n",
      "[Epoch 5] Train Loss: 0.5349 | Train Acc: 0.7695 | Train Perplexity: 1.7073\n",
      "[Epoch 5] Val Loss: 0.8584 | Val Acc: 0.6297 | Val Perplexity: 2.3593\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 6/40\n",
      "----------------------------------------\n",
      "Train Batch 1/60 | Loss: 0.4496\n",
      "Train Batch 11/60 | Loss: 0.5317\n",
      "Train Batch 21/60 | Loss: 0.4773\n",
      "Train Batch 31/60 | Loss: 0.5714\n",
      "Train Batch 41/60 | Loss: 0.5342\n",
      "Train Batch 51/60 | Loss: 0.5743\n",
      "Train Batch 60/60 | Loss: 0.5720\n",
      "[Epoch 6] Train Loss: 0.5211 | Train Acc: 0.7781 | Train Perplexity: 1.6838\n",
      "[Epoch 6] Val Loss: 0.8619 | Val Acc: 0.6352 | Val Perplexity: 2.3678\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 7/40\n",
      "----------------------------------------\n",
      "Train Batch 1/60 | Loss: 0.5512\n",
      "Train Batch 11/60 | Loss: 0.4439\n",
      "Train Batch 21/60 | Loss: 0.5049\n",
      "Train Batch 31/60 | Loss: 0.5446\n",
      "Train Batch 41/60 | Loss: 0.5252\n",
      "Train Batch 51/60 | Loss: 0.5189\n",
      "Train Batch 60/60 | Loss: 0.4107\n",
      "[Epoch 7] Train Loss: 0.5165 | Train Acc: 0.7737 | Train Perplexity: 1.6761\n",
      "[Epoch 7] Val Loss: 0.8817 | Val Acc: 0.6320 | Val Perplexity: 2.4150\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 8/40\n",
      "----------------------------------------\n",
      "Train Batch 1/60 | Loss: 0.4181\n",
      "Train Batch 11/60 | Loss: 0.5119\n",
      "Train Batch 21/60 | Loss: 0.4958\n",
      "Train Batch 31/60 | Loss: 0.5745\n",
      "Train Batch 41/60 | Loss: 0.5800\n",
      "Train Batch 51/60 | Loss: 0.5802\n",
      "Train Batch 60/60 | Loss: 0.5633\n",
      "[Epoch 8] Train Loss: 0.5219 | Train Acc: 0.7753 | Train Perplexity: 1.6852\n",
      "[Epoch 8] Val Loss: 0.8768 | Val Acc: 0.6250 | Val Perplexity: 2.4033\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 9/40\n",
      "----------------------------------------\n",
      "Train Batch 1/60 | Loss: 0.5809\n",
      "Train Batch 11/60 | Loss: 0.6198\n",
      "Train Batch 21/60 | Loss: 0.5301\n",
      "Train Batch 31/60 | Loss: 0.5187\n",
      "Train Batch 41/60 | Loss: 0.5441\n",
      "Train Batch 51/60 | Loss: 0.3992\n",
      "Train Batch 60/60 | Loss: 0.4909\n",
      "[Epoch 9] Train Loss: 0.5216 | Train Acc: 0.7820 | Train Perplexity: 1.6847\n",
      "[Epoch 9] Val Loss: 0.8812 | Val Acc: 0.6383 | Val Perplexity: 2.4137\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 10/40\n",
      "----------------------------------------\n",
      "Train Batch 1/60 | Loss: 0.5391\n",
      "Train Batch 11/60 | Loss: 0.5216\n",
      "Train Batch 21/60 | Loss: 0.3575\n",
      "Train Batch 31/60 | Loss: 0.4776\n",
      "Train Batch 41/60 | Loss: 0.4892\n",
      "Train Batch 51/60 | Loss: 0.4962\n",
      "Train Batch 60/60 | Loss: 0.5127\n",
      "[Epoch 10] Train Loss: 0.5153 | Train Acc: 0.7807 | Train Perplexity: 1.6741\n",
      "[Epoch 10] Val Loss: 0.8967 | Val Acc: 0.6203 | Val Perplexity: 2.4516\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 11/40\n",
      "----------------------------------------\n",
      "Train Batch 1/60 | Loss: 0.4779\n",
      "Train Batch 11/60 | Loss: 0.5083\n",
      "Train Batch 21/60 | Loss: 0.4727\n",
      "Train Batch 31/60 | Loss: 0.4659\n",
      "Train Batch 41/60 | Loss: 0.6169\n",
      "Train Batch 51/60 | Loss: 0.5138\n",
      "Train Batch 60/60 | Loss: 0.4896\n",
      "[Epoch 11] Train Loss: 0.5182 | Train Acc: 0.7831 | Train Perplexity: 1.6790\n",
      "[Epoch 11] Val Loss: 0.8742 | Val Acc: 0.6375 | Val Perplexity: 2.3968\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 12/40\n",
      "----------------------------------------\n",
      "Train Batch 1/60 | Loss: 0.5778\n",
      "Train Batch 11/60 | Loss: 0.4670\n",
      "Train Batch 21/60 | Loss: 0.4567\n",
      "Train Batch 31/60 | Loss: 0.6725\n",
      "Train Batch 41/60 | Loss: 0.6734\n",
      "Train Batch 51/60 | Loss: 0.4715\n",
      "Train Batch 60/60 | Loss: 0.5327\n",
      "[Epoch 12] Train Loss: 0.5012 | Train Acc: 0.7880 | Train Perplexity: 1.6507\n",
      "[Epoch 12] Val Loss: 0.8936 | Val Acc: 0.6336 | Val Perplexity: 2.4439\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 13/40\n",
      "----------------------------------------\n",
      "Train Batch 1/60 | Loss: 0.4042\n",
      "Train Batch 11/60 | Loss: 0.4848\n",
      "Train Batch 21/60 | Loss: 0.4556\n",
      "Train Batch 31/60 | Loss: 0.5333\n",
      "Train Batch 41/60 | Loss: 0.4487\n",
      "Train Batch 51/60 | Loss: 0.4105\n",
      "Train Batch 60/60 | Loss: 0.5186\n",
      "[Epoch 13] Train Loss: 0.4995 | Train Acc: 0.7875 | Train Perplexity: 1.6479\n",
      "[Epoch 13] Val Loss: 0.9030 | Val Acc: 0.6359 | Val Perplexity: 2.4670\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 14/40\n",
      "----------------------------------------\n",
      "Train Batch 1/60 | Loss: 0.5114\n",
      "Train Batch 11/60 | Loss: 0.6390\n",
      "Train Batch 21/60 | Loss: 0.4964\n",
      "Train Batch 31/60 | Loss: 0.5874\n",
      "Train Batch 41/60 | Loss: 0.5492\n",
      "Train Batch 51/60 | Loss: 0.4663\n",
      "Train Batch 60/60 | Loss: 0.4795\n",
      "[Epoch 14] Train Loss: 0.4948 | Train Acc: 0.7891 | Train Perplexity: 1.6402\n",
      "[Epoch 14] Val Loss: 0.9172 | Val Acc: 0.6359 | Val Perplexity: 2.5023\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 15/40\n",
      "----------------------------------------\n",
      "Train Batch 1/60 | Loss: 0.4898\n",
      "Train Batch 11/60 | Loss: 0.3461\n",
      "Train Batch 21/60 | Loss: 0.5972\n",
      "Train Batch 31/60 | Loss: 0.5450\n",
      "Train Batch 41/60 | Loss: 0.5418\n",
      "Train Batch 51/60 | Loss: 0.3777\n",
      "Train Batch 60/60 | Loss: 0.5191\n",
      "[Epoch 15] Train Loss: 0.4899 | Train Acc: 0.7904 | Train Perplexity: 1.6321\n",
      "[Epoch 15] Val Loss: 0.9324 | Val Acc: 0.6250 | Val Perplexity: 2.5405\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 16/40\n",
      "----------------------------------------\n",
      "Train Batch 1/60 | Loss: 0.4844\n",
      "Train Batch 11/60 | Loss: 0.4404\n",
      "Train Batch 21/60 | Loss: 0.5216\n",
      "Train Batch 31/60 | Loss: 0.5000\n",
      "Train Batch 41/60 | Loss: 0.5112\n",
      "Train Batch 51/60 | Loss: 0.5774\n",
      "Train Batch 60/60 | Loss: 0.4652\n",
      "[Epoch 16] Train Loss: 0.5033 | Train Acc: 0.7906 | Train Perplexity: 1.6541\n",
      "[Epoch 16] Val Loss: 0.9008 | Val Acc: 0.6305 | Val Perplexity: 2.4615\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 17/40\n",
      "----------------------------------------\n",
      "Train Batch 1/60 | Loss: 0.5243\n",
      "Train Batch 11/60 | Loss: 0.6435\n",
      "Train Batch 21/60 | Loss: 0.4800\n",
      "Train Batch 31/60 | Loss: 0.4998\n",
      "Train Batch 41/60 | Loss: 0.4065\n",
      "Train Batch 51/60 | Loss: 0.5705\n",
      "Train Batch 60/60 | Loss: 0.4578\n",
      "[Epoch 17] Train Loss: 0.4939 | Train Acc: 0.7943 | Train Perplexity: 1.6387\n",
      "[Epoch 17] Val Loss: 0.9289 | Val Acc: 0.6281 | Val Perplexity: 2.5316\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 18/40\n",
      "----------------------------------------\n",
      "Train Batch 1/60 | Loss: 0.5686\n",
      "Train Batch 11/60 | Loss: 0.4629\n",
      "Train Batch 21/60 | Loss: 0.5211\n",
      "Train Batch 31/60 | Loss: 0.4834\n",
      "Train Batch 41/60 | Loss: 0.5388\n",
      "Train Batch 51/60 | Loss: 0.5639\n",
      "Train Batch 60/60 | Loss: 0.4641\n",
      "[Epoch 18] Train Loss: 0.4881 | Train Acc: 0.7953 | Train Perplexity: 1.6292\n",
      "[Epoch 18] Val Loss: 0.9210 | Val Acc: 0.6352 | Val Perplexity: 2.5117\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 19/40\n",
      "----------------------------------------\n",
      "Train Batch 1/60 | Loss: 0.4932\n",
      "Train Batch 11/60 | Loss: 0.4969\n",
      "Train Batch 21/60 | Loss: 0.4275\n",
      "Train Batch 31/60 | Loss: 0.5840\n",
      "Train Batch 41/60 | Loss: 0.5670\n",
      "Train Batch 51/60 | Loss: 0.6155\n",
      "Train Batch 60/60 | Loss: 0.5147\n",
      "[Epoch 19] Train Loss: 0.4847 | Train Acc: 0.7945 | Train Perplexity: 1.6237\n",
      "[Epoch 19] Val Loss: 0.9282 | Val Acc: 0.6195 | Val Perplexity: 2.5299\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 20/40\n",
      "----------------------------------------\n",
      "Train Batch 1/60 | Loss: 0.4463\n",
      "Train Batch 11/60 | Loss: 0.4275\n",
      "Train Batch 21/60 | Loss: 0.4829\n",
      "Train Batch 31/60 | Loss: 0.5075\n",
      "Train Batch 41/60 | Loss: 0.5244\n",
      "Train Batch 51/60 | Loss: 0.4673\n",
      "Train Batch 60/60 | Loss: 0.5329\n",
      "[Epoch 20] Train Loss: 0.4742 | Train Acc: 0.7961 | Train Perplexity: 1.6067\n",
      "[Epoch 20] Val Loss: 0.9623 | Val Acc: 0.6328 | Val Perplexity: 2.6177\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 21/40\n",
      "----------------------------------------\n",
      "Train Batch 1/60 | Loss: 0.4745\n",
      "Train Batch 11/60 | Loss: 0.4385\n",
      "Train Batch 21/60 | Loss: 0.3715\n",
      "Train Batch 31/60 | Loss: 0.4534\n",
      "Train Batch 41/60 | Loss: 0.5150\n",
      "Train Batch 51/60 | Loss: 0.4662\n",
      "Train Batch 60/60 | Loss: 0.4167\n",
      "[Epoch 21] Train Loss: 0.4663 | Train Acc: 0.8021 | Train Perplexity: 1.5940\n",
      "[Epoch 21] Val Loss: 0.9468 | Val Acc: 0.6289 | Val Perplexity: 2.5776\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 22/40\n",
      "----------------------------------------\n",
      "Train Batch 1/60 | Loss: 0.4739\n",
      "Train Batch 11/60 | Loss: 0.4458\n",
      "Train Batch 21/60 | Loss: 0.4341\n",
      "Train Batch 31/60 | Loss: 0.4853\n",
      "Train Batch 41/60 | Loss: 0.5139\n",
      "Train Batch 51/60 | Loss: 0.4990\n",
      "Train Batch 60/60 | Loss: 0.5957\n",
      "[Epoch 22] Train Loss: 0.4795 | Train Acc: 0.8042 | Train Perplexity: 1.6153\n",
      "[Epoch 22] Val Loss: 0.9331 | Val Acc: 0.6266 | Val Perplexity: 2.5424\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 23/40\n",
      "----------------------------------------\n",
      "Train Batch 1/60 | Loss: 0.5142\n",
      "Train Batch 11/60 | Loss: 0.4664\n",
      "Train Batch 21/60 | Loss: 0.4465\n",
      "Train Batch 31/60 | Loss: 0.4716\n",
      "Train Batch 41/60 | Loss: 0.6367\n",
      "Train Batch 51/60 | Loss: 0.5539\n",
      "Train Batch 60/60 | Loss: 0.3746\n",
      "[Epoch 23] Train Loss: 0.4668 | Train Acc: 0.8096 | Train Perplexity: 1.5948\n",
      "[Epoch 23] Val Loss: 0.9727 | Val Acc: 0.6375 | Val Perplexity: 2.6451\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 24/40\n",
      "----------------------------------------\n",
      "Train Batch 1/60 | Loss: 0.4964\n",
      "Train Batch 11/60 | Loss: 0.4741\n",
      "Train Batch 21/60 | Loss: 0.4982\n",
      "Train Batch 31/60 | Loss: 0.4926\n",
      "Train Batch 41/60 | Loss: 0.4206\n",
      "Train Batch 51/60 | Loss: 0.4323\n",
      "Train Batch 60/60 | Loss: 0.5488\n",
      "[Epoch 24] Train Loss: 0.4670 | Train Acc: 0.8031 | Train Perplexity: 1.5953\n",
      "[Epoch 24] Val Loss: 0.9684 | Val Acc: 0.6320 | Val Perplexity: 2.6338\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 25/40\n",
      "----------------------------------------\n",
      "Train Batch 1/60 | Loss: 0.4542\n",
      "Train Batch 11/60 | Loss: 0.4605\n",
      "Train Batch 21/60 | Loss: 0.4959\n",
      "Train Batch 31/60 | Loss: 0.4496\n",
      "Train Batch 41/60 | Loss: 0.4933\n",
      "Train Batch 51/60 | Loss: 0.4544\n",
      "Train Batch 60/60 | Loss: 0.5022\n",
      "[Epoch 25] Train Loss: 0.4666 | Train Acc: 0.8107 | Train Perplexity: 1.5946\n",
      "[Epoch 25] Val Loss: 0.9945 | Val Acc: 0.6312 | Val Perplexity: 2.7034\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 26/40\n",
      "----------------------------------------\n",
      "Train Batch 1/60 | Loss: 0.3641\n",
      "Train Batch 11/60 | Loss: 0.4986\n",
      "Train Batch 21/60 | Loss: 0.3814\n",
      "Train Batch 31/60 | Loss: 0.4070\n",
      "Train Batch 41/60 | Loss: 0.3553\n",
      "Train Batch 51/60 | Loss: 0.5796\n",
      "Train Batch 60/60 | Loss: 0.4738\n",
      "[Epoch 26] Train Loss: 0.4668 | Train Acc: 0.8078 | Train Perplexity: 1.5948\n",
      "[Epoch 26] Val Loss: 0.9655 | Val Acc: 0.6289 | Val Perplexity: 2.6261\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 27/40\n",
      "----------------------------------------\n",
      "Train Batch 1/60 | Loss: 0.4188\n",
      "Train Batch 11/60 | Loss: 0.5204\n",
      "Train Batch 21/60 | Loss: 0.4616\n",
      "Train Batch 31/60 | Loss: 0.3895\n",
      "Train Batch 41/60 | Loss: 0.4019\n",
      "Train Batch 51/60 | Loss: 0.4100\n",
      "Train Batch 60/60 | Loss: 0.4302\n",
      "[Epoch 27] Train Loss: 0.4548 | Train Acc: 0.8036 | Train Perplexity: 1.5758\n",
      "[Epoch 27] Val Loss: 0.9871 | Val Acc: 0.6383 | Val Perplexity: 2.6835\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 28/40\n",
      "----------------------------------------\n",
      "Train Batch 1/60 | Loss: 0.4913\n",
      "Train Batch 11/60 | Loss: 0.5037\n",
      "Train Batch 21/60 | Loss: 0.4211\n",
      "Train Batch 31/60 | Loss: 0.5611\n",
      "Train Batch 41/60 | Loss: 0.4214\n",
      "Train Batch 51/60 | Loss: 0.5005\n",
      "Train Batch 60/60 | Loss: 0.3168\n",
      "[Epoch 28] Train Loss: 0.4599 | Train Acc: 0.8102 | Train Perplexity: 1.5839\n",
      "[Epoch 28] Val Loss: 0.9732 | Val Acc: 0.6398 | Val Perplexity: 2.6465\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 29/40\n",
      "----------------------------------------\n",
      "Train Batch 1/60 | Loss: 0.3744\n",
      "Train Batch 11/60 | Loss: 0.4346\n",
      "Train Batch 21/60 | Loss: 0.4235\n",
      "Train Batch 31/60 | Loss: 0.4412\n",
      "Train Batch 41/60 | Loss: 0.4687\n",
      "Train Batch 51/60 | Loss: 0.4659\n",
      "Train Batch 60/60 | Loss: 0.5684\n",
      "[Epoch 29] Train Loss: 0.4556 | Train Acc: 0.8141 | Train Perplexity: 1.5771\n",
      "[Epoch 29] Val Loss: 0.9765 | Val Acc: 0.6273 | Val Perplexity: 2.6553\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 30/40\n",
      "----------------------------------------\n",
      "Train Batch 1/60 | Loss: 0.3661\n",
      "Train Batch 11/60 | Loss: 0.4188\n",
      "Train Batch 21/60 | Loss: 0.3327\n",
      "Train Batch 31/60 | Loss: 0.5048\n",
      "Train Batch 41/60 | Loss: 0.4249\n",
      "Train Batch 51/60 | Loss: 0.5063\n",
      "Train Batch 60/60 | Loss: 0.4656\n",
      "[Epoch 30] Train Loss: 0.4486 | Train Acc: 0.8177 | Train Perplexity: 1.5662\n",
      "[Epoch 30] Val Loss: 0.9946 | Val Acc: 0.6328 | Val Perplexity: 2.7036\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 31/40\n",
      "----------------------------------------\n",
      "Train Batch 1/60 | Loss: 0.4226\n",
      "Train Batch 11/60 | Loss: 0.3636\n",
      "Train Batch 21/60 | Loss: 0.4230\n",
      "Train Batch 31/60 | Loss: 0.5164\n",
      "Train Batch 41/60 | Loss: 0.3592\n",
      "Train Batch 51/60 | Loss: 0.5306\n",
      "Train Batch 60/60 | Loss: 0.3732\n",
      "[Epoch 31] Train Loss: 0.4443 | Train Acc: 0.8195 | Train Perplexity: 1.5594\n",
      "[Epoch 31] Val Loss: 0.9902 | Val Acc: 0.6266 | Val Perplexity: 2.6918\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 32/40\n",
      "----------------------------------------\n",
      "Train Batch 1/60 | Loss: 0.3341\n",
      "Train Batch 11/60 | Loss: 0.3563\n",
      "Train Batch 21/60 | Loss: 0.4058\n",
      "Train Batch 31/60 | Loss: 0.3542\n",
      "Train Batch 41/60 | Loss: 0.5678\n",
      "Train Batch 51/60 | Loss: 0.4361\n",
      "Train Batch 60/60 | Loss: 0.4605\n",
      "[Epoch 32] Train Loss: 0.4462 | Train Acc: 0.8201 | Train Perplexity: 1.5624\n",
      "[Epoch 32] Val Loss: 1.0001 | Val Acc: 0.6242 | Val Perplexity: 2.7185\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 33/40\n",
      "----------------------------------------\n",
      "Train Batch 1/60 | Loss: 0.3157\n",
      "Train Batch 11/60 | Loss: 0.4421\n",
      "Train Batch 21/60 | Loss: 0.4221\n",
      "Train Batch 31/60 | Loss: 0.3566\n",
      "Train Batch 41/60 | Loss: 0.3515\n",
      "Train Batch 51/60 | Loss: 0.4906\n",
      "Train Batch 60/60 | Loss: 0.4825\n",
      "[Epoch 33] Train Loss: 0.4281 | Train Acc: 0.8229 | Train Perplexity: 1.5344\n",
      "[Epoch 33] Val Loss: 1.0019 | Val Acc: 0.6391 | Val Perplexity: 2.7235\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 34/40\n",
      "----------------------------------------\n",
      "Train Batch 1/60 | Loss: 0.4180\n",
      "Train Batch 11/60 | Loss: 0.3568\n",
      "Train Batch 21/60 | Loss: 0.3988\n",
      "Train Batch 31/60 | Loss: 0.4336\n",
      "Train Batch 41/60 | Loss: 0.4943\n",
      "Train Batch 51/60 | Loss: 0.4457\n",
      "Train Batch 60/60 | Loss: 0.3818\n",
      "[Epoch 34] Train Loss: 0.4332 | Train Acc: 0.8159 | Train Perplexity: 1.5422\n",
      "[Epoch 34] Val Loss: 1.0013 | Val Acc: 0.6359 | Val Perplexity: 2.7218\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 35/40\n",
      "----------------------------------------\n",
      "Train Batch 1/60 | Loss: 0.5258\n",
      "Train Batch 11/60 | Loss: 0.3119\n",
      "Train Batch 21/60 | Loss: 0.5202\n",
      "Train Batch 31/60 | Loss: 0.4082\n",
      "Train Batch 41/60 | Loss: 0.3533\n",
      "Train Batch 51/60 | Loss: 0.3472\n",
      "Train Batch 60/60 | Loss: 0.5152\n",
      "[Epoch 35] Train Loss: 0.4356 | Train Acc: 0.8242 | Train Perplexity: 1.5459\n",
      "[Epoch 35] Val Loss: 1.0066 | Val Acc: 0.6328 | Val Perplexity: 2.7361\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 36/40\n",
      "----------------------------------------\n",
      "Train Batch 1/60 | Loss: 0.3879\n",
      "Train Batch 11/60 | Loss: 0.4075\n",
      "Train Batch 21/60 | Loss: 0.3823\n",
      "Train Batch 31/60 | Loss: 0.4655\n",
      "Train Batch 41/60 | Loss: 0.3876\n",
      "Train Batch 51/60 | Loss: 0.5187\n",
      "Train Batch 60/60 | Loss: 0.4197\n",
      "[Epoch 36] Train Loss: 0.4245 | Train Acc: 0.8276 | Train Perplexity: 1.5288\n",
      "[Epoch 36] Val Loss: 1.0004 | Val Acc: 0.6281 | Val Perplexity: 2.7193\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 37/40\n",
      "----------------------------------------\n",
      "Train Batch 1/60 | Loss: 0.5178\n",
      "Train Batch 11/60 | Loss: 0.3678\n",
      "Train Batch 21/60 | Loss: 0.5115\n",
      "Train Batch 31/60 | Loss: 0.5067\n",
      "Train Batch 41/60 | Loss: 0.3996\n",
      "Train Batch 51/60 | Loss: 0.2580\n",
      "Train Batch 60/60 | Loss: 0.3925\n",
      "[Epoch 37] Train Loss: 0.4195 | Train Acc: 0.8232 | Train Perplexity: 1.5211\n",
      "[Epoch 37] Val Loss: 1.0114 | Val Acc: 0.6359 | Val Perplexity: 2.7494\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 38/40\n",
      "----------------------------------------\n",
      "Train Batch 1/60 | Loss: 0.4958\n",
      "Train Batch 11/60 | Loss: 0.2894\n",
      "Train Batch 21/60 | Loss: 0.4056\n",
      "Train Batch 31/60 | Loss: 0.3427\n",
      "Train Batch 41/60 | Loss: 0.4723\n",
      "Train Batch 51/60 | Loss: 0.3716\n",
      "Train Batch 60/60 | Loss: 0.3178\n",
      "[Epoch 38] Train Loss: 0.4360 | Train Acc: 0.8190 | Train Perplexity: 1.5465\n",
      "[Epoch 38] Val Loss: 0.9866 | Val Acc: 0.6367 | Val Perplexity: 2.6821\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 39/40\n",
      "----------------------------------------\n",
      "Train Batch 1/60 | Loss: 0.4254\n",
      "Train Batch 11/60 | Loss: 0.4434\n",
      "Train Batch 21/60 | Loss: 0.5386\n",
      "Train Batch 31/60 | Loss: 0.3647\n",
      "Train Batch 41/60 | Loss: 0.4236\n",
      "Train Batch 51/60 | Loss: 0.5597\n",
      "Train Batch 60/60 | Loss: 0.3447\n",
      "[Epoch 39] Train Loss: 0.4406 | Train Acc: 0.8159 | Train Perplexity: 1.5537\n",
      "[Epoch 39] Val Loss: 0.9839 | Val Acc: 0.6289 | Val Perplexity: 2.6748\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 40/40\n",
      "----------------------------------------\n",
      "Train Batch 1/60 | Loss: 0.3445\n",
      "Train Batch 11/60 | Loss: 0.4260\n",
      "Train Batch 21/60 | Loss: 0.4543\n",
      "Train Batch 31/60 | Loss: 0.4379\n",
      "Train Batch 41/60 | Loss: 0.3654\n",
      "Train Batch 51/60 | Loss: 0.4802\n",
      "Train Batch 60/60 | Loss: 0.3662\n",
      "[Epoch 40] Train Loss: 0.4267 | Train Acc: 0.8263 | Train Perplexity: 1.5323\n",
      "[Epoch 40] Val Loss: 1.0263 | Val Acc: 0.6281 | Val Perplexity: 2.7908\n",
      "----------------------------------------\n",
      "\n",
      "Training complete\n"
     ]
    }
   ],
   "source": [
    "criterion =  nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(transformer.parameters(), lr=LEARNING_RATE, betas=BETAS, eps=EPSILON)\n",
    "\n",
    "results = train_model(\n",
    "    train_dataset_loader, validation_dataset_loader, transformer, criterion, \n",
    "    optimizer, max_epochs=MAX_EPOCHS, device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917a1fa4",
   "metadata": {},
   "source": [
    "### 3. Test Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "7d144441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0766 | Train Acc: 0.6258 | Train Perplexity: 2.9346\n"
     ]
    }
   ],
   "source": [
    "results = test_model(test_dataset_loader, transformer, criterion, device)\n",
    "print(f\"Train Loss: {results[\"test_loss\"]:.4f} | Train Acc: {results[\"test_acc\"]:.4f} | Train Perplexity: {results[\"test_perplexity\"]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28dac7b",
   "metadata": {},
   "source": [
    "## Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6b590910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inference(model: Transformer, source_sequence, start_tokens):\n",
    "    model.eval()\n",
    "    generated = start_tokens\n",
    "    \n",
    "    with T.no_grad():\n",
    "        logits = model(source_sequence, generated)\n",
    "    \n",
    "    # Greedy Selection\n",
    "    next_token = T.argmax(logits[:, -1, :], dim=-1, keepdim=True)\n",
    "\n",
    "    generated = T.cat([generated, next_token], dim=1)\n",
    "\n",
    "    return generated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "7f458678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Actual Trials\n",
      "[array(['yellow', 'cross', '2'], dtype='<U6'), array(['yellow', 'square', '2'], dtype='<U6'), array(['green', 'star', '1'], dtype='<U6'), array(['yellow', 'circle', '4'], dtype='<U6'), array(['yellow', 'star', '3'], dtype='<U6'), 'SEP', 'C3', 'EOS', array(['yellow', 'circle', '1'], dtype='<U6'), 'SEP', 'C4']\n",
      "[array(['yellow', 'square', '2'], dtype='<U6'), array(['red', 'cross', '1'], dtype='<U6'), array(['blue', 'circle', '4'], dtype='<U6'), array(['blue', 'star', '4'], dtype='<U6'), array(['red', 'star', '3'], dtype='<U6'), 'SEP', 'C4', 'EOS', array(['blue', 'star', '2'], dtype='<U6'), 'SEP', 'C4']\n",
      "[array(['red', 'circle', '1'], dtype='<U6'), array(['red', 'square', '4'], dtype='<U6'), array(['green', 'star', '3'], dtype='<U6'), array(['yellow', 'cross', '1'], dtype='<U6'), array(['blue', 'circle', '2'], dtype='<U6'), 'SEP', 'C1', 'EOS', array(['yellow', 'square', '1'], dtype='<U6'), 'SEP', 'C2']\n",
      "[array(['green', 'circle', '4'], dtype='<U6'), array(['green', 'cross', '4'], dtype='<U6'), array(['yellow', 'square', '2'], dtype='<U6'), array(['green', 'star', '4'], dtype='<U6'), array(['red', 'square', '3'], dtype='<U6'), 'SEP', 'C3', 'EOS', array(['green', 'square', '3'], dtype='<U6'), 'SEP', 'C3']\n",
      "[array(['blue', 'square', '4'], dtype='<U6'), array(['blue', 'star', '4'], dtype='<U6'), array(['green', 'circle', '2'], dtype='<U6'), array(['blue', 'cross', '4'], dtype='<U6'), array(['red', 'cross', '4'], dtype='<U6'), 'SEP', 'C4', 'EOS', array(['red', 'star', '4'], dtype='<U6'), 'SEP', 'C2']\n",
      "Feature for Classification:  0 \n",
      "\n",
      "# Predicted Trials\n",
      "[array(['yellow', 'cross', '2'], dtype='<U6'), array(['yellow', 'square', '2'], dtype='<U6'), array(['green', 'star', '1'], dtype='<U6'), array(['yellow', 'circle', '4'], dtype='<U6'), array(['yellow', 'star', '3'], dtype='<U6'), 'SEP', 'C3', 'EOS', array(['yellow', 'circle', '1'], dtype='<U6'), 'SEP', 'C2']\n",
      "[array(['yellow', 'square', '2'], dtype='<U6'), array(['red', 'cross', '1'], dtype='<U6'), array(['blue', 'circle', '4'], dtype='<U6'), array(['blue', 'star', '4'], dtype='<U6'), array(['red', 'star', '3'], dtype='<U6'), 'SEP', 'C4', 'EOS', array(['blue', 'star', '2'], dtype='<U6'), 'SEP', 'C3']\n",
      "[array(['red', 'circle', '1'], dtype='<U6'), array(['red', 'square', '4'], dtype='<U6'), array(['green', 'star', '3'], dtype='<U6'), array(['yellow', 'cross', '1'], dtype='<U6'), array(['blue', 'circle', '2'], dtype='<U6'), 'SEP', 'C1', 'EOS', array(['yellow', 'square', '1'], dtype='<U6'), 'SEP', 'C4']\n",
      "[array(['green', 'circle', '4'], dtype='<U6'), array(['green', 'cross', '4'], dtype='<U6'), array(['yellow', 'square', '2'], dtype='<U6'), array(['green', 'star', '4'], dtype='<U6'), array(['red', 'square', '3'], dtype='<U6'), 'SEP', 'C3', 'EOS', array(['green', 'square', '3'], dtype='<U6'), 'SEP', 'C1']\n",
      "[array(['blue', 'square', '4'], dtype='<U6'), array(['blue', 'star', '4'], dtype='<U6'), array(['green', 'circle', '2'], dtype='<U6'), array(['blue', 'cross', '4'], dtype='<U6'), array(['red', 'cross', '4'], dtype='<U6'), 'SEP', 'C4', 'EOS', array(['red', 'star', '4'], dtype='<U6'), 'SEP', 'C1']\n",
      "Feature for Classification:  0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "x, target = test_data[:5].to(device), test_targets[:5].to(device)\n",
    "prediction = model_inference(transformer, x, target[:, : -1])\n",
    "\n",
    "print(\"# Actual Trials\")\n",
    "test_batch = [np.asarray(item.cpu()) for item in [x, target]]\n",
    "output = wcst.visualise_batch(test_batch)\n",
    "\n",
    "print(\"# Predicted Trials\")\n",
    "prediction_batch = [np.asarray(item.cpu()) for item in [x, prediction]]\n",
    "output = wcst.visualise_batch(prediction_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd57522b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
