{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd555848",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch as T\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"../..\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93acfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_global_seed(seed: int):\n",
    "    T.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    if T.cuda.is_available():\n",
    "        T.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389721a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "\n",
    "device = T.device('cuda' if T.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "\n",
    "set_global_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08945224",
   "metadata": {},
   "source": [
    "## Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25ea2ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from datasets.custom_dataset import CustomWCSTDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ba4f2c",
   "metadata": {},
   "source": [
    "### 1. Dataset Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e95fdcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "TOTAL_BATCHES = 500\n",
    "TRAIN_TEST_SPLIT_RATIO = 0.6\n",
    "VALIDATION_TEST_SPLIT_RATIO = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e47ffd5",
   "metadata": {},
   "source": [
    "### 2. Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365835c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset Init] Fixed context: 0\n",
      "[Dataset Init] Fixed context: 1\n",
      "[Dataset Init] Fixed context: 2\n",
      "[Dataset Init] Context switched -> 2\n",
      "[Dataset Init] Context switched -> 1\n",
      "[Dataset Init] Context switched -> 2\n",
      "[Dataset Init] Context switched -> 0\n",
      "[Dataset Init] Context switched -> 2\n",
      "[Dataset Init] Context switched -> 1\n",
      "[Dataset Init] Context switched -> 0\n",
      "[Dataset Init] Context switched -> 1\n",
      "[Dataset Init] Context switched -> 0\n",
      "[Dataset Init] Context switched -> 2\n",
      "[Dataset Init] Context switched -> 1\n",
      "[Dataset Init] Context switched -> 2\n",
      "[Dataset Init] Context switched -> 0\n",
      "[Dataset Init] Context switched -> 1\n",
      "[Dataset Init] Context switched -> 0\n",
      "[Dataset Init] Context switched -> 1\n",
      "[Dataset Init] Context switched -> 0\n",
      "[Dataset Init] Context switched -> 1\n"
     ]
    }
   ],
   "source": [
    "train_size = int(TOTAL_BATCHES * TRAIN_TEST_SPLIT_RATIO)\n",
    "validation_size = int((TOTAL_BATCHES - train_size) * VALIDATION_TEST_SPLIT_RATIO)\n",
    "test_size = TOTAL_BATCHES - train_size - validation_size\n",
    "\n",
    "train_dataset = CustomWCSTDataset(total_batches=train_size, sample_batch_size=BATCH_SIZE)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "validation_datasets = {\n",
    "    \"color\": CustomWCSTDataset(total_batches=validation_size // 3, fixed_context=0, sample_batch_size=BATCH_SIZE, allow_switch=False),\n",
    "    \"shape\": CustomWCSTDataset(total_batches=validation_size // 3, fixed_context=1, sample_batch_size=BATCH_SIZE, allow_switch=False),\n",
    "    \"quantity\": CustomWCSTDataset(total_batches=validation_size // 3, fixed_context=2, sample_batch_size=BATCH_SIZE, allow_switch=False)\n",
    "}\n",
    "\n",
    "validation_loaders = {\n",
    "    ctx: DataLoader(ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    for ctx, ds in validation_datasets.items()\n",
    "}\n",
    "\n",
    "test_dataset = CustomWCSTDataset(\n",
    "        total_batches=test_size, sample_batch_size=BATCH_SIZE\n",
    "    )\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f524267",
   "metadata": {},
   "source": [
    "## Transformer Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a8bc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.transformer import BaselineTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7739c3fe",
   "metadata": {},
   "source": [
    "### 1. Transformer Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606c1008",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCABULARY_SIZE = 70        # 64 cards + 4 categories + SEP + EOS\n",
    "EMBEDDING_SIZE = 256        # larger embedding to capture card features\n",
    "N_ATTENTION_HEADS = 8       # more heads for better multi-feature attention\n",
    "N_BLOCKS = 6                # same depth as before\n",
    "MAX_SEQUENCE_LENGTH = 10    # longer max sequence to accommodate multiple past trials\n",
    "FF_DIMS = 1024               # larger feedforward layer for better representation\n",
    "DROPOUT_PROB = 0.2        # reduce dropout slightly to retain signal in small batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab9fc96",
   "metadata": {},
   "source": [
    "### 2. Transformer Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f25ee98",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = BaselineTransformer(\n",
    "    VOCABULARY_SIZE, EMBEDDING_SIZE, N_ATTENTION_HEADS,\n",
    "    N_BLOCKS, MAX_SEQUENCE_LENGTH, FF_DIMS, DROPOUT_PROB, device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7489333",
   "metadata": {},
   "source": [
    "## Training Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f3ebdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch as T\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn, optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c5d23d",
   "metadata": {},
   "source": [
    "### 1. Train, Validate, Evaluate Model Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a2ddcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_per_context(\n",
    "        model: nn.Module, criterion: nn.CrossEntropyLoss, loaders: dict[str, DataLoader], device: T.device | str =\"cpu\"\n",
    "    ):\n",
    "    model.eval()\n",
    "    results = {}\n",
    "\n",
    "    with T.no_grad():\n",
    "        for ctx_name, loader in loaders.items():\n",
    "            total_loss, total_correct, total_samples = 0.0, 0, 0\n",
    "            for input_seq, target in loader:\n",
    "                input_seq, target = input_seq.to(device), target.view(-1).to(device)\n",
    "\n",
    "                logits = model(input_seq)\n",
    "                logits = logits[:, -1, :]\n",
    "                \n",
    "                loss = criterion(logits, target)\n",
    "\n",
    "                preds = logits.argmax(dim=1)\n",
    "                total_loss += loss.item() * target.size(0)\n",
    "                total_correct += (preds == target).sum().item()\n",
    "                total_samples += target.size(0)\n",
    "\n",
    "            avg_loss = total_loss / total_samples\n",
    "            avg_acc = total_correct / total_samples\n",
    "\n",
    "            results[ctx_name] = {\n",
    "                \"loss\": avg_loss,\n",
    "                \"acc\": avg_acc,\n",
    "                \"perplexity\": np.exp(avg_loss)\n",
    "            }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8ee8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    train_loader: DataLoader, validation_loaders: dict[str, DataLoader], model: nn.Module, criterion: nn.CrossEntropyLoss, \n",
    "    optimizer: optim.Optimizer, max_epochs: int = 20, device: str | T.device = \"cpu\"\n",
    "):\n",
    "    history = defaultdict(list)\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{max_epochs}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "        # --- Training ---\n",
    "        model.train()\n",
    "        epoch_train_losses = []\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        for batch_idx, (input_seq, target) in enumerate(train_loader):\n",
    "            input_seq, target = input_seq.to(device), target.view(-1).to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            logits = model(input_seq)  # [batch, seq_len, vocab]\n",
    "            logits = logits[:, -1, :]  # only the final step prediction [batch, vocab]\n",
    "\n",
    "            loss = criterion(logits, target)\n",
    "\n",
    "            # Backprop\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accuracy\n",
    "            preds = logits.argmax(dim=1)\n",
    "            total_correct += (preds == target).sum().item()\n",
    "            total_samples += target.size(0)\n",
    "\n",
    "            epoch_train_losses.append(loss.item())\n",
    "\n",
    "            if batch_idx % 100 == 0 or batch_idx == len(train_loader) - 1:\n",
    "                print(f\"Train Batch {batch_idx+1}/{len(train_loader)} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "        train_loss = np.mean(epoch_train_losses)\n",
    "        train_acc = total_correct / total_samples\n",
    "        train_perplexity = np.exp(train_loss)\n",
    "\n",
    "        history[\"train_losses\"].append(train_loss)\n",
    "        history[\"train_accs\"].append(train_acc)\n",
    "        history[\"train_perplexities\"].append(train_perplexity)\n",
    "\n",
    "        print(f\"[Epoch {epoch+1}] Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Train Perplexity: {train_perplexity:.4f}\")\n",
    "\n",
    "        # --- Validation ---\n",
    "        context_results = validate_per_context(model, criterion, validation_loaders, device)\n",
    "        val_loss = []\n",
    "        val_acc = []\n",
    "        val_perplexity = []\n",
    "        for ctx, metrics in context_results.items():\n",
    "            print(f\"[Validation] {ctx}: Loss={metrics['loss']:.4f} | Acc={metrics['acc']:.4f} | Perplexity={metrics['perplexity']:.4f}\")\n",
    "            val_loss.append(metrics['loss'])\n",
    "            val_acc.append(metrics['acc'])\n",
    "            val_perplexity.append(metrics['perplexity'])\n",
    "\n",
    "        val_loss = np.mean(val_loss)\n",
    "        val_acc = np.mean(val_acc)\n",
    "        val_perplexity = np.mean(val_perplexity)\n",
    "\n",
    "        print(f\"[Validation Summary] Val Loss: {np.mean(val_loss):.4f} | \"\n",
    "              f\"Val Acc: {np.mean(val_acc):.4f} | Val Perplexity: {np.mean(val_perplexity):.4f}\", end=\"\\n\\n\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "        history[\"val_context\"].append(context_results)\n",
    "        history[\"val_losses\"].append(val_loss)\n",
    "        history[\"val_accs\"].append(val_acc)\n",
    "        history[\"val_perplexities\"].append(val_perplexity)\n",
    "\n",
    "    print(\"\\nTraining complete\")\n",
    "\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44864dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(test_loader: DataLoader, model: nn.Module, criterion: nn.CrossEntropyLoss, device: str | T.device = \"cpu\"):\n",
    "\n",
    "    model.eval()\n",
    "    test_batch_losses = []\n",
    "    test_correct = 0\n",
    "    test_tokens = 0\n",
    "\n",
    "    with T.no_grad():\n",
    "        for input_seq, target in test_loader:\n",
    "            input_seq, target = input_seq.to(device), target.view(-1).to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            logits = model(input_seq)  # [batch, seq_len, vocab]\n",
    "            logits = logits[:, -1, :]\n",
    "            loss = criterion(logits, target)\n",
    "\n",
    "            preds = logits.argmax(dim=1)\n",
    "            test_correct += (preds == target).sum().item()\n",
    "            test_tokens += target.size(0)\n",
    "\n",
    "            test_batch_losses.append(loss.item())\n",
    "\n",
    "    test_loss = np.mean(test_batch_losses)\n",
    "    test_acc = test_correct / test_tokens\n",
    "    test_perplexity = np.exp(test_loss)\n",
    "\n",
    "    return {\n",
    "        \"test_loss\": test_loss,\n",
    "        \"test_acc\": test_acc,\n",
    "        \"test_perplexity\": test_perplexity\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d863c67",
   "metadata": {},
   "source": [
    "### 2. Train Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "37f53def",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 3e-4\n",
    "BATCH_SIZE = 64\n",
    "WEIGHT_DECAY = 1e-2\n",
    "WARMUP_STEPS = 400\n",
    "LABEL_SMOOTHING = 0.1\n",
    "MAX_EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c8f99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 1/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 163.1925\n",
      "[quantity] Batch 51/450 | Loss: 4.9344\n",
      "[shape] Batch 101/450 | Loss: 4.5522\n",
      "[color] Batch 151/450 | Loss: 3.7767\n",
      "[quantity] Batch 201/450 | Loss: 3.1851\n",
      "[shape] Batch 251/450 | Loss: 3.3180\n",
      "[color] Batch 301/450 | Loss: 3.0815\n",
      "[quantity] Batch 351/450 | Loss: 3.0033\n",
      "[shape] Batch 401/450 | Loss: 3.1230\n",
      "[quantity] Batch 450/450 | Loss: 3.0852\n",
      "[Epoch 1] Train Loss: 4.6574 | Acc: 0.2407 | Perplexity: 105.3670\n",
      "[Epoch 1] Val Loss: 2.2154 | Acc: 0.2458 | Perplexity: 9.1649\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 2/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 2.8136\n",
      "[quantity] Batch 51/450 | Loss: 2.8871\n",
      "[shape] Batch 101/450 | Loss: 2.4255\n",
      "[color] Batch 151/450 | Loss: 2.9158\n",
      "[quantity] Batch 201/450 | Loss: 2.7198\n",
      "[shape] Batch 251/450 | Loss: 2.9274\n",
      "[color] Batch 301/450 | Loss: 2.6777\n",
      "[quantity] Batch 351/450 | Loss: 2.3512\n",
      "[shape] Batch 401/450 | Loss: 2.4511\n",
      "[quantity] Batch 450/450 | Loss: 2.4266\n",
      "[Epoch 2] Train Loss: 2.7021 | Acc: 0.2441 | Perplexity: 14.9116\n",
      "[Epoch 2] Val Loss: 2.2520 | Acc: 0.2581 | Perplexity: 9.5068\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 3/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 2.4363\n",
      "[quantity] Batch 51/450 | Loss: 2.5463\n",
      "[shape] Batch 101/450 | Loss: 2.6457\n",
      "[color] Batch 151/450 | Loss: 2.3465\n",
      "[quantity] Batch 201/450 | Loss: 2.7365\n",
      "[shape] Batch 251/450 | Loss: 2.9248\n",
      "[color] Batch 301/450 | Loss: 2.5077\n",
      "[quantity] Batch 351/450 | Loss: 2.5777\n",
      "[shape] Batch 401/450 | Loss: 2.2871\n",
      "[quantity] Batch 450/450 | Loss: 2.4112\n",
      "[Epoch 3] Train Loss: 2.5206 | Acc: 0.2476 | Perplexity: 12.4366\n",
      "[Epoch 3] Val Loss: 2.2381 | Acc: 0.2461 | Perplexity: 9.3758\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 4/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 2.3985\n",
      "[quantity] Batch 51/450 | Loss: 2.4373\n",
      "[shape] Batch 101/450 | Loss: 2.3805\n",
      "[color] Batch 151/450 | Loss: 2.4818\n",
      "[quantity] Batch 201/450 | Loss: 2.4703\n",
      "[shape] Batch 251/450 | Loss: 2.4861\n",
      "[color] Batch 301/450 | Loss: 2.3513\n",
      "[quantity] Batch 351/450 | Loss: 2.1977\n",
      "[shape] Batch 401/450 | Loss: 2.3199\n",
      "[quantity] Batch 450/450 | Loss: 2.2179\n",
      "[Epoch 4] Train Loss: 2.3878 | Acc: 0.2488 | Perplexity: 10.8895\n",
      "[Epoch 4] Val Loss: 2.0593 | Acc: 0.2541 | Perplexity: 7.8406\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 5/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 2.3283\n",
      "[quantity] Batch 51/450 | Loss: 2.2543\n",
      "[shape] Batch 101/450 | Loss: 2.2386\n",
      "[color] Batch 151/450 | Loss: 2.2205\n",
      "[quantity] Batch 201/450 | Loss: 2.2017\n",
      "[shape] Batch 251/450 | Loss: 2.3514\n",
      "[color] Batch 301/450 | Loss: 2.1914\n",
      "[quantity] Batch 351/450 | Loss: 2.0959\n",
      "[shape] Batch 401/450 | Loss: 2.1585\n",
      "[quantity] Batch 450/450 | Loss: 2.1293\n",
      "[Epoch 5] Train Loss: 2.2800 | Acc: 0.2481 | Perplexity: 9.7767\n",
      "[Epoch 5] Val Loss: 2.0832 | Acc: 0.2541 | Perplexity: 8.0299\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 6/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 2.1609\n",
      "[quantity] Batch 51/450 | Loss: 2.3797\n",
      "[shape] Batch 101/450 | Loss: 2.2170\n",
      "[color] Batch 151/450 | Loss: 2.4055\n",
      "[quantity] Batch 201/450 | Loss: 2.1133\n",
      "[shape] Batch 251/450 | Loss: 2.1417\n",
      "[color] Batch 301/450 | Loss: 2.3076\n",
      "[quantity] Batch 351/450 | Loss: 2.2480\n",
      "[shape] Batch 401/450 | Loss: 2.1294\n",
      "[quantity] Batch 450/450 | Loss: 2.1955\n",
      "[Epoch 6] Train Loss: 2.2344 | Acc: 0.2531 | Perplexity: 9.3410\n",
      "[Epoch 6] Val Loss: 2.0334 | Acc: 0.2539 | Perplexity: 7.6401\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 7/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 2.1461\n",
      "[quantity] Batch 51/450 | Loss: 2.1453\n",
      "[shape] Batch 101/450 | Loss: 2.1548\n",
      "[color] Batch 151/450 | Loss: 2.2262\n",
      "[quantity] Batch 201/450 | Loss: 2.1469\n",
      "[shape] Batch 251/450 | Loss: 2.0820\n",
      "[color] Batch 301/450 | Loss: 2.2456\n",
      "[quantity] Batch 351/450 | Loss: 2.1496\n",
      "[shape] Batch 401/450 | Loss: 2.0792\n",
      "[quantity] Batch 450/450 | Loss: 2.1757\n",
      "[Epoch 7] Train Loss: 2.1685 | Acc: 0.2505 | Perplexity: 8.7448\n",
      "[Epoch 7] Val Loss: 2.0131 | Acc: 0.2461 | Perplexity: 7.4863\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 8/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 2.1417\n",
      "[quantity] Batch 51/450 | Loss: 2.0273\n",
      "[shape] Batch 101/450 | Loss: 2.1048\n",
      "[color] Batch 151/450 | Loss: 2.1908\n",
      "[quantity] Batch 201/450 | Loss: 2.1167\n",
      "[shape] Batch 251/450 | Loss: 2.0778\n",
      "[color] Batch 301/450 | Loss: 2.1442\n",
      "[quantity] Batch 351/450 | Loss: 2.1043\n",
      "[shape] Batch 401/450 | Loss: 2.1702\n",
      "[quantity] Batch 450/450 | Loss: 2.0712\n",
      "[Epoch 8] Train Loss: 2.1197 | Acc: 0.2556 | Perplexity: 8.3288\n",
      "[Epoch 8] Val Loss: 2.0873 | Acc: 0.2423 | Perplexity: 8.0632\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 9/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 2.2323\n",
      "[quantity] Batch 51/450 | Loss: 2.2953\n",
      "[shape] Batch 101/450 | Loss: 2.1231\n",
      "[color] Batch 151/450 | Loss: 2.2423\n",
      "[quantity] Batch 201/450 | Loss: 2.2086\n",
      "[shape] Batch 251/450 | Loss: 2.1674\n",
      "[color] Batch 301/450 | Loss: 2.0155\n",
      "[quantity] Batch 351/450 | Loss: 2.0155\n",
      "[shape] Batch 401/450 | Loss: 2.0660\n",
      "[quantity] Batch 450/450 | Loss: 2.1093\n",
      "[Epoch 9] Train Loss: 2.1031 | Acc: 0.2540 | Perplexity: 8.1919\n",
      "[Epoch 9] Val Loss: 1.9958 | Acc: 0.2461 | Perplexity: 7.3581\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 10/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 2.0756\n",
      "[quantity] Batch 51/450 | Loss: 2.1072\n",
      "[shape] Batch 101/450 | Loss: 2.0566\n",
      "[color] Batch 151/450 | Loss: 2.0774\n",
      "[quantity] Batch 201/450 | Loss: 2.0157\n",
      "[shape] Batch 251/450 | Loss: 2.0528\n",
      "[color] Batch 301/450 | Loss: 2.0230\n",
      "[quantity] Batch 351/450 | Loss: 2.1756\n",
      "[shape] Batch 401/450 | Loss: 2.0243\n",
      "[quantity] Batch 450/450 | Loss: 2.0941\n",
      "[Epoch 10] Train Loss: 2.0740 | Acc: 0.2488 | Perplexity: 7.9567\n",
      "[Epoch 10] Val Loss: 2.0341 | Acc: 0.2509 | Perplexity: 7.6454\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 11/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 2.1391\n",
      "[quantity] Batch 51/450 | Loss: 2.0510\n",
      "[shape] Batch 101/450 | Loss: 2.0596\n",
      "[color] Batch 151/450 | Loss: 2.0776\n",
      "[quantity] Batch 201/450 | Loss: 2.0778\n",
      "[shape] Batch 251/450 | Loss: 2.0345\n",
      "[color] Batch 301/450 | Loss: 2.0069\n",
      "[quantity] Batch 351/450 | Loss: 2.0641\n",
      "[shape] Batch 401/450 | Loss: 2.0776\n",
      "[quantity] Batch 450/450 | Loss: 2.1985\n",
      "[Epoch 11] Train Loss: 2.0583 | Acc: 0.2533 | Perplexity: 7.8323\n",
      "[Epoch 11] Val Loss: 2.0344 | Acc: 0.2541 | Perplexity: 7.6474\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 12/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 2.0131\n",
      "[quantity] Batch 51/450 | Loss: 2.0129\n",
      "[shape] Batch 101/450 | Loss: 2.0054\n",
      "[color] Batch 151/450 | Loss: 1.9946\n",
      "[quantity] Batch 201/450 | Loss: 2.0204\n",
      "[shape] Batch 251/450 | Loss: 2.1001\n",
      "[color] Batch 301/450 | Loss: 2.0014\n",
      "[quantity] Batch 351/450 | Loss: 2.0368\n",
      "[shape] Batch 401/450 | Loss: 1.9874\n",
      "[quantity] Batch 450/450 | Loss: 2.0335\n",
      "[Epoch 12] Train Loss: 2.0375 | Acc: 0.2565 | Perplexity: 7.6717\n",
      "[Epoch 12] Val Loss: 1.9832 | Acc: 0.2562 | Perplexity: 7.2661\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 13/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 2.0408\n",
      "[quantity] Batch 51/450 | Loss: 2.0794\n",
      "[shape] Batch 101/450 | Loss: 2.0962\n",
      "[color] Batch 151/450 | Loss: 2.0303\n",
      "[quantity] Batch 201/450 | Loss: 2.2436\n",
      "[shape] Batch 251/450 | Loss: 2.1315\n",
      "[color] Batch 301/450 | Loss: 2.0097\n",
      "[quantity] Batch 351/450 | Loss: 2.0243\n",
      "[shape] Batch 401/450 | Loss: 2.0723\n",
      "[quantity] Batch 450/450 | Loss: 2.0621\n",
      "[Epoch 13] Train Loss: 2.0321 | Acc: 0.2493 | Perplexity: 7.6299\n",
      "[Epoch 13] Val Loss: 1.9733 | Acc: 0.2419 | Perplexity: 7.1946\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 14/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 2.0235\n",
      "[quantity] Batch 51/450 | Loss: 2.0250\n",
      "[shape] Batch 101/450 | Loss: 1.9825\n",
      "[color] Batch 151/450 | Loss: 1.9876\n",
      "[quantity] Batch 201/450 | Loss: 2.0239\n",
      "[shape] Batch 251/450 | Loss: 2.0431\n",
      "[color] Batch 301/450 | Loss: 2.0561\n",
      "[quantity] Batch 351/450 | Loss: 2.0594\n",
      "[shape] Batch 401/450 | Loss: 2.0232\n",
      "[quantity] Batch 450/450 | Loss: 2.0270\n",
      "[Epoch 14] Train Loss: 2.0197 | Acc: 0.2533 | Perplexity: 7.5363\n",
      "[Epoch 14] Val Loss: 1.9935 | Acc: 0.2575 | Perplexity: 7.3415\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 15/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.9789\n",
      "[quantity] Batch 51/450 | Loss: 2.0677\n",
      "[shape] Batch 101/450 | Loss: 2.0283\n",
      "[color] Batch 151/450 | Loss: 2.0124\n",
      "[quantity] Batch 201/450 | Loss: 1.9861\n",
      "[shape] Batch 251/450 | Loss: 1.9995\n",
      "[color] Batch 301/450 | Loss: 2.0242\n",
      "[quantity] Batch 351/450 | Loss: 2.0013\n",
      "[shape] Batch 401/450 | Loss: 1.9812\n",
      "[quantity] Batch 450/450 | Loss: 2.0741\n",
      "[Epoch 15] Train Loss: 2.0125 | Acc: 0.2530 | Perplexity: 7.4822\n",
      "[Epoch 15] Val Loss: 1.9759 | Acc: 0.2512 | Perplexity: 7.2131\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 16/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.9803\n",
      "[quantity] Batch 51/450 | Loss: 1.9872\n",
      "[shape] Batch 101/450 | Loss: 1.9807\n",
      "[color] Batch 151/450 | Loss: 2.0713\n",
      "[quantity] Batch 201/450 | Loss: 1.9888\n",
      "[shape] Batch 251/450 | Loss: 2.0182\n",
      "[color] Batch 301/450 | Loss: 1.9728\n",
      "[quantity] Batch 351/450 | Loss: 1.9415\n",
      "[shape] Batch 401/450 | Loss: 1.9641\n",
      "[quantity] Batch 450/450 | Loss: 2.0621\n",
      "[Epoch 16] Train Loss: 2.0024 | Acc: 0.2587 | Perplexity: 7.4070\n",
      "[Epoch 16] Val Loss: 1.9732 | Acc: 0.2672 | Perplexity: 7.1936\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 17/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.9891\n",
      "[quantity] Batch 51/450 | Loss: 2.1174\n",
      "[shape] Batch 101/450 | Loss: 1.9915\n",
      "[color] Batch 151/450 | Loss: 1.9773\n",
      "[quantity] Batch 201/450 | Loss: 1.9760\n",
      "[shape] Batch 251/450 | Loss: 2.0166\n",
      "[color] Batch 301/450 | Loss: 2.0317\n",
      "[quantity] Batch 351/450 | Loss: 2.0186\n",
      "[shape] Batch 401/450 | Loss: 2.0340\n",
      "[quantity] Batch 450/450 | Loss: 1.9607\n",
      "[Epoch 17] Train Loss: 2.0027 | Acc: 0.2568 | Perplexity: 7.4087\n",
      "[Epoch 17] Val Loss: 1.9868 | Acc: 0.2527 | Perplexity: 7.2924\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 18/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.9843\n",
      "[quantity] Batch 51/450 | Loss: 1.9667\n",
      "[shape] Batch 101/450 | Loss: 2.0095\n",
      "[color] Batch 151/450 | Loss: 1.9850\n",
      "[quantity] Batch 201/450 | Loss: 1.8780\n",
      "[shape] Batch 251/450 | Loss: 1.9872\n",
      "[color] Batch 301/450 | Loss: 1.9825\n",
      "[quantity] Batch 351/450 | Loss: 1.9481\n",
      "[shape] Batch 401/450 | Loss: 2.0208\n",
      "[quantity] Batch 450/450 | Loss: 1.9601\n",
      "[Epoch 18] Train Loss: 1.9950 | Acc: 0.2661 | Perplexity: 7.3520\n",
      "[Epoch 18] Val Loss: 1.9645 | Acc: 0.2634 | Perplexity: 7.1316\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 19/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.9671\n",
      "[quantity] Batch 51/450 | Loss: 1.9770\n",
      "[shape] Batch 101/450 | Loss: 2.0324\n",
      "[color] Batch 151/450 | Loss: 2.0495\n",
      "[quantity] Batch 201/450 | Loss: 1.9985\n",
      "[shape] Batch 251/450 | Loss: 2.0221\n",
      "[color] Batch 301/450 | Loss: 1.9720\n",
      "[quantity] Batch 351/450 | Loss: 1.9966\n",
      "[shape] Batch 401/450 | Loss: 2.0358\n",
      "[quantity] Batch 450/450 | Loss: 1.9969\n",
      "[Epoch 19] Train Loss: 1.9905 | Acc: 0.2615 | Perplexity: 7.3195\n",
      "[Epoch 19] Val Loss: 1.9632 | Acc: 0.2706 | Perplexity: 7.1220\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 20/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.9972\n",
      "[quantity] Batch 51/450 | Loss: 1.9184\n",
      "[shape] Batch 101/450 | Loss: 2.0038\n",
      "[color] Batch 151/450 | Loss: 1.9895\n",
      "[quantity] Batch 201/450 | Loss: 1.9790\n",
      "[shape] Batch 251/450 | Loss: 1.9748\n",
      "[color] Batch 301/450 | Loss: 1.9915\n",
      "[quantity] Batch 351/450 | Loss: 1.9664\n",
      "[shape] Batch 401/450 | Loss: 1.9784\n",
      "[quantity] Batch 450/450 | Loss: 2.0196\n",
      "[Epoch 20] Train Loss: 1.9821 | Acc: 0.2752 | Perplexity: 7.2579\n",
      "[Epoch 20] Val Loss: 1.9936 | Acc: 0.2725 | Perplexity: 7.3419\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 21/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 2.0595\n",
      "[quantity] Batch 51/450 | Loss: 1.9013\n",
      "[shape] Batch 101/450 | Loss: 2.0124\n",
      "[color] Batch 151/450 | Loss: 1.9507\n",
      "[quantity] Batch 201/450 | Loss: 2.0227\n",
      "[shape] Batch 251/450 | Loss: 1.9128\n",
      "[color] Batch 301/450 | Loss: 1.9729\n",
      "[quantity] Batch 351/450 | Loss: 1.9123\n",
      "[shape] Batch 401/450 | Loss: 2.0053\n",
      "[quantity] Batch 450/450 | Loss: 1.9507\n",
      "[Epoch 21] Train Loss: 1.9799 | Acc: 0.2819 | Perplexity: 7.2418\n",
      "[Epoch 21] Val Loss: 1.9581 | Acc: 0.2903 | Perplexity: 7.0862\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 22/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.9651\n",
      "[quantity] Batch 51/450 | Loss: 1.9902\n",
      "[shape] Batch 101/450 | Loss: 2.0070\n",
      "[color] Batch 151/450 | Loss: 1.9505\n",
      "[quantity] Batch 201/450 | Loss: 1.9415\n",
      "[shape] Batch 251/450 | Loss: 1.9753\n",
      "[color] Batch 301/450 | Loss: 1.9401\n",
      "[quantity] Batch 351/450 | Loss: 1.9656\n",
      "[shape] Batch 401/450 | Loss: 2.0273\n",
      "[quantity] Batch 450/450 | Loss: 1.9090\n",
      "[Epoch 22] Train Loss: 1.9737 | Acc: 0.2849 | Perplexity: 7.1975\n",
      "[Epoch 22] Val Loss: 1.9693 | Acc: 0.2680 | Perplexity: 7.1654\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 23/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.9622\n",
      "[quantity] Batch 51/450 | Loss: 1.8826\n",
      "[shape] Batch 101/450 | Loss: 1.9965\n",
      "[color] Batch 151/450 | Loss: 1.9488\n",
      "[quantity] Batch 201/450 | Loss: 1.9161\n",
      "[shape] Batch 251/450 | Loss: 2.0120\n",
      "[color] Batch 301/450 | Loss: 1.9891\n",
      "[quantity] Batch 351/450 | Loss: 1.9611\n",
      "[shape] Batch 401/450 | Loss: 2.0594\n",
      "[quantity] Batch 450/450 | Loss: 1.9231\n",
      "[Epoch 23] Train Loss: 1.9685 | Acc: 0.2980 | Perplexity: 7.1601\n",
      "[Epoch 23] Val Loss: 1.9579 | Acc: 0.2998 | Perplexity: 7.0845\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 24/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 2.0207\n",
      "[quantity] Batch 51/450 | Loss: 1.8994\n",
      "[shape] Batch 101/450 | Loss: 1.9401\n",
      "[color] Batch 151/450 | Loss: 2.0115\n",
      "[quantity] Batch 201/450 | Loss: 1.8950\n",
      "[shape] Batch 251/450 | Loss: 1.9793\n",
      "[color] Batch 301/450 | Loss: 1.9968\n",
      "[quantity] Batch 351/450 | Loss: 1.9079\n",
      "[shape] Batch 401/450 | Loss: 1.9501\n",
      "[quantity] Batch 450/450 | Loss: 1.8371\n",
      "[Epoch 24] Train Loss: 1.9546 | Acc: 0.3115 | Perplexity: 7.0609\n",
      "[Epoch 24] Val Loss: 1.9881 | Acc: 0.2822 | Perplexity: 7.3016\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 25/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.9665\n",
      "[quantity] Batch 51/450 | Loss: 1.8503\n",
      "[shape] Batch 101/450 | Loss: 1.9414\n",
      "[color] Batch 151/450 | Loss: 1.9910\n",
      "[quantity] Batch 201/450 | Loss: 1.8737\n",
      "[shape] Batch 251/450 | Loss: 2.0142\n",
      "[color] Batch 301/450 | Loss: 2.0012\n",
      "[quantity] Batch 351/450 | Loss: 1.8952\n",
      "[shape] Batch 401/450 | Loss: 2.0074\n",
      "[quantity] Batch 450/450 | Loss: 1.8637\n",
      "[Epoch 25] Train Loss: 1.9501 | Acc: 0.3189 | Perplexity: 7.0297\n",
      "[Epoch 25] Val Loss: 1.9564 | Acc: 0.3119 | Perplexity: 7.0741\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 26/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.9863\n",
      "[quantity] Batch 51/450 | Loss: 1.7738\n",
      "[shape] Batch 101/450 | Loss: 2.0265\n",
      "[color] Batch 151/450 | Loss: 1.9420\n",
      "[quantity] Batch 201/450 | Loss: 1.8295\n",
      "[shape] Batch 251/450 | Loss: 1.9988\n",
      "[color] Batch 301/450 | Loss: 2.0355\n",
      "[quantity] Batch 351/450 | Loss: 1.8307\n",
      "[shape] Batch 401/450 | Loss: 1.9836\n",
      "[quantity] Batch 450/450 | Loss: 1.7565\n",
      "[Epoch 26] Train Loss: 1.9340 | Acc: 0.3353 | Perplexity: 6.9169\n",
      "[Epoch 26] Val Loss: 1.9501 | Acc: 0.3272 | Perplexity: 7.0295\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 27/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 2.0136\n",
      "[quantity] Batch 51/450 | Loss: 1.8217\n",
      "[shape] Batch 101/450 | Loss: 1.9695\n",
      "[color] Batch 151/450 | Loss: 1.9770\n",
      "[quantity] Batch 201/450 | Loss: 1.8568\n",
      "[shape] Batch 251/450 | Loss: 2.0519\n",
      "[color] Batch 301/450 | Loss: 1.9582\n",
      "[quantity] Batch 351/450 | Loss: 1.7509\n",
      "[shape] Batch 401/450 | Loss: 2.0259\n",
      "[quantity] Batch 450/450 | Loss: 1.7333\n",
      "[Epoch 27] Train Loss: 1.9221 | Acc: 0.3489 | Perplexity: 6.8353\n",
      "[Epoch 27] Val Loss: 1.9429 | Acc: 0.3480 | Perplexity: 6.9787\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 28/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 2.0156\n",
      "[quantity] Batch 51/450 | Loss: 1.7085\n",
      "[shape] Batch 101/450 | Loss: 2.0556\n",
      "[color] Batch 151/450 | Loss: 1.9256\n",
      "[quantity] Batch 201/450 | Loss: 1.7160\n",
      "[shape] Batch 251/450 | Loss: 2.0291\n",
      "[color] Batch 301/450 | Loss: 1.9557\n",
      "[quantity] Batch 351/450 | Loss: 1.7562\n",
      "[shape] Batch 401/450 | Loss: 2.0946\n",
      "[quantity] Batch 450/450 | Loss: 1.6826\n",
      "[Epoch 28] Train Loss: 1.9071 | Acc: 0.3685 | Perplexity: 6.7334\n",
      "[Epoch 28] Val Loss: 1.9316 | Acc: 0.3600 | Perplexity: 6.9005\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 29/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 2.0488\n",
      "[quantity] Batch 51/450 | Loss: 1.7160\n",
      "[shape] Batch 101/450 | Loss: 1.9682\n",
      "[color] Batch 151/450 | Loss: 2.0429\n",
      "[quantity] Batch 201/450 | Loss: 1.7327\n",
      "[shape] Batch 251/450 | Loss: 2.0975\n",
      "[color] Batch 301/450 | Loss: 2.0649\n",
      "[quantity] Batch 351/450 | Loss: 1.6864\n",
      "[shape] Batch 401/450 | Loss: 2.0051\n",
      "[quantity] Batch 450/450 | Loss: 1.5541\n",
      "[Epoch 29] Train Loss: 1.8839 | Acc: 0.3950 | Perplexity: 6.5789\n",
      "[Epoch 29] Val Loss: 1.9208 | Acc: 0.3808 | Perplexity: 6.8266\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 30/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.9161\n",
      "[quantity] Batch 51/450 | Loss: 1.5808\n",
      "[shape] Batch 101/450 | Loss: 2.1289\n",
      "[color] Batch 151/450 | Loss: 2.1622\n",
      "[quantity] Batch 201/450 | Loss: 1.5727\n",
      "[shape] Batch 251/450 | Loss: 2.0022\n",
      "[color] Batch 301/450 | Loss: 2.0369\n",
      "[quantity] Batch 351/450 | Loss: 1.5870\n",
      "[shape] Batch 401/450 | Loss: 2.0241\n",
      "[quantity] Batch 450/450 | Loss: 1.5932\n",
      "[Epoch 30] Train Loss: 1.8505 | Acc: 0.4253 | Perplexity: 6.3632\n",
      "[Epoch 30] Val Loss: 1.9007 | Acc: 0.4130 | Perplexity: 6.6903\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 31/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.9946\n",
      "[quantity] Batch 51/450 | Loss: 1.4346\n",
      "[shape] Batch 101/450 | Loss: 2.0256\n",
      "[color] Batch 151/450 | Loss: 2.0859\n",
      "[quantity] Batch 201/450 | Loss: 1.4435\n",
      "[shape] Batch 251/450 | Loss: 2.0906\n",
      "[color] Batch 301/450 | Loss: 2.0596\n",
      "[quantity] Batch 351/450 | Loss: 1.4534\n",
      "[shape] Batch 401/450 | Loss: 2.1139\n",
      "[quantity] Batch 450/450 | Loss: 1.4296\n",
      "[Epoch 31] Train Loss: 1.8163 | Acc: 0.4548 | Perplexity: 6.1491\n",
      "[Epoch 31] Val Loss: 1.8622 | Acc: 0.4523 | Perplexity: 6.4377\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 32/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 2.0684\n",
      "[quantity] Batch 51/450 | Loss: 1.4401\n",
      "[shape] Batch 101/450 | Loss: 1.9197\n",
      "[color] Batch 151/450 | Loss: 1.9733\n",
      "[quantity] Batch 201/450 | Loss: 1.4197\n",
      "[shape] Batch 251/450 | Loss: 1.9916\n",
      "[color] Batch 301/450 | Loss: 2.1322\n",
      "[quantity] Batch 351/450 | Loss: 1.3519\n",
      "[shape] Batch 401/450 | Loss: 1.9415\n",
      "[quantity] Batch 450/450 | Loss: 1.2804\n",
      "[Epoch 32] Train Loss: 1.7800 | Acc: 0.4846 | Perplexity: 5.9300\n",
      "[Epoch 32] Val Loss: 1.8104 | Acc: 0.4727 | Perplexity: 6.1132\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 33/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.9180\n",
      "[quantity] Batch 51/450 | Loss: 1.3333\n",
      "[shape] Batch 101/450 | Loss: 2.0105\n",
      "[color] Batch 151/450 | Loss: 2.0241\n",
      "[quantity] Batch 201/450 | Loss: 1.3713\n",
      "[shape] Batch 251/450 | Loss: 1.9631\n",
      "[color] Batch 301/450 | Loss: 1.8860\n",
      "[quantity] Batch 351/450 | Loss: 1.3675\n",
      "[shape] Batch 401/450 | Loss: 1.7757\n",
      "[quantity] Batch 450/450 | Loss: 1.3702\n",
      "[Epoch 33] Train Loss: 1.7529 | Acc: 0.5027 | Perplexity: 5.7713\n",
      "[Epoch 33] Val Loss: 1.8300 | Acc: 0.4839 | Perplexity: 6.2340\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 34/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 2.0792\n",
      "[quantity] Batch 51/450 | Loss: 1.2443\n",
      "[shape] Batch 101/450 | Loss: 1.9782\n",
      "[color] Batch 151/450 | Loss: 1.8846\n",
      "[quantity] Batch 201/450 | Loss: 1.2561\n",
      "[shape] Batch 251/450 | Loss: 2.0790\n",
      "[color] Batch 301/450 | Loss: 1.9955\n",
      "[quantity] Batch 351/450 | Loss: 1.2226\n",
      "[shape] Batch 401/450 | Loss: 1.9820\n",
      "[quantity] Batch 450/450 | Loss: 1.1570\n",
      "[Epoch 34] Train Loss: 1.7203 | Acc: 0.5225 | Perplexity: 5.5860\n",
      "[Epoch 34] Val Loss: 1.8094 | Acc: 0.5033 | Perplexity: 6.1066\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 35/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.8459\n",
      "[quantity] Batch 51/450 | Loss: 1.2515\n",
      "[shape] Batch 101/450 | Loss: 1.8399\n",
      "[color] Batch 151/450 | Loss: 1.8839\n",
      "[quantity] Batch 201/450 | Loss: 1.1838\n",
      "[shape] Batch 251/450 | Loss: 2.0854\n",
      "[color] Batch 301/450 | Loss: 1.9186\n",
      "[quantity] Batch 351/450 | Loss: 1.1628\n",
      "[shape] Batch 401/450 | Loss: 1.8477\n",
      "[quantity] Batch 450/450 | Loss: 1.1540\n",
      "[Epoch 35] Train Loss: 1.6905 | Acc: 0.5388 | Perplexity: 5.4223\n",
      "[Epoch 35] Val Loss: 1.7456 | Acc: 0.5250 | Perplexity: 5.7291\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 36/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.8642\n",
      "[quantity] Batch 51/450 | Loss: 1.2167\n",
      "[shape] Batch 101/450 | Loss: 1.8117\n",
      "[color] Batch 151/450 | Loss: 1.7171\n",
      "[quantity] Batch 201/450 | Loss: 1.2138\n",
      "[shape] Batch 251/450 | Loss: 2.0025\n",
      "[color] Batch 301/450 | Loss: 1.7949\n",
      "[quantity] Batch 351/450 | Loss: 1.1985\n",
      "[shape] Batch 401/450 | Loss: 1.8349\n",
      "[quantity] Batch 450/450 | Loss: 1.2116\n",
      "[Epoch 36] Train Loss: 1.6611 | Acc: 0.5542 | Perplexity: 5.2653\n",
      "[Epoch 36] Val Loss: 1.7157 | Acc: 0.5308 | Perplexity: 5.5606\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 37/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.8617\n",
      "[quantity] Batch 51/450 | Loss: 1.0875\n",
      "[shape] Batch 101/450 | Loss: 1.7539\n",
      "[color] Batch 151/450 | Loss: 1.9215\n",
      "[quantity] Batch 201/450 | Loss: 1.1435\n",
      "[shape] Batch 251/450 | Loss: 1.7638\n",
      "[color] Batch 301/450 | Loss: 1.8178\n",
      "[quantity] Batch 351/450 | Loss: 1.1519\n",
      "[shape] Batch 401/450 | Loss: 1.9126\n",
      "[quantity] Batch 450/450 | Loss: 1.2512\n",
      "[Epoch 37] Train Loss: 1.6352 | Acc: 0.5691 | Perplexity: 5.1305\n",
      "[Epoch 37] Val Loss: 1.6979 | Acc: 0.5483 | Perplexity: 5.4625\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 38/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.8095\n",
      "[quantity] Batch 51/450 | Loss: 1.1188\n",
      "[shape] Batch 101/450 | Loss: 1.7949\n",
      "[color] Batch 151/450 | Loss: 1.8253\n",
      "[quantity] Batch 201/450 | Loss: 1.0621\n",
      "[shape] Batch 251/450 | Loss: 1.8474\n",
      "[color] Batch 301/450 | Loss: 1.7699\n",
      "[quantity] Batch 351/450 | Loss: 1.0672\n",
      "[shape] Batch 401/450 | Loss: 1.8714\n",
      "[quantity] Batch 450/450 | Loss: 1.1908\n",
      "[Epoch 38] Train Loss: 1.6051 | Acc: 0.5801 | Perplexity: 4.9782\n",
      "[Epoch 38] Val Loss: 1.6472 | Acc: 0.5648 | Perplexity: 5.1922\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 39/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.7835\n",
      "[quantity] Batch 51/450 | Loss: 1.1732\n",
      "[shape] Batch 101/450 | Loss: 1.7687\n",
      "[color] Batch 151/450 | Loss: 2.0013\n",
      "[quantity] Batch 201/450 | Loss: 1.1987\n",
      "[shape] Batch 251/450 | Loss: 1.5949\n",
      "[color] Batch 301/450 | Loss: 1.8126\n",
      "[quantity] Batch 351/450 | Loss: 1.0698\n",
      "[shape] Batch 401/450 | Loss: 1.7066\n",
      "[quantity] Batch 450/450 | Loss: 1.1700\n",
      "[Epoch 39] Train Loss: 1.5776 | Acc: 0.5998 | Perplexity: 4.8435\n",
      "[Epoch 39] Val Loss: 1.6335 | Acc: 0.5852 | Perplexity: 5.1218\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 40/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.7885\n",
      "[quantity] Batch 51/450 | Loss: 1.0888\n",
      "[shape] Batch 101/450 | Loss: 1.5476\n",
      "[color] Batch 151/450 | Loss: 1.9076\n",
      "[quantity] Batch 201/450 | Loss: 1.1481\n",
      "[shape] Batch 251/450 | Loss: 1.8494\n",
      "[color] Batch 301/450 | Loss: 1.7856\n",
      "[quantity] Batch 351/450 | Loss: 1.1718\n",
      "[shape] Batch 401/450 | Loss: 1.7092\n",
      "[quantity] Batch 450/450 | Loss: 1.0173\n",
      "[Epoch 40] Train Loss: 1.5492 | Acc: 0.6154 | Perplexity: 4.7079\n",
      "[Epoch 40] Val Loss: 1.5864 | Acc: 0.6080 | Perplexity: 4.8863\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 41/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.7460\n",
      "[quantity] Batch 51/450 | Loss: 1.0440\n",
      "[shape] Batch 101/450 | Loss: 1.7289\n",
      "[color] Batch 151/450 | Loss: 1.8556\n",
      "[quantity] Batch 201/450 | Loss: 1.0578\n",
      "[shape] Batch 251/450 | Loss: 1.8293\n",
      "[color] Batch 301/450 | Loss: 1.8544\n",
      "[quantity] Batch 351/450 | Loss: 1.1703\n",
      "[shape] Batch 401/450 | Loss: 1.6892\n",
      "[quantity] Batch 450/450 | Loss: 1.0620\n",
      "[Epoch 41] Train Loss: 1.5111 | Acc: 0.6364 | Perplexity: 4.5318\n",
      "[Epoch 41] Val Loss: 1.5614 | Acc: 0.6312 | Perplexity: 4.7655\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 42/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.7362\n",
      "[quantity] Batch 51/450 | Loss: 1.0544\n",
      "[shape] Batch 101/450 | Loss: 1.5994\n",
      "[color] Batch 151/450 | Loss: 1.6239\n",
      "[quantity] Batch 201/450 | Loss: 1.1246\n",
      "[shape] Batch 251/450 | Loss: 1.7676\n",
      "[color] Batch 301/450 | Loss: 1.7704\n",
      "[quantity] Batch 351/450 | Loss: 1.1301\n",
      "[shape] Batch 401/450 | Loss: 1.7134\n",
      "[quantity] Batch 450/450 | Loss: 0.9800\n",
      "[Epoch 42] Train Loss: 1.4800 | Acc: 0.6566 | Perplexity: 4.3928\n",
      "[Epoch 42] Val Loss: 1.5424 | Acc: 0.6573 | Perplexity: 4.6759\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 43/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.5660\n",
      "[quantity] Batch 51/450 | Loss: 0.9947\n",
      "[shape] Batch 101/450 | Loss: 1.5265\n",
      "[color] Batch 151/450 | Loss: 1.5142\n",
      "[quantity] Batch 201/450 | Loss: 1.0286\n",
      "[shape] Batch 251/450 | Loss: 1.6808\n",
      "[color] Batch 301/450 | Loss: 1.7544\n",
      "[quantity] Batch 351/450 | Loss: 1.1592\n",
      "[shape] Batch 401/450 | Loss: 1.9389\n",
      "[quantity] Batch 450/450 | Loss: 1.0601\n",
      "[Epoch 43] Train Loss: 1.4462 | Acc: 0.6809 | Perplexity: 4.2469\n",
      "[Epoch 43] Val Loss: 1.4480 | Acc: 0.7067 | Perplexity: 4.2544\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 44/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.5875\n",
      "[quantity] Batch 51/450 | Loss: 1.1804\n",
      "[shape] Batch 101/450 | Loss: 1.5584\n",
      "[color] Batch 151/450 | Loss: 1.6277\n",
      "[quantity] Batch 201/450 | Loss: 1.0422\n",
      "[shape] Batch 251/450 | Loss: 1.7020\n",
      "[color] Batch 301/450 | Loss: 1.5716\n",
      "[quantity] Batch 351/450 | Loss: 0.9638\n",
      "[shape] Batch 401/450 | Loss: 1.5739\n",
      "[quantity] Batch 450/450 | Loss: 1.0120\n",
      "[Epoch 44] Train Loss: 1.4190 | Acc: 0.7000 | Perplexity: 4.1329\n",
      "[Epoch 44] Val Loss: 1.4330 | Acc: 0.7214 | Perplexity: 4.1914\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 45/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.5691\n",
      "[quantity] Batch 51/450 | Loss: 0.9005\n",
      "[shape] Batch 101/450 | Loss: 1.5670\n",
      "[color] Batch 151/450 | Loss: 1.5516\n",
      "[quantity] Batch 201/450 | Loss: 1.0019\n",
      "[shape] Batch 251/450 | Loss: 1.6386\n",
      "[color] Batch 301/450 | Loss: 1.5267\n",
      "[quantity] Batch 351/450 | Loss: 0.9464\n",
      "[shape] Batch 401/450 | Loss: 1.7027\n",
      "[quantity] Batch 450/450 | Loss: 0.8958\n",
      "[Epoch 45] Train Loss: 1.3831 | Acc: 0.7169 | Perplexity: 3.9873\n",
      "[Epoch 45] Val Loss: 1.4166 | Acc: 0.7319 | Perplexity: 4.1230\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 46/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.4231\n",
      "[quantity] Batch 51/450 | Loss: 1.0635\n",
      "[shape] Batch 101/450 | Loss: 1.2885\n",
      "[color] Batch 151/450 | Loss: 1.3363\n",
      "[quantity] Batch 201/450 | Loss: 1.0451\n",
      "[shape] Batch 251/450 | Loss: 1.6585\n",
      "[color] Batch 301/450 | Loss: 1.4026\n",
      "[quantity] Batch 351/450 | Loss: 0.9530\n",
      "[shape] Batch 401/450 | Loss: 1.4482\n",
      "[quantity] Batch 450/450 | Loss: 0.9509\n",
      "[Epoch 46] Train Loss: 1.3611 | Acc: 0.7309 | Perplexity: 3.9006\n",
      "[Epoch 46] Val Loss: 1.3723 | Acc: 0.7445 | Perplexity: 3.9445\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 47/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.6119\n",
      "[quantity] Batch 51/450 | Loss: 1.0554\n",
      "[shape] Batch 101/450 | Loss: 1.4546\n",
      "[color] Batch 151/450 | Loss: 1.4787\n",
      "[quantity] Batch 201/450 | Loss: 0.9641\n",
      "[shape] Batch 251/450 | Loss: 1.4777\n",
      "[color] Batch 301/450 | Loss: 1.3732\n",
      "[quantity] Batch 351/450 | Loss: 0.9917\n",
      "[shape] Batch 401/450 | Loss: 1.5774\n",
      "[quantity] Batch 450/450 | Loss: 0.8963\n",
      "[Epoch 47] Train Loss: 1.3397 | Acc: 0.7413 | Perplexity: 3.8180\n",
      "[Epoch 47] Val Loss: 1.3571 | Acc: 0.7578 | Perplexity: 3.8851\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 48/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.4853\n",
      "[quantity] Batch 51/450 | Loss: 0.9808\n",
      "[shape] Batch 101/450 | Loss: 1.4327\n",
      "[color] Batch 151/450 | Loss: 1.5390\n",
      "[quantity] Batch 201/450 | Loss: 1.0170\n",
      "[shape] Batch 251/450 | Loss: 1.2338\n",
      "[color] Batch 301/450 | Loss: 1.5324\n",
      "[quantity] Batch 351/450 | Loss: 0.9827\n",
      "[shape] Batch 401/450 | Loss: 1.4993\n",
      "[quantity] Batch 450/450 | Loss: 0.9868\n",
      "[Epoch 48] Train Loss: 1.3161 | Acc: 0.7502 | Perplexity: 3.7290\n",
      "[Epoch 48] Val Loss: 1.3518 | Acc: 0.7577 | Perplexity: 3.8645\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 49/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.5618\n",
      "[quantity] Batch 51/450 | Loss: 0.9657\n",
      "[shape] Batch 101/450 | Loss: 1.7135\n",
      "[color] Batch 151/450 | Loss: 1.5178\n",
      "[quantity] Batch 201/450 | Loss: 0.9593\n",
      "[shape] Batch 251/450 | Loss: 1.5972\n",
      "[color] Batch 301/450 | Loss: 1.5419\n",
      "[quantity] Batch 351/450 | Loss: 0.9057\n",
      "[shape] Batch 401/450 | Loss: 1.4373\n",
      "[quantity] Batch 450/450 | Loss: 0.9448\n",
      "[Epoch 49] Train Loss: 1.2973 | Acc: 0.7607 | Perplexity: 3.6593\n",
      "[Epoch 49] Val Loss: 1.3348 | Acc: 0.7634 | Perplexity: 3.7993\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 50/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.2263\n",
      "[quantity] Batch 51/450 | Loss: 0.8623\n",
      "[shape] Batch 101/450 | Loss: 1.2610\n",
      "[color] Batch 151/450 | Loss: 1.6334\n",
      "[quantity] Batch 201/450 | Loss: 0.9244\n",
      "[shape] Batch 251/450 | Loss: 1.4801\n",
      "[color] Batch 301/450 | Loss: 1.5032\n",
      "[quantity] Batch 351/450 | Loss: 0.9644\n",
      "[shape] Batch 401/450 | Loss: 1.3957\n",
      "[quantity] Batch 450/450 | Loss: 0.9307\n",
      "[Epoch 50] Train Loss: 1.2705 | Acc: 0.7717 | Perplexity: 3.5628\n",
      "[Epoch 50] Val Loss: 1.2958 | Acc: 0.7756 | Perplexity: 3.6541\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 51/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.3551\n",
      "[quantity] Batch 51/450 | Loss: 0.8731\n",
      "[shape] Batch 101/450 | Loss: 1.4242\n",
      "[color] Batch 151/450 | Loss: 1.4689\n",
      "[quantity] Batch 201/450 | Loss: 0.9111\n",
      "[shape] Batch 251/450 | Loss: 1.4659\n",
      "[color] Batch 301/450 | Loss: 1.4449\n",
      "[quantity] Batch 351/450 | Loss: 0.9493\n",
      "[shape] Batch 401/450 | Loss: 1.6697\n",
      "[quantity] Batch 450/450 | Loss: 0.8755\n",
      "[Epoch 51] Train Loss: 1.2584 | Acc: 0.7781 | Perplexity: 3.5199\n",
      "[Epoch 51] Val Loss: 1.2892 | Acc: 0.7791 | Perplexity: 3.6298\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 52/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.3213\n",
      "[quantity] Batch 51/450 | Loss: 0.8867\n",
      "[shape] Batch 101/450 | Loss: 1.3621\n",
      "[color] Batch 151/450 | Loss: 1.4202\n",
      "[quantity] Batch 201/450 | Loss: 0.9284\n",
      "[shape] Batch 251/450 | Loss: 1.1881\n",
      "[color] Batch 301/450 | Loss: 1.1766\n",
      "[quantity] Batch 351/450 | Loss: 0.8967\n",
      "[shape] Batch 401/450 | Loss: 1.5070\n",
      "[quantity] Batch 450/450 | Loss: 1.0165\n",
      "[Epoch 52] Train Loss: 1.2293 | Acc: 0.7870 | Perplexity: 3.4187\n",
      "[Epoch 52] Val Loss: 1.2639 | Acc: 0.7780 | Perplexity: 3.5391\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 53/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.5358\n",
      "[quantity] Batch 51/450 | Loss: 0.9089\n",
      "[shape] Batch 101/450 | Loss: 1.3379\n",
      "[color] Batch 151/450 | Loss: 1.3023\n",
      "[quantity] Batch 201/450 | Loss: 0.9029\n",
      "[shape] Batch 251/450 | Loss: 1.2190\n",
      "[color] Batch 301/450 | Loss: 1.5164\n",
      "[quantity] Batch 351/450 | Loss: 0.9053\n",
      "[shape] Batch 401/450 | Loss: 1.1693\n",
      "[quantity] Batch 450/450 | Loss: 0.9475\n",
      "[Epoch 53] Train Loss: 1.2017 | Acc: 0.8017 | Perplexity: 3.3259\n",
      "[Epoch 53] Val Loss: 1.2169 | Acc: 0.8023 | Perplexity: 3.3767\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 54/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.3678\n",
      "[quantity] Batch 51/450 | Loss: 0.8720\n",
      "[shape] Batch 101/450 | Loss: 1.2543\n",
      "[color] Batch 151/450 | Loss: 1.2909\n",
      "[quantity] Batch 201/450 | Loss: 0.8766\n",
      "[shape] Batch 251/450 | Loss: 1.2507\n",
      "[color] Batch 301/450 | Loss: 1.3490\n",
      "[quantity] Batch 351/450 | Loss: 0.9136\n",
      "[shape] Batch 401/450 | Loss: 1.2569\n",
      "[quantity] Batch 450/450 | Loss: 0.9448\n",
      "[Epoch 54] Train Loss: 1.1715 | Acc: 0.8120 | Perplexity: 3.2269\n",
      "[Epoch 54] Val Loss: 1.1483 | Acc: 0.8103 | Perplexity: 3.1528\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 55/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.1836\n",
      "[quantity] Batch 51/450 | Loss: 0.9122\n",
      "[shape] Batch 101/450 | Loss: 1.1887\n",
      "[color] Batch 151/450 | Loss: 1.1915\n",
      "[quantity] Batch 201/450 | Loss: 0.9561\n",
      "[shape] Batch 251/450 | Loss: 1.3443\n",
      "[color] Batch 301/450 | Loss: 1.2328\n",
      "[quantity] Batch 351/450 | Loss: 0.9562\n",
      "[shape] Batch 401/450 | Loss: 1.5180\n",
      "[quantity] Batch 450/450 | Loss: 0.8722\n",
      "[Epoch 55] Train Loss: 1.1463 | Acc: 0.8205 | Perplexity: 3.1465\n",
      "[Epoch 55] Val Loss: 1.1550 | Acc: 0.8109 | Perplexity: 3.1740\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 56/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.1155\n",
      "[quantity] Batch 51/450 | Loss: 0.9280\n",
      "[shape] Batch 101/450 | Loss: 1.1480\n",
      "[color] Batch 151/450 | Loss: 1.3178\n",
      "[quantity] Batch 201/450 | Loss: 0.9351\n",
      "[shape] Batch 251/450 | Loss: 1.4237\n",
      "[color] Batch 301/450 | Loss: 1.2762\n",
      "[quantity] Batch 351/450 | Loss: 0.9148\n",
      "[shape] Batch 401/450 | Loss: 1.0745\n",
      "[quantity] Batch 450/450 | Loss: 0.8891\n",
      "[Epoch 56] Train Loss: 1.1200 | Acc: 0.8291 | Perplexity: 3.0650\n",
      "[Epoch 56] Val Loss: 1.1186 | Acc: 0.8252 | Perplexity: 3.0606\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 57/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.3394\n",
      "[quantity] Batch 51/450 | Loss: 0.9920\n",
      "[shape] Batch 101/450 | Loss: 1.1790\n",
      "[color] Batch 151/450 | Loss: 1.2453\n",
      "[quantity] Batch 201/450 | Loss: 0.9248\n",
      "[shape] Batch 251/450 | Loss: 1.1430\n",
      "[color] Batch 301/450 | Loss: 1.2434\n",
      "[quantity] Batch 351/450 | Loss: 0.8198\n",
      "[shape] Batch 401/450 | Loss: 1.2561\n",
      "[quantity] Batch 450/450 | Loss: 0.9721\n",
      "[Epoch 57] Train Loss: 1.1063 | Acc: 0.8367 | Perplexity: 3.0231\n",
      "[Epoch 57] Val Loss: 1.0901 | Acc: 0.8284 | Perplexity: 2.9746\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 58/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.1108\n",
      "[quantity] Batch 51/450 | Loss: 0.8504\n",
      "[shape] Batch 101/450 | Loss: 1.3352\n",
      "[color] Batch 151/450 | Loss: 1.2362\n",
      "[quantity] Batch 201/450 | Loss: 0.9375\n",
      "[shape] Batch 251/450 | Loss: 1.1530\n",
      "[color] Batch 301/450 | Loss: 1.0199\n",
      "[quantity] Batch 351/450 | Loss: 0.9091\n",
      "[shape] Batch 401/450 | Loss: 1.2154\n",
      "[quantity] Batch 450/450 | Loss: 0.8281\n",
      "[Epoch 58] Train Loss: 1.0791 | Acc: 0.8481 | Perplexity: 2.9421\n",
      "[Epoch 58] Val Loss: 1.0801 | Acc: 0.8389 | Perplexity: 2.9449\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 59/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.2166\n",
      "[quantity] Batch 51/450 | Loss: 0.8942\n",
      "[shape] Batch 101/450 | Loss: 1.0720\n",
      "[color] Batch 151/450 | Loss: 1.0864\n",
      "[quantity] Batch 201/450 | Loss: 0.7963\n",
      "[shape] Batch 251/450 | Loss: 1.1065\n",
      "[color] Batch 301/450 | Loss: 1.0879\n",
      "[quantity] Batch 351/450 | Loss: 0.8856\n",
      "[shape] Batch 401/450 | Loss: 1.1707\n",
      "[quantity] Batch 450/450 | Loss: 0.9516\n",
      "[Epoch 59] Train Loss: 1.0680 | Acc: 0.8547 | Perplexity: 2.9097\n",
      "[Epoch 59] Val Loss: 1.0811 | Acc: 0.8405 | Perplexity: 2.9479\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 60/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.2695\n",
      "[quantity] Batch 51/450 | Loss: 0.8222\n",
      "[shape] Batch 101/450 | Loss: 1.2002\n",
      "[color] Batch 151/450 | Loss: 1.1010\n",
      "[quantity] Batch 201/450 | Loss: 0.9003\n",
      "[shape] Batch 251/450 | Loss: 1.1534\n",
      "[color] Batch 301/450 | Loss: 1.1020\n",
      "[quantity] Batch 351/450 | Loss: 0.9398\n",
      "[shape] Batch 401/450 | Loss: 1.2661\n",
      "[quantity] Batch 450/450 | Loss: 0.8842\n",
      "[Epoch 60] Train Loss: 1.0574 | Acc: 0.8580 | Perplexity: 2.8790\n",
      "[Epoch 60] Val Loss: 1.0809 | Acc: 0.8416 | Perplexity: 2.9472\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 61/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.1396\n",
      "[quantity] Batch 51/450 | Loss: 0.8367\n",
      "[shape] Batch 101/450 | Loss: 1.0243\n",
      "[color] Batch 151/450 | Loss: 1.1637\n",
      "[quantity] Batch 201/450 | Loss: 0.8700\n",
      "[shape] Batch 251/450 | Loss: 1.0985\n",
      "[color] Batch 301/450 | Loss: 1.0784\n",
      "[quantity] Batch 351/450 | Loss: 0.9704\n",
      "[shape] Batch 401/450 | Loss: 1.1774\n",
      "[quantity] Batch 450/450 | Loss: 0.8355\n",
      "[Epoch 61] Train Loss: 1.0440 | Acc: 0.8645 | Perplexity: 2.8405\n",
      "[Epoch 61] Val Loss: 1.0620 | Acc: 0.8489 | Perplexity: 2.8920\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 62/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.1327\n",
      "[quantity] Batch 51/450 | Loss: 0.9198\n",
      "[shape] Batch 101/450 | Loss: 1.1114\n",
      "[color] Batch 151/450 | Loss: 1.1582\n",
      "[quantity] Batch 201/450 | Loss: 0.8677\n",
      "[shape] Batch 251/450 | Loss: 1.1193\n",
      "[color] Batch 301/450 | Loss: 1.0857\n",
      "[quantity] Batch 351/450 | Loss: 0.8622\n",
      "[shape] Batch 401/450 | Loss: 1.0011\n",
      "[quantity] Batch 450/450 | Loss: 0.8876\n",
      "[Epoch 62] Train Loss: 1.0379 | Acc: 0.8658 | Perplexity: 2.8233\n",
      "[Epoch 62] Val Loss: 1.0790 | Acc: 0.8466 | Perplexity: 2.9417\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 63/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.1171\n",
      "[quantity] Batch 51/450 | Loss: 0.8351\n",
      "[shape] Batch 101/450 | Loss: 0.9768\n",
      "[color] Batch 151/450 | Loss: 1.1092\n",
      "[quantity] Batch 201/450 | Loss: 0.8232\n",
      "[shape] Batch 251/450 | Loss: 0.9610\n",
      "[color] Batch 301/450 | Loss: 1.2029\n",
      "[quantity] Batch 351/450 | Loss: 0.9032\n",
      "[shape] Batch 401/450 | Loss: 1.0644\n",
      "[quantity] Batch 450/450 | Loss: 0.9416\n",
      "[Epoch 63] Train Loss: 1.0272 | Acc: 0.8707 | Perplexity: 2.7932\n",
      "[Epoch 63] Val Loss: 1.0448 | Acc: 0.8509 | Perplexity: 2.8427\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 64/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.0427\n",
      "[quantity] Batch 51/450 | Loss: 0.9069\n",
      "[shape] Batch 101/450 | Loss: 1.0826\n",
      "[color] Batch 151/450 | Loss: 1.0060\n",
      "[quantity] Batch 201/450 | Loss: 0.8532\n",
      "[shape] Batch 251/450 | Loss: 1.0780\n",
      "[color] Batch 301/450 | Loss: 1.2444\n",
      "[quantity] Batch 351/450 | Loss: 0.8626\n",
      "[shape] Batch 401/450 | Loss: 1.0977\n",
      "[quantity] Batch 450/450 | Loss: 0.8542\n",
      "[Epoch 64] Train Loss: 1.0121 | Acc: 0.8784 | Perplexity: 2.7515\n",
      "[Epoch 64] Val Loss: 1.0487 | Acc: 0.8520 | Perplexity: 2.8538\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 65/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.1639\n",
      "[quantity] Batch 51/450 | Loss: 0.8733\n",
      "[shape] Batch 101/450 | Loss: 1.2206\n",
      "[color] Batch 151/450 | Loss: 1.0837\n",
      "[quantity] Batch 201/450 | Loss: 0.7748\n",
      "[shape] Batch 251/450 | Loss: 1.1124\n",
      "[color] Batch 301/450 | Loss: 1.1028\n",
      "[quantity] Batch 351/450 | Loss: 0.8267\n",
      "[shape] Batch 401/450 | Loss: 1.1328\n",
      "[quantity] Batch 450/450 | Loss: 0.8481\n",
      "[Epoch 65] Train Loss: 1.0090 | Acc: 0.8780 | Perplexity: 2.7428\n",
      "[Epoch 65] Val Loss: 1.0521 | Acc: 0.8498 | Perplexity: 2.8635\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 66/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.0238\n",
      "[quantity] Batch 51/450 | Loss: 0.8654\n",
      "[shape] Batch 101/450 | Loss: 0.8941\n",
      "[color] Batch 151/450 | Loss: 1.0960\n",
      "[quantity] Batch 201/450 | Loss: 0.8294\n",
      "[shape] Batch 251/450 | Loss: 1.1731\n",
      "[color] Batch 301/450 | Loss: 1.0968\n",
      "[quantity] Batch 351/450 | Loss: 0.8312\n",
      "[shape] Batch 401/450 | Loss: 1.0327\n",
      "[quantity] Batch 450/450 | Loss: 0.7998\n",
      "[Epoch 66] Train Loss: 0.9975 | Acc: 0.8829 | Perplexity: 2.7116\n",
      "[Epoch 66] Val Loss: 1.0352 | Acc: 0.8503 | Perplexity: 2.8158\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 67/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.0425\n",
      "[quantity] Batch 51/450 | Loss: 0.9079\n",
      "[shape] Batch 101/450 | Loss: 1.0602\n",
      "[color] Batch 151/450 | Loss: 0.9517\n",
      "[quantity] Batch 201/450 | Loss: 0.8283\n",
      "[shape] Batch 251/450 | Loss: 1.0694\n",
      "[color] Batch 301/450 | Loss: 1.1288\n",
      "[quantity] Batch 351/450 | Loss: 0.8885\n",
      "[shape] Batch 401/450 | Loss: 0.9971\n",
      "[quantity] Batch 450/450 | Loss: 0.8607\n",
      "[Epoch 67] Train Loss: 0.9938 | Acc: 0.8875 | Perplexity: 2.7015\n",
      "[Epoch 67] Val Loss: 1.0302 | Acc: 0.8591 | Perplexity: 2.8017\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 68/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.0055\n",
      "[quantity] Batch 51/450 | Loss: 0.8498\n",
      "[shape] Batch 101/450 | Loss: 0.9944\n",
      "[color] Batch 151/450 | Loss: 1.0750\n",
      "[quantity] Batch 201/450 | Loss: 0.7883\n",
      "[shape] Batch 251/450 | Loss: 1.0171\n",
      "[color] Batch 301/450 | Loss: 0.8846\n",
      "[quantity] Batch 351/450 | Loss: 0.8049\n",
      "[shape] Batch 401/450 | Loss: 1.0049\n",
      "[quantity] Batch 450/450 | Loss: 0.7962\n",
      "[Epoch 68] Train Loss: 0.9806 | Acc: 0.8931 | Perplexity: 2.6662\n",
      "[Epoch 68] Val Loss: 1.0637 | Acc: 0.8588 | Perplexity: 2.8971\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 69/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 0.9682\n",
      "[quantity] Batch 51/450 | Loss: 0.8248\n",
      "[shape] Batch 101/450 | Loss: 1.0118\n",
      "[color] Batch 151/450 | Loss: 1.0128\n",
      "[quantity] Batch 201/450 | Loss: 0.8183\n",
      "[shape] Batch 251/450 | Loss: 1.0941\n",
      "[color] Batch 301/450 | Loss: 1.0673\n",
      "[quantity] Batch 351/450 | Loss: 0.8139\n",
      "[shape] Batch 401/450 | Loss: 1.2111\n",
      "[quantity] Batch 450/450 | Loss: 0.8436\n",
      "[Epoch 69] Train Loss: 0.9813 | Acc: 0.8927 | Perplexity: 2.6679\n",
      "[Epoch 69] Val Loss: 1.0325 | Acc: 0.8611 | Perplexity: 2.8082\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 70/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.0327\n",
      "[quantity] Batch 51/450 | Loss: 0.8165\n",
      "[shape] Batch 101/450 | Loss: 0.9936\n",
      "[color] Batch 151/450 | Loss: 1.0187\n",
      "[quantity] Batch 201/450 | Loss: 1.0041\n",
      "[shape] Batch 251/450 | Loss: 1.0202\n",
      "[color] Batch 301/450 | Loss: 1.0152\n",
      "[quantity] Batch 351/450 | Loss: 0.9119\n",
      "[shape] Batch 401/450 | Loss: 0.9637\n",
      "[quantity] Batch 450/450 | Loss: 1.0047\n",
      "[Epoch 70] Train Loss: 0.9746 | Acc: 0.8941 | Perplexity: 2.6502\n",
      "[Epoch 70] Val Loss: 1.0259 | Acc: 0.8564 | Perplexity: 2.7897\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 71/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 0.9825\n",
      "[quantity] Batch 51/450 | Loss: 0.8036\n",
      "[shape] Batch 101/450 | Loss: 1.0055\n",
      "[color] Batch 151/450 | Loss: 0.9946\n",
      "[quantity] Batch 201/450 | Loss: 0.8356\n",
      "[shape] Batch 251/450 | Loss: 0.9942\n",
      "[color] Batch 301/450 | Loss: 1.0064\n",
      "[quantity] Batch 351/450 | Loss: 0.8691\n",
      "[shape] Batch 401/450 | Loss: 1.0486\n",
      "[quantity] Batch 450/450 | Loss: 0.8530\n",
      "[Epoch 71] Train Loss: 0.9720 | Acc: 0.8981 | Perplexity: 2.6431\n",
      "[Epoch 71] Val Loss: 1.0381 | Acc: 0.8580 | Perplexity: 2.8240\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 72/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.0111\n",
      "[quantity] Batch 51/450 | Loss: 0.8928\n",
      "[shape] Batch 101/450 | Loss: 0.9546\n",
      "[color] Batch 151/450 | Loss: 0.9308\n",
      "[quantity] Batch 201/450 | Loss: 0.7798\n",
      "[shape] Batch 251/450 | Loss: 1.0105\n",
      "[color] Batch 301/450 | Loss: 0.9936\n",
      "[quantity] Batch 351/450 | Loss: 0.9025\n",
      "[shape] Batch 401/450 | Loss: 0.9075\n",
      "[quantity] Batch 450/450 | Loss: 0.8900\n",
      "[Epoch 72] Train Loss: 0.9598 | Acc: 0.9039 | Perplexity: 2.6111\n",
      "[Epoch 72] Val Loss: 1.0457 | Acc: 0.8600 | Perplexity: 2.8454\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 73/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 0.9318\n",
      "[quantity] Batch 51/450 | Loss: 0.7998\n",
      "[shape] Batch 101/450 | Loss: 0.9848\n",
      "[color] Batch 151/450 | Loss: 0.9983\n",
      "[quantity] Batch 201/450 | Loss: 0.7865\n",
      "[shape] Batch 251/450 | Loss: 1.0386\n",
      "[color] Batch 301/450 | Loss: 0.9813\n",
      "[quantity] Batch 351/450 | Loss: 0.8615\n",
      "[shape] Batch 401/450 | Loss: 1.0118\n",
      "[quantity] Batch 450/450 | Loss: 0.9731\n",
      "[Epoch 73] Train Loss: 0.9567 | Acc: 0.9020 | Perplexity: 2.6030\n",
      "[Epoch 73] Val Loss: 1.0428 | Acc: 0.8539 | Perplexity: 2.8372\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 74/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.0187\n",
      "[quantity] Batch 51/450 | Loss: 0.8139\n",
      "[shape] Batch 101/450 | Loss: 1.0248\n",
      "[color] Batch 151/450 | Loss: 1.1326\n",
      "[quantity] Batch 201/450 | Loss: 0.9341\n",
      "[shape] Batch 251/450 | Loss: 0.9228\n",
      "[color] Batch 301/450 | Loss: 1.0596\n",
      "[quantity] Batch 351/450 | Loss: 0.8616\n",
      "[shape] Batch 401/450 | Loss: 1.1562\n",
      "[quantity] Batch 450/450 | Loss: 0.8263\n",
      "[Epoch 74] Train Loss: 0.9563 | Acc: 0.9030 | Perplexity: 2.6021\n",
      "[Epoch 74] Val Loss: 1.0399 | Acc: 0.8558 | Perplexity: 2.8289\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 75/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 0.9897\n",
      "[quantity] Batch 51/450 | Loss: 0.7836\n",
      "[shape] Batch 101/450 | Loss: 0.8942\n",
      "[color] Batch 151/450 | Loss: 1.0093\n",
      "[quantity] Batch 201/450 | Loss: 0.8483\n",
      "[shape] Batch 251/450 | Loss: 1.0306\n",
      "[color] Batch 301/450 | Loss: 1.1387\n",
      "[quantity] Batch 351/450 | Loss: 0.8281\n",
      "[shape] Batch 401/450 | Loss: 0.9979\n",
      "[quantity] Batch 450/450 | Loss: 0.7815\n",
      "[Epoch 75] Train Loss: 0.9467 | Acc: 0.9080 | Perplexity: 2.5772\n",
      "[Epoch 75] Val Loss: 1.0418 | Acc: 0.8561 | Perplexity: 2.8343\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 76/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 0.8832\n",
      "[quantity] Batch 51/450 | Loss: 0.7959\n",
      "[shape] Batch 101/450 | Loss: 0.9775\n",
      "[color] Batch 151/450 | Loss: 1.0970\n",
      "[quantity] Batch 201/450 | Loss: 0.7976\n",
      "[shape] Batch 251/450 | Loss: 0.9664\n",
      "[color] Batch 301/450 | Loss: 0.9440\n",
      "[quantity] Batch 351/450 | Loss: 0.8185\n",
      "[shape] Batch 401/450 | Loss: 1.0023\n",
      "[quantity] Batch 450/450 | Loss: 0.8093\n",
      "[Epoch 76] Train Loss: 0.9416 | Acc: 0.9126 | Perplexity: 2.5640\n",
      "[Epoch 76] Val Loss: 1.0309 | Acc: 0.8564 | Perplexity: 2.8035\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 77/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.0227\n",
      "[quantity] Batch 51/450 | Loss: 0.8258\n",
      "[shape] Batch 101/450 | Loss: 1.0238\n",
      "[color] Batch 151/450 | Loss: 0.9799\n",
      "[quantity] Batch 201/450 | Loss: 0.8158\n",
      "[shape] Batch 251/450 | Loss: 1.0781\n",
      "[color] Batch 301/450 | Loss: 0.9320\n",
      "[quantity] Batch 351/450 | Loss: 0.7998\n",
      "[shape] Batch 401/450 | Loss: 0.9472\n",
      "[quantity] Batch 450/450 | Loss: 0.7958\n",
      "[Epoch 77] Train Loss: 0.9441 | Acc: 0.9107 | Perplexity: 2.5706\n",
      "[Epoch 77] Val Loss: 1.0531 | Acc: 0.8588 | Perplexity: 2.8664\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 78/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 0.9758\n",
      "[quantity] Batch 51/450 | Loss: 0.8467\n",
      "[shape] Batch 101/450 | Loss: 0.9346\n",
      "[color] Batch 151/450 | Loss: 1.0214\n",
      "[quantity] Batch 201/450 | Loss: 0.9071\n",
      "[shape] Batch 251/450 | Loss: 0.9737\n",
      "[color] Batch 301/450 | Loss: 0.9863\n",
      "[quantity] Batch 351/450 | Loss: 0.8068\n",
      "[shape] Batch 401/450 | Loss: 0.8936\n",
      "[quantity] Batch 450/450 | Loss: 0.8616\n",
      "[Epoch 78] Train Loss: 0.9338 | Acc: 0.9161 | Perplexity: 2.5442\n",
      "[Epoch 78] Val Loss: 1.0607 | Acc: 0.8511 | Perplexity: 2.8883\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 79/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 0.9359\n",
      "[quantity] Batch 51/450 | Loss: 0.8401\n",
      "[shape] Batch 101/450 | Loss: 1.0218\n",
      "[color] Batch 151/450 | Loss: 1.1741\n",
      "[quantity] Batch 201/450 | Loss: 0.7852\n",
      "[shape] Batch 251/450 | Loss: 0.9012\n",
      "[color] Batch 301/450 | Loss: 0.9198\n",
      "[quantity] Batch 351/450 | Loss: 0.8656\n",
      "[shape] Batch 401/450 | Loss: 0.9532\n",
      "[quantity] Batch 450/450 | Loss: 0.8277\n",
      "[Epoch 79] Train Loss: 0.9335 | Acc: 0.9133 | Perplexity: 2.5434\n",
      "[Epoch 79] Val Loss: 1.0501 | Acc: 0.8611 | Perplexity: 2.8580\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 80/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 0.9446\n",
      "[quantity] Batch 51/450 | Loss: 0.7803\n",
      "[shape] Batch 101/450 | Loss: 1.0282\n",
      "[color] Batch 151/450 | Loss: 1.0238\n",
      "[quantity] Batch 201/450 | Loss: 0.8623\n",
      "[shape] Batch 251/450 | Loss: 1.0463\n",
      "[color] Batch 301/450 | Loss: 1.0921\n",
      "[quantity] Batch 351/450 | Loss: 0.7642\n",
      "[shape] Batch 401/450 | Loss: 1.0684\n",
      "[quantity] Batch 450/450 | Loss: 0.8356\n",
      "[Epoch 80] Train Loss: 0.9235 | Acc: 0.9195 | Perplexity: 2.5181\n",
      "[Epoch 80] Val Loss: 1.0447 | Acc: 0.8558 | Perplexity: 2.8425\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 81/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 0.9837\n",
      "[quantity] Batch 51/450 | Loss: 0.8710\n",
      "[shape] Batch 101/450 | Loss: 0.8636\n",
      "[color] Batch 151/450 | Loss: 1.0972\n",
      "[quantity] Batch 201/450 | Loss: 0.8441\n",
      "[shape] Batch 251/450 | Loss: 0.9973\n",
      "[color] Batch 301/450 | Loss: 1.0830\n",
      "[quantity] Batch 351/450 | Loss: 0.8718\n",
      "[shape] Batch 401/450 | Loss: 1.0632\n",
      "[quantity] Batch 450/450 | Loss: 0.8481\n",
      "[Epoch 81] Train Loss: 0.9246 | Acc: 0.9203 | Perplexity: 2.5208\n",
      "[Epoch 81] Val Loss: 1.0604 | Acc: 0.8542 | Perplexity: 2.8876\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 82/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 0.9137\n",
      "[quantity] Batch 51/450 | Loss: 0.8248\n",
      "[shape] Batch 101/450 | Loss: 0.9794\n",
      "[color] Batch 151/450 | Loss: 1.0060\n",
      "[quantity] Batch 201/450 | Loss: 0.7858\n",
      "[shape] Batch 251/450 | Loss: 0.9175\n",
      "[color] Batch 301/450 | Loss: 0.9768\n",
      "[quantity] Batch 351/450 | Loss: 0.8157\n",
      "[shape] Batch 401/450 | Loss: 0.9349\n",
      "[quantity] Batch 450/450 | Loss: 0.7725\n",
      "[Epoch 82] Train Loss: 0.9220 | Acc: 0.9211 | Perplexity: 2.5143\n",
      "[Epoch 82] Val Loss: 1.0540 | Acc: 0.8622 | Perplexity: 2.8690\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 83/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 0.9513\n",
      "[quantity] Batch 51/450 | Loss: 0.8516\n",
      "[shape] Batch 101/450 | Loss: 1.0632\n",
      "[color] Batch 151/450 | Loss: 1.1170\n",
      "[quantity] Batch 201/450 | Loss: 0.7924\n",
      "[shape] Batch 251/450 | Loss: 1.0506\n",
      "[color] Batch 301/450 | Loss: 0.9496\n",
      "[quantity] Batch 351/450 | Loss: 0.9060\n",
      "[shape] Batch 401/450 | Loss: 1.0148\n",
      "[quantity] Batch 450/450 | Loss: 0.7969\n",
      "[Epoch 83] Train Loss: 0.9148 | Acc: 0.9233 | Perplexity: 2.4964\n",
      "[Epoch 83] Val Loss: 1.0750 | Acc: 0.8581 | Perplexity: 2.9300\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 84/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 0.9463\n",
      "[quantity] Batch 51/450 | Loss: 0.8124\n",
      "[shape] Batch 101/450 | Loss: 0.9930\n",
      "[color] Batch 151/450 | Loss: 0.9535\n",
      "[quantity] Batch 201/450 | Loss: 0.8363\n",
      "[shape] Batch 251/450 | Loss: 0.8838\n",
      "[color] Batch 301/450 | Loss: 1.0705\n",
      "[quantity] Batch 351/450 | Loss: 0.8474\n",
      "[shape] Batch 401/450 | Loss: 0.9018\n",
      "[quantity] Batch 450/450 | Loss: 0.7842\n",
      "[Epoch 84] Train Loss: 0.9162 | Acc: 0.9224 | Perplexity: 2.4998\n",
      "[Epoch 84] Val Loss: 1.0546 | Acc: 0.8641 | Perplexity: 2.8709\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 85/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 0.9205\n",
      "[quantity] Batch 51/450 | Loss: 0.9067\n",
      "[shape] Batch 101/450 | Loss: 0.8668\n",
      "[color] Batch 151/450 | Loss: 0.8638\n",
      "[quantity] Batch 201/450 | Loss: 0.7883\n",
      "[shape] Batch 251/450 | Loss: 0.9635\n",
      "[color] Batch 301/450 | Loss: 0.9535\n",
      "[quantity] Batch 351/450 | Loss: 0.8574\n",
      "[shape] Batch 401/450 | Loss: 1.0032\n",
      "[quantity] Batch 450/450 | Loss: 0.8565\n",
      "[Epoch 85] Train Loss: 0.9120 | Acc: 0.9248 | Perplexity: 2.4892\n",
      "[Epoch 85] Val Loss: 1.0524 | Acc: 0.8586 | Perplexity: 2.8644\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 86/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 0.9431\n",
      "[quantity] Batch 51/450 | Loss: 0.7814\n",
      "[shape] Batch 101/450 | Loss: 0.9158\n",
      "[color] Batch 151/450 | Loss: 1.0355\n",
      "[quantity] Batch 201/450 | Loss: 0.7955\n",
      "[shape] Batch 251/450 | Loss: 1.0088\n",
      "[color] Batch 301/450 | Loss: 0.8854\n",
      "[quantity] Batch 351/450 | Loss: 0.7666\n",
      "[shape] Batch 401/450 | Loss: 1.0062\n",
      "[quantity] Batch 450/450 | Loss: 0.7848\n",
      "[Epoch 86] Train Loss: 0.9070 | Acc: 0.9295 | Perplexity: 2.4769\n",
      "[Epoch 86] Val Loss: 1.0626 | Acc: 0.8612 | Perplexity: 2.8938\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 87/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 0.8584\n",
      "[quantity] Batch 51/450 | Loss: 0.8608\n",
      "[shape] Batch 101/450 | Loss: 0.9249\n",
      "[color] Batch 151/450 | Loss: 0.9110\n",
      "[quantity] Batch 201/450 | Loss: 0.7976\n",
      "[shape] Batch 251/450 | Loss: 0.9337\n",
      "[color] Batch 301/450 | Loss: 1.0310\n",
      "[quantity] Batch 351/450 | Loss: 0.7985\n",
      "[shape] Batch 401/450 | Loss: 0.8940\n",
      "[quantity] Batch 450/450 | Loss: 0.7967\n",
      "[Epoch 87] Train Loss: 0.9065 | Acc: 0.9278 | Perplexity: 2.4755\n",
      "[Epoch 87] Val Loss: 1.0588 | Acc: 0.8589 | Perplexity: 2.8829\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 88/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 0.9352\n",
      "[quantity] Batch 51/450 | Loss: 0.7633\n",
      "[shape] Batch 101/450 | Loss: 0.8684\n",
      "[color] Batch 151/450 | Loss: 0.9898\n",
      "[quantity] Batch 201/450 | Loss: 0.8086\n",
      "[shape] Batch 251/450 | Loss: 0.9096\n",
      "[color] Batch 301/450 | Loss: 1.0738\n",
      "[quantity] Batch 351/450 | Loss: 0.7950\n",
      "[shape] Batch 401/450 | Loss: 0.8192\n",
      "[quantity] Batch 450/450 | Loss: 0.8428\n",
      "[Epoch 88] Train Loss: 0.8989 | Acc: 0.9320 | Perplexity: 2.4569\n",
      "[Epoch 88] Val Loss: 1.0558 | Acc: 0.8633 | Perplexity: 2.8742\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 89/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 0.8933\n",
      "[quantity] Batch 51/450 | Loss: 0.7698\n",
      "[shape] Batch 101/450 | Loss: 0.8948\n",
      "[color] Batch 151/450 | Loss: 0.8902\n",
      "[quantity] Batch 201/450 | Loss: 0.8255\n",
      "[shape] Batch 251/450 | Loss: 0.9944\n",
      "[color] Batch 301/450 | Loss: 1.0008\n",
      "[quantity] Batch 351/450 | Loss: 0.8083\n",
      "[shape] Batch 401/450 | Loss: 0.9356\n",
      "[quantity] Batch 450/450 | Loss: 0.8405\n",
      "[Epoch 89] Train Loss: 0.8991 | Acc: 0.9330 | Perplexity: 2.4574\n",
      "[Epoch 89] Val Loss: 1.0438 | Acc: 0.8608 | Perplexity: 2.8400\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 90/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 0.8406\n",
      "[quantity] Batch 51/450 | Loss: 0.7772\n",
      "[shape] Batch 101/450 | Loss: 0.8969\n",
      "[color] Batch 151/450 | Loss: 0.8339\n",
      "[quantity] Batch 201/450 | Loss: 0.7881\n",
      "[shape] Batch 251/450 | Loss: 0.8950\n",
      "[color] Batch 301/450 | Loss: 0.8977\n",
      "[quantity] Batch 351/450 | Loss: 0.8764\n",
      "[shape] Batch 401/450 | Loss: 0.9060\n",
      "[quantity] Batch 450/450 | Loss: 0.7757\n",
      "[Epoch 90] Train Loss: 0.8902 | Acc: 0.9351 | Perplexity: 2.4356\n",
      "[Epoch 90] Val Loss: 1.0989 | Acc: 0.8581 | Perplexity: 3.0010\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 91/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.0407\n",
      "[quantity] Batch 51/450 | Loss: 0.8516\n",
      "[shape] Batch 101/450 | Loss: 0.9189\n",
      "[color] Batch 151/450 | Loss: 0.9180\n",
      "[quantity] Batch 201/450 | Loss: 0.8144\n",
      "[shape] Batch 251/450 | Loss: 1.0133\n",
      "[color] Batch 301/450 | Loss: 0.9133\n",
      "[quantity] Batch 351/450 | Loss: 0.9261\n",
      "[shape] Batch 401/450 | Loss: 1.0846\n",
      "[quantity] Batch 450/450 | Loss: 0.9070\n",
      "[Epoch 91] Train Loss: 0.8964 | Acc: 0.9336 | Perplexity: 2.4508\n",
      "[Epoch 91] Val Loss: 1.0617 | Acc: 0.8642 | Perplexity: 2.8912\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 92/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 0.9223\n",
      "[quantity] Batch 51/450 | Loss: 0.7977\n",
      "[shape] Batch 101/450 | Loss: 0.8768\n",
      "[color] Batch 151/450 | Loss: 1.0530\n",
      "[quantity] Batch 201/450 | Loss: 0.7746\n",
      "[shape] Batch 251/450 | Loss: 0.9343\n",
      "[color] Batch 301/450 | Loss: 0.9382\n",
      "[quantity] Batch 351/450 | Loss: 0.8122\n",
      "[shape] Batch 401/450 | Loss: 0.9862\n",
      "[quantity] Batch 450/450 | Loss: 0.8055\n",
      "[Epoch 92] Train Loss: 0.8915 | Acc: 0.9355 | Perplexity: 2.4388\n",
      "[Epoch 92] Val Loss: 1.0728 | Acc: 0.8559 | Perplexity: 2.9237\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 93/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 0.9200\n",
      "[quantity] Batch 51/450 | Loss: 0.7906\n",
      "[shape] Batch 101/450 | Loss: 0.9565\n",
      "[color] Batch 151/450 | Loss: 0.9957\n",
      "[quantity] Batch 201/450 | Loss: 0.7588\n",
      "[shape] Batch 251/450 | Loss: 0.8972\n",
      "[color] Batch 301/450 | Loss: 0.8766\n",
      "[quantity] Batch 351/450 | Loss: 0.8911\n",
      "[shape] Batch 401/450 | Loss: 1.0568\n",
      "[quantity] Batch 450/450 | Loss: 0.7665\n",
      "[Epoch 93] Train Loss: 0.8857 | Acc: 0.9378 | Perplexity: 2.4247\n",
      "[Epoch 93] Val Loss: 1.0801 | Acc: 0.8595 | Perplexity: 2.9451\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 94/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 0.8835\n",
      "[quantity] Batch 51/450 | Loss: 0.7863\n",
      "[shape] Batch 101/450 | Loss: 0.9377\n",
      "[color] Batch 151/450 | Loss: 0.8639\n",
      "[quantity] Batch 201/450 | Loss: 0.7969\n",
      "[shape] Batch 251/450 | Loss: 1.0185\n",
      "[color] Batch 301/450 | Loss: 0.9404\n",
      "[quantity] Batch 351/450 | Loss: 0.7550\n",
      "[shape] Batch 401/450 | Loss: 1.0409\n",
      "[quantity] Batch 450/450 | Loss: 0.8190\n",
      "[Epoch 94] Train Loss: 0.8861 | Acc: 0.9372 | Perplexity: 2.4256\n",
      "[Epoch 94] Val Loss: 1.0864 | Acc: 0.8506 | Perplexity: 2.9635\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 95/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 0.8642\n",
      "[quantity] Batch 51/450 | Loss: 0.7945\n",
      "[shape] Batch 101/450 | Loss: 0.8362\n",
      "[color] Batch 151/450 | Loss: 1.0265\n",
      "[quantity] Batch 201/450 | Loss: 0.7796\n",
      "[shape] Batch 251/450 | Loss: 0.8488\n",
      "[color] Batch 301/450 | Loss: 0.9424\n",
      "[quantity] Batch 351/450 | Loss: 0.8046\n",
      "[shape] Batch 401/450 | Loss: 0.8349\n",
      "[quantity] Batch 450/450 | Loss: 0.7904\n",
      "[Epoch 95] Train Loss: 0.8831 | Acc: 0.9399 | Perplexity: 2.4184\n",
      "[Epoch 95] Val Loss: 1.0804 | Acc: 0.8581 | Perplexity: 2.9460\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 96/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 0.9519\n",
      "[quantity] Batch 51/450 | Loss: 0.8076\n",
      "[shape] Batch 101/450 | Loss: 0.8303\n",
      "[color] Batch 151/450 | Loss: 0.8863\n",
      "[quantity] Batch 201/450 | Loss: 0.7978\n",
      "[shape] Batch 251/450 | Loss: 0.8998\n",
      "[color] Batch 301/450 | Loss: 0.8769\n",
      "[quantity] Batch 351/450 | Loss: 0.8186\n",
      "[shape] Batch 401/450 | Loss: 0.8785\n",
      "[quantity] Batch 450/450 | Loss: 0.8721\n",
      "[Epoch 96] Train Loss: 0.8809 | Acc: 0.9394 | Perplexity: 2.4132\n",
      "[Epoch 96] Val Loss: 1.0653 | Acc: 0.8611 | Perplexity: 2.9018\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 97/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 0.9317\n",
      "[quantity] Batch 51/450 | Loss: 0.7489\n",
      "[shape] Batch 101/450 | Loss: 0.9235\n",
      "[color] Batch 151/450 | Loss: 0.8971\n",
      "[quantity] Batch 201/450 | Loss: 0.8511\n",
      "[shape] Batch 251/450 | Loss: 0.9106\n",
      "[color] Batch 301/450 | Loss: 1.0068\n",
      "[quantity] Batch 351/450 | Loss: 0.7695\n",
      "[shape] Batch 401/450 | Loss: 1.1143\n",
      "[quantity] Batch 450/450 | Loss: 0.7612\n",
      "[Epoch 97] Train Loss: 0.8761 | Acc: 0.9428 | Perplexity: 2.4014\n",
      "[Epoch 97] Val Loss: 1.0903 | Acc: 0.8559 | Perplexity: 2.9750\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 98/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 0.8464\n",
      "[quantity] Batch 51/450 | Loss: 0.7900\n",
      "[shape] Batch 101/450 | Loss: 0.8894\n",
      "[color] Batch 151/450 | Loss: 0.9695\n",
      "[quantity] Batch 201/450 | Loss: 0.8440\n",
      "[shape] Batch 251/450 | Loss: 0.8737\n",
      "[color] Batch 301/450 | Loss: 0.8311\n",
      "[quantity] Batch 351/450 | Loss: 0.7921\n",
      "[shape] Batch 401/450 | Loss: 0.9904\n",
      "[quantity] Batch 450/450 | Loss: 0.8289\n",
      "[Epoch 98] Train Loss: 0.8740 | Acc: 0.9443 | Perplexity: 2.3966\n",
      "[Epoch 98] Val Loss: 1.0966 | Acc: 0.8567 | Perplexity: 2.9939\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 99/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 0.9149\n",
      "[quantity] Batch 51/450 | Loss: 0.8368\n",
      "[shape] Batch 101/450 | Loss: 0.8437\n",
      "[color] Batch 151/450 | Loss: 1.0833\n",
      "[quantity] Batch 201/450 | Loss: 0.7702\n",
      "[shape] Batch 251/450 | Loss: 1.0045\n",
      "[color] Batch 301/450 | Loss: 0.8532\n",
      "[quantity] Batch 351/450 | Loss: 0.8817\n",
      "[shape] Batch 401/450 | Loss: 0.9248\n",
      "[quantity] Batch 450/450 | Loss: 0.8790\n",
      "[Epoch 99] Train Loss: 0.8743 | Acc: 0.9432 | Perplexity: 2.3973\n",
      "[Epoch 99] Val Loss: 1.0903 | Acc: 0.8566 | Perplexity: 2.9752\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 100/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 0.8452\n",
      "[quantity] Batch 51/450 | Loss: 0.7573\n",
      "[shape] Batch 101/450 | Loss: 0.9040\n",
      "[color] Batch 151/450 | Loss: 0.8674\n",
      "[quantity] Batch 201/450 | Loss: 0.7706\n",
      "[shape] Batch 251/450 | Loss: 0.9641\n",
      "[color] Batch 301/450 | Loss: 0.8588\n",
      "[quantity] Batch 351/450 | Loss: 0.8094\n",
      "[shape] Batch 401/450 | Loss: 0.8437\n",
      "[quantity] Batch 450/450 | Loss: 0.7571\n",
      "[Epoch 100] Train Loss: 0.8701 | Acc: 0.9450 | Perplexity: 2.3872\n",
      "[Epoch 100] Val Loss: 1.1066 | Acc: 0.8530 | Perplexity: 3.0241\n",
      "------------------------------------------------------------\n",
      "\n",
      " Training complete (Round-Robin Mode)\n"
     ]
    }
   ],
   "source": [
    "criterion =  nn.CrossEntropyLoss(label_smoothing=LABEL_SMOOTHING)\n",
    "optimizer = optim.AdamW(transformer.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=WARMUP_STEPS)  \n",
    "\n",
    "results = train_model(\n",
    "    train_loader, validation_loaders, transformer, criterion, \n",
    "    optimizer, max_epochs=MAX_EPOCHS, device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917a1fa4",
   "metadata": {},
   "source": [
    "### 3. Test Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7d144441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.1241 | Acc: 0.8445 | Perplexity: 3.0775\n"
     ]
    }
   ],
   "source": [
    "test_results = test_model(test_loader, transformer, criterion, device)\n",
    "print(f\"Test Loss: {test_results[\"test_loss\"]:.4f} | Acc: {test_results[\"test_acc\"]:.4f} | Perplexity: {test_results[\"test_perplexity\"]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f603f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.save(f\"../models/baseline_transformer-{0.74}-{0.27}-{0.27}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e87154e",
   "metadata": {},
   "source": [
    "### 4. Save Training & Test Results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3fc205ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c7cf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results = {**results, **test_results}\n",
    "\n",
    "with open(f\"./../results/baseline_transformer_performace.json\", \"w\") as file:\n",
    "    json.dump(final_results, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28dac7b",
   "metadata": {},
   "source": [
    "## Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c3c7ecc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.wcst import WCST\n",
    "wcst = WCST(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6b590910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inference(model: nn.Module, source_sequence):\n",
    "    model.eval()\n",
    "    generated = source_sequence\n",
    "    \n",
    "    with T.no_grad():\n",
    "        full_sequence = T.cat([generated], dim=1)\n",
    "        logits = model(full_sequence)\n",
    "    \n",
    "    # Greedy Selection\n",
    "    next_token = T.argmax(logits[:, -1, :], dim=-1, keepdim=True)\n",
    "\n",
    "    generated = T.cat([generated, next_token], dim=1)\n",
    "\n",
    "    return generated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7f458678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Actual Trials\n",
      "[array(['green', 'star', '1'], dtype='<U6'), array(['red', 'square', '1'], dtype='<U6'), array(['yellow', 'cross', '2'], dtype='<U6'), array(['red', 'circle', '3'], dtype='<U6'), array(['blue', 'circle', '3'], dtype='<U6'), 'SEP', 'C4', 'EOS', array(['green', 'cross', '4'], dtype='<U6'), 'SEP', 'C3']\n",
      "[array(['green', 'cross', '3'], dtype='<U6'), array(['blue', 'square', '1'], dtype='<U6'), array(['red', 'star', '2'], dtype='<U6'), array(['red', 'circle', '2'], dtype='<U6'), array(['yellow', 'square', '4'], dtype='<U6'), 'SEP', 'C2', 'EOS', array(['blue', 'circle', '2'], dtype='<U6'), 'SEP', 'C4']\n",
      "[array(['blue', 'circle', '4'], dtype='<U6'), array(['red', 'star', '2'], dtype='<U6'), array(['green', 'square', '3'], dtype='<U6'), array(['red', 'cross', '4'], dtype='<U6'), array(['yellow', 'star', '4'], dtype='<U6'), 'SEP', 'C2', 'EOS', array(['blue', 'circle', '3'], dtype='<U6'), 'SEP', 'C1']\n",
      "[array(['yellow', 'star', '4'], dtype='<U6'), array(['red', 'cross', '1'], dtype='<U6'), array(['red', 'square', '2'], dtype='<U6'), array(['green', 'circle', '1'], dtype='<U6'), array(['green', 'star', '1'], dtype='<U6'), 'SEP', 'C1', 'EOS', array(['blue', 'square', '4'], dtype='<U6'), 'SEP', 'C3']\n",
      "[array(['yellow', 'star', '1'], dtype='<U6'), array(['red', 'cross', '1'], dtype='<U6'), array(['red', 'circle', '3'], dtype='<U6'), array(['red', 'square', '4'], dtype='<U6'), array(['green', 'star', '4'], dtype='<U6'), 'SEP', 'C1', 'EOS', array(['green', 'cross', '2'], dtype='<U6'), 'SEP', 'C2']\n",
      "[array(['yellow', 'star', '3'], dtype='<U6'), array(['blue', 'circle', '3'], dtype='<U6'), array(['red', 'square', '4'], dtype='<U6'), array(['green', 'cross', '2'], dtype='<U6'), array(['yellow', 'square', '4'], dtype='<U6'), 'SEP', 'C3', 'EOS', array(['yellow', 'cross', '2'], dtype='<U6'), 'SEP', 'C4']\n",
      "[array(['yellow', 'circle', '1'], dtype='<U6'), array(['yellow', 'square', '4'], dtype='<U6'), array(['blue', 'cross', '4'], dtype='<U6'), array(['yellow', 'star', '1'], dtype='<U6'), array(['yellow', 'circle', '3'], dtype='<U6'), 'SEP', 'C1', 'EOS', array(['blue', 'square', '2'], dtype='<U6'), 'SEP', 'C2']\n",
      "[array(['green', 'cross', '3'], dtype='<U6'), array(['green', 'circle', '3'], dtype='<U6'), array(['blue', 'star', '1'], dtype='<U6'), array(['yellow', 'square', '2'], dtype='<U6'), array(['yellow', 'square', '1'], dtype='<U6'), 'SEP', 'C4', 'EOS', array(['yellow', 'star', '3'], dtype='<U6'), 'SEP', 'C3']\n",
      "[array(['red', 'circle', '4'], dtype='<U6'), array(['yellow', 'cross', '2'], dtype='<U6'), array(['blue', 'star', '4'], dtype='<U6'), array(['red', 'square', '3'], dtype='<U6'), array(['yellow', 'square', '3'], dtype='<U6'), 'SEP', 'C4', 'EOS', array(['blue', 'circle', '4'], dtype='<U6'), 'SEP', 'C1']\n",
      "[array(['blue', 'cross', '2'], dtype='<U6'), array(['green', 'star', '1'], dtype='<U6'), array(['red', 'circle', '2'], dtype='<U6'), array(['green', 'square', '3'], dtype='<U6'), array(['yellow', 'star', '1'], dtype='<U6'), 'SEP', 'C2', 'EOS', array(['blue', 'square', '4'], dtype='<U6'), 'SEP', 'C4']\n",
      "Feature for Classification:  0 \n",
      "\n",
      "# Predicted Trials\n",
      "[array(['green', 'star', '1'], dtype='<U6'), array(['red', 'square', '1'], dtype='<U6'), array(['yellow', 'cross', '2'], dtype='<U6'), array(['red', 'circle', '3'], dtype='<U6'), array(['blue', 'circle', '3'], dtype='<U6'), 'SEP', 'C4', 'EOS', array(['green', 'cross', '4'], dtype='<U6'), 'SEP', 'C1']\n",
      "[array(['green', 'cross', '3'], dtype='<U6'), array(['blue', 'square', '1'], dtype='<U6'), array(['red', 'star', '2'], dtype='<U6'), array(['red', 'circle', '2'], dtype='<U6'), array(['yellow', 'square', '4'], dtype='<U6'), 'SEP', 'C2', 'EOS', array(['blue', 'circle', '2'], dtype='<U6'), 'SEP', 'C4']\n",
      "[array(['blue', 'circle', '4'], dtype='<U6'), array(['red', 'star', '2'], dtype='<U6'), array(['green', 'square', '3'], dtype='<U6'), array(['red', 'cross', '4'], dtype='<U6'), array(['yellow', 'star', '4'], dtype='<U6'), 'SEP', 'C2', 'EOS', array(['blue', 'circle', '3'], dtype='<U6'), 'SEP', 'C1']\n",
      "[array(['yellow', 'star', '4'], dtype='<U6'), array(['red', 'cross', '1'], dtype='<U6'), array(['red', 'square', '2'], dtype='<U6'), array(['green', 'circle', '1'], dtype='<U6'), array(['green', 'star', '1'], dtype='<U6'), 'SEP', 'C1', 'EOS', array(['blue', 'square', '4'], dtype='<U6'), 'SEP', 'C3']\n",
      "[array(['yellow', 'star', '1'], dtype='<U6'), array(['red', 'cross', '1'], dtype='<U6'), array(['red', 'circle', '3'], dtype='<U6'), array(['red', 'square', '4'], dtype='<U6'), array(['green', 'star', '4'], dtype='<U6'), 'SEP', 'C1', 'EOS', array(['green', 'cross', '2'], dtype='<U6'), 'SEP', 'C2']\n",
      "[array(['yellow', 'star', '3'], dtype='<U6'), array(['blue', 'circle', '3'], dtype='<U6'), array(['red', 'square', '4'], dtype='<U6'), array(['green', 'cross', '2'], dtype='<U6'), array(['yellow', 'square', '4'], dtype='<U6'), 'SEP', 'C3', 'EOS', array(['yellow', 'cross', '2'], dtype='<U6'), 'SEP', 'C4']\n",
      "[array(['yellow', 'circle', '1'], dtype='<U6'), array(['yellow', 'square', '4'], dtype='<U6'), array(['blue', 'cross', '4'], dtype='<U6'), array(['yellow', 'star', '1'], dtype='<U6'), array(['yellow', 'circle', '3'], dtype='<U6'), 'SEP', 'C1', 'EOS', array(['blue', 'square', '2'], dtype='<U6'), 'SEP', 'C2']\n",
      "[array(['green', 'cross', '3'], dtype='<U6'), array(['green', 'circle', '3'], dtype='<U6'), array(['blue', 'star', '1'], dtype='<U6'), array(['yellow', 'square', '2'], dtype='<U6'), array(['yellow', 'square', '1'], dtype='<U6'), 'SEP', 'C4', 'EOS', array(['yellow', 'star', '3'], dtype='<U6'), 'SEP', 'C4']\n",
      "[array(['red', 'circle', '4'], dtype='<U6'), array(['yellow', 'cross', '2'], dtype='<U6'), array(['blue', 'star', '4'], dtype='<U6'), array(['red', 'square', '3'], dtype='<U6'), array(['yellow', 'square', '3'], dtype='<U6'), 'SEP', 'C4', 'EOS', array(['blue', 'circle', '4'], dtype='<U6'), 'SEP', 'C3']\n",
      "[array(['blue', 'cross', '2'], dtype='<U6'), array(['green', 'star', '1'], dtype='<U6'), array(['red', 'circle', '2'], dtype='<U6'), array(['green', 'square', '3'], dtype='<U6'), array(['yellow', 'star', '1'], dtype='<U6'), 'SEP', 'C2', 'EOS', array(['blue', 'square', '4'], dtype='<U6'), 'SEP', 'C4']\n",
      "Feature for Classification:  0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_seqs, targets = [], []\n",
    "for input_seq, target in test_dataset[:10]:\n",
    "    input_seqs.append(input_seq)\n",
    "    targets.append(target)\n",
    "\n",
    "input_seqs = T.stack(input_seqs).to(device)\n",
    "targets = T.stack(targets).unsqueeze(1).to(device)\n",
    "\n",
    "predictions = model_inference(transformer, input_seqs)\n",
    "\n",
    "print(\"# Actual Trials\")\n",
    "test_batch = [np.asarray(item.cpu()) for item in [input_seqs[:, :-2], T.concatenate([input_seqs[:, -2:], targets], dim=1)]]\n",
    "output = wcst.visualise_batch(test_batch)\n",
    "\n",
    "print(\"# Predicted Trials\")\n",
    "prediction_batch = [np.asarray(item.cpu()) for item in [predictions[:, :-2], predictions[:, -2:]]]\n",
    "output = wcst.visualise_batch(prediction_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285911a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
