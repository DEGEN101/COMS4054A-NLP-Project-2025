{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd555848",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch as T\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "389721a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "\n",
    "device = T.device('cuda' if T.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "\n",
    "T.manual_seed(SEED)\n",
    "\n",
    "if T.cuda.is_available():\n",
    "    T.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08945224",
   "metadata": {},
   "source": [
    "## Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25ea2ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from datasets.custom_dataset import CustomWCSTDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ba4f2c",
   "metadata": {},
   "source": [
    "### 1. Dataset Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e95fdcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "TOTAL_BATCHES = 500\n",
    "TRAIN_TEST_SPLIT_RATIO = 0.6\n",
    "VALIDATION_TEST_SPLIT_RATIO = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e47ffd5",
   "metadata": {},
   "source": [
    "### 2. Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "365835c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset Init] Fixed context: 0\n",
      "[Dataset Init] Fixed context: 1\n",
      "[Dataset Init] Fixed context: 2\n",
      "[Dataset Init] Context switched -> 0\n",
      "[Dataset Init] Context switched -> 1\n",
      "[Dataset Init] Context switched -> 2\n",
      "[Dataset Init] Context switched -> 0\n",
      "[Dataset Init] Context switched -> 1\n",
      "[Dataset Init] Context switched -> 0\n",
      "[Dataset Init] Context switched -> 2\n",
      "[Dataset Init] Context switched -> 0\n",
      "[Dataset Init] Context switched -> 1\n",
      "[Dataset Init] Context switched -> 0\n",
      "[Dataset Init] Context switched -> 2\n",
      "[Dataset Init] Context switched -> 0\n",
      "[Dataset Init] Context switched -> 1\n",
      "[Dataset Init] Context switched -> 0\n",
      "[Dataset Init] Context switched -> 1\n",
      "[Dataset Init] Context switched -> 0\n",
      "[Dataset Init] Context switched -> 2\n",
      "[Dataset Init] Context switched -> 1\n",
      "[Dataset Init] Context switched -> 2\n",
      "[Dataset Init] Context switched -> 0\n",
      "[Dataset Init] Context switched -> 2\n"
     ]
    }
   ],
   "source": [
    "train_size = int(TOTAL_BATCHES * TRAIN_TEST_SPLIT_RATIO)\n",
    "validation_size = int((TOTAL_BATCHES - train_size) * VALIDATION_TEST_SPLIT_RATIO)\n",
    "test_size = TOTAL_BATCHES - train_size - validation_size\n",
    "\n",
    "train_datasets = {\n",
    "    \"color\": CustomWCSTDataset(total_batches=train_size // 2, fixed_context=0, sample_batch_size=BATCH_SIZE, allow_switch=False),\n",
    "    \"shape\": CustomWCSTDataset(total_batches=train_size // 2, fixed_context=1, sample_batch_size=BATCH_SIZE, allow_switch=False),\n",
    "    \"quantity\": CustomWCSTDataset(total_batches=train_size // 2, fixed_context=2, sample_batch_size=BATCH_SIZE, allow_switch=False)\n",
    "}\n",
    "\n",
    "train_loaders = {\n",
    "    ctx: DataLoader(ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    for ctx, ds in train_datasets.items()\n",
    "}\n",
    "\n",
    "validation_dataset = CustomWCSTDataset(\n",
    "        total_batches=validation_size, sample_batch_size=BATCH_SIZE\n",
    "    )\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "test_dataset = CustomWCSTDataset(\n",
    "        total_batches=validation_size, sample_batch_size=BATCH_SIZE\n",
    "    )\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f524267",
   "metadata": {},
   "source": [
    "## Transformer Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3a8bc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.decoder_transformer import DecoderTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7739c3fe",
   "metadata": {},
   "source": [
    "### 1. Transformer Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "606c1008",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCABULARY_SIZE = 70        # 64 cards + 4 categories + SEP + EOS\n",
    "EMBEDDING_SIZE = 256        # larger embedding to capture card features\n",
    "N_ATTENTION_HEADS = 8       # more heads for better multi-feature attention\n",
    "N_BLOCKS = 6                # same depth as before\n",
    "MAX_SEQUENCE_LENGTH = 10    # longer max sequence to accommodate multiple past trials\n",
    "FF_DIMS = 1024               # larger feedforward layer for better representation\n",
    "DROPOUT_PROB = 0.2        # reduce dropout slightly to retain signal in small batches\n",
    "CARD_DIMS = (4, 4, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab9fc96",
   "metadata": {},
   "source": [
    "### 2. Transformer Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f25ee98",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = DecoderTransformer(\n",
    "    VOCABULARY_SIZE, CARD_DIMS, EMBEDDING_SIZE, N_ATTENTION_HEADS,\n",
    "    N_BLOCKS, MAX_SEQUENCE_LENGTH, FF_DIMS, DROPOUT_PROB, device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7489333",
   "metadata": {},
   "source": [
    "## Training Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42f3ebdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import torch as T\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn, optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c5d23d",
   "metadata": {},
   "source": [
    "### 1. Train, Validate, Evaluate Model Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a8ee8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    train_loader: DataLoader,\n",
    "    validation_loader: DataLoader,\n",
    "    model: DecoderTransformer,\n",
    "    criterion: nn.CrossEntropyLoss,\n",
    "    optimizer: optim.Optimizer,\n",
    "    max_epochs: int = 20,\n",
    "    device: str | T.device = \"cpu\",\n",
    "):\n",
    "    best_val_loss = np.inf\n",
    "\n",
    "    train_losses, train_accs, train_perplexities = [], [], []\n",
    "    val_losses, val_accs, val_perplexities = [], [], []\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{max_epochs}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "        # --- Training ---\n",
    "        model.train()\n",
    "        epoch_train_losses = []\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        for batch_idx, (input_seq, target) in enumerate(train_loader):\n",
    "            input_seq, target = input_seq.to(device), target.view(-1).to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            logits = model(input_seq)  # [batch, seq_len, vocab]\n",
    "            logits = logits[:, -1, :]  # only the final step prediction [batch, vocab]\n",
    "\n",
    "            loss = criterion(logits, target)\n",
    "\n",
    "            # Backprop\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accuracy\n",
    "            preds = logits.argmax(dim=1)\n",
    "            total_correct += (preds == target).sum().item()\n",
    "            total_samples += target.size(0)\n",
    "\n",
    "            epoch_train_losses.append(loss.item())\n",
    "\n",
    "            if batch_idx % 100 == 0 or batch_idx == len(train_loader) - 1:\n",
    "                print(f\"Train Batch {batch_idx+1}/{len(train_loader)} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "        train_loss = np.mean(epoch_train_losses)\n",
    "        train_acc = total_correct / total_samples\n",
    "        train_perplexity = np.exp(train_loss)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        train_perplexities.append(train_perplexity)\n",
    "\n",
    "        print(f\"[Epoch {epoch+1}] Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Train Perplexity: {train_perplexity:.4f}\")\n",
    "\n",
    "        # --- Validation ---\n",
    "        model.eval()\n",
    "        val_batch_losses = []\n",
    "        val_correct = 0\n",
    "        val_samples = 0\n",
    "\n",
    "        with T.no_grad():\n",
    "            for input_seq, target in validation_loader:\n",
    "                input_seq, target = input_seq.to(device), target.view(-1).to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                logits = model(input_seq)  # [batch, seq_len, vocab]\n",
    "                logits = logits[:, -1, :]  # only the final step prediction [batch, vocab]\n",
    "\n",
    "                loss = criterion(logits, target)\n",
    "\n",
    "                preds = logits.argmax(dim=1)\n",
    "                val_correct += (preds == target).sum().item()\n",
    "                val_samples += target.size(0)\n",
    "\n",
    "                val_batch_losses.append(loss.item())\n",
    "\n",
    "        val_loss = np.mean(val_batch_losses)\n",
    "        val_acc = val_correct / val_samples\n",
    "        val_perplexity = np.exp(val_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_acc)\n",
    "        val_perplexities.append(val_perplexity)\n",
    "\n",
    "        print(f\"[Epoch {epoch+1}] Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | Val Perplexity: {val_perplexity:.4f}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "    print(\"\\nTraining complete\")\n",
    "\n",
    "    return {\n",
    "        \"train_losses\": train_losses,\n",
    "        \"train_accs\": train_accs,\n",
    "        \"train_perplexities\": train_perplexities,\n",
    "        \"val_losses\": val_losses,\n",
    "        \"val_accs\": val_accs,\n",
    "        \"val_perplexities\": val_perplexities,\n",
    "        \"best_val_loss\": best_val_loss,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6af9014f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_round_robin(\n",
    "    train_loaders: dict[str, DataLoader], validation_loader: DataLoader, model: nn.Module,\n",
    "    criterion: nn.CrossEntropyLoss, optimizer: optim.Optimizer, max_epochs: int = 20,\n",
    "    device: str | T.device = \"cpu\"\n",
    "):\n",
    "\n",
    "    history = {k: [] for k in [\n",
    "        \"train_losses\", \"train_accs\", \"train_perplexities\",\n",
    "        \"val_losses\", \"val_accs\", \"val_perplexities\"\n",
    "    ]}\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        print(f\"\\n[Epoch {epoch + 1}/{max_epochs}]\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "        model.train()\n",
    "        epoch_losses, total_correct, total_samples = [], 0, 0\n",
    "\n",
    "        # --- Build a round-robin iterator across all loaders ---\n",
    "        loaders_cycle = itertools.cycle(train_loaders.items())\n",
    "        active_iters = {ctx: iter(dl) for ctx, dl in train_loaders.items()}\n",
    "\n",
    "        # Find smallest loader length to roughly balance epoch size\n",
    "        min_len = min(len(dl) for dl in train_loaders.values())\n",
    "        total_batches = min_len * len(train_loaders)\n",
    "\n",
    "        for batch_idx in range(total_batches):\n",
    "            context, _ = next(loaders_cycle)\n",
    "            loader_iter = active_iters[context]\n",
    "\n",
    "            try:\n",
    "                input_seq, target = next(loader_iter)\n",
    "            except StopIteration:\n",
    "                # Restart exhausted iterator\n",
    "                active_iters[context] = iter(train_loaders[context])\n",
    "                input_seq, target = next(active_iters[context])\n",
    "\n",
    "            input_seq, target = input_seq.to(device), target.view(-1).to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            logits = model(input_seq)  # [batch, seq_len, vocab]\n",
    "            logits = logits[:, -1, :]  # predict final token only\n",
    "            loss = criterion(logits, target)\n",
    "\n",
    "            # Backprop\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Metrics\n",
    "            preds = logits.argmax(dim=1)\n",
    "            total_correct += (preds == target).sum().item()\n",
    "            total_samples += target.size(0)\n",
    "            epoch_losses.append(loss.item())\n",
    "\n",
    "            if batch_idx % 50 == 0 or batch_idx == total_batches - 1:\n",
    "                print(f\"[{context}] Batch {batch_idx+1}/{total_batches} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "        # --- Epoch stats ---\n",
    "        train_loss = np.mean(epoch_losses)\n",
    "        train_acc = total_correct / total_samples\n",
    "        train_perplexity = np.exp(train_loss)\n",
    "\n",
    "        print(f\"[Epoch {epoch+1}] Train Loss: {train_loss:.4f} | Acc: {train_acc:.4f} | Perplexity: {train_perplexity:.4f}\")\n",
    "\n",
    "        # --- Validation ---\n",
    "        model.eval()\n",
    "        val_losses, val_correct, val_samples = [], 0, 0\n",
    "\n",
    "        with T.no_grad():\n",
    "            for input_seq, target in validation_loader:\n",
    "                input_seq, target = input_seq.to(device), target.view(-1).to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                logits = model(input_seq)  # [batch, seq_len, vocab]\n",
    "\n",
    "                logits = logits[:, -1, :]\n",
    "                loss = criterion(logits, target)\n",
    "                \n",
    "                preds = logits.argmax(dim=1)\n",
    "                val_correct += (preds == target).sum().item()\n",
    "                val_samples += target.size(0)\n",
    "                val_losses.append(loss.item())\n",
    "\n",
    "        val_loss = np.mean(val_losses)\n",
    "        val_acc = val_correct / val_samples\n",
    "        val_perplexity = np.exp(val_loss)\n",
    "\n",
    "        print(f\"[Epoch {epoch+1}] Val Loss: {val_loss:.4f} | Acc: {val_acc:.4f} | Perplexity: {val_perplexity:.4f}\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "        # --- Logging ---\n",
    "        history[\"train_losses\"].append(train_loss)\n",
    "        history[\"train_accs\"].append(train_acc)\n",
    "        history[\"train_perplexities\"].append(train_perplexity)\n",
    "        history[\"val_losses\"].append(val_loss)\n",
    "        history[\"val_accs\"].append(val_acc)\n",
    "        history[\"val_perplexities\"].append(val_perplexity)\n",
    "\n",
    "    print(\"\\n Training complete (Round-Robin Mode)\")\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44864dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(test_loader: DataLoader, model: DecoderTransformer, criterion: nn.CrossEntropyLoss, device: str | T.device = \"cpu\"):\n",
    "\n",
    "    model.eval()\n",
    "    test_batch_losses = []\n",
    "    test_correct = 0\n",
    "    test_tokens = 0\n",
    "\n",
    "    with T.no_grad():\n",
    "        for input_seq, target in test_loader:\n",
    "            input_seq, target = input_seq.to(device), target.view(-1).to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            logits = model(input_seq)  # [batch, seq_len, vocab]\n",
    "            logits = logits[:, -1, :]\n",
    "            loss = criterion(logits, target)\n",
    "\n",
    "            preds = logits.argmax(dim=1)\n",
    "            test_correct += (preds == target).sum().item()\n",
    "            test_tokens += target.size(0)\n",
    "\n",
    "            test_batch_losses.append(loss.item())\n",
    "\n",
    "    test_loss = np.mean(test_batch_losses)\n",
    "    test_acc = test_correct / test_tokens\n",
    "    test_perplexity = np.exp(test_loss)\n",
    "\n",
    "    return {\n",
    "        \"test_loss\": test_loss,\n",
    "        \"test_acc\": test_acc,\n",
    "        \"test_perplexity\": test_perplexity\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d863c67",
   "metadata": {},
   "source": [
    "### 2. Train Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "37f53def",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 3e-4\n",
    "BATCH_SIZE = 64\n",
    "WEIGHT_DECAY = 1e-2\n",
    "WARMUP_STEPS = 400\n",
    "LABEL_SMOOTHING = 0.1\n",
    "MAX_EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c8f99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion =  nn.CrossEntropyLoss(label_smoothing=LABEL_SMOOTHING)\n",
    "optimizer = optim.AdamW(transformer.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=WARMUP_STEPS)  \n",
    "\n",
    "results = train_model_round_robin(\n",
    "    train_loaders, validation_loader, transformer, criterion, \n",
    "    optimizer, max_epochs=MAX_EPOCHS, device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917a1fa4",
   "metadata": {},
   "source": [
    "### 3. Test Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d144441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 193.9377 | Acc: 0.0000 | Perplexity: 1683039979285988384656731698260391983574259495674324658788660293877366467870005919744.0000\n"
     ]
    }
   ],
   "source": [
    "results = test_model(test_loader, transformer, criterion, device)\n",
    "print(f\"Test Loss: {results[\"test_loss\"]:.4f} | Acc: {results[\"test_acc\"]:.4f} | Perplexity: {results[\"test_perplexity\"]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28dac7b",
   "metadata": {},
   "source": [
    "## Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c3c7ecc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.wcst import WCST\n",
    "wcst = WCST(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0fc226e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.load(\"../models/decoder_transformer-0.8385_0.8361_0.8347.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6b590910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inference(model: DecoderTransformer, source_sequence):\n",
    "    model.eval()\n",
    "    generated = source_sequence\n",
    "    \n",
    "    with T.no_grad():\n",
    "        full_sequence = T.cat([generated], dim=1)\n",
    "        logits = model(full_sequence)\n",
    "    \n",
    "    # Greedy Selection\n",
    "    next_token = T.argmax(logits[:, -1, :], dim=-1, keepdim=True)\n",
    "\n",
    "    generated = T.cat([generated, next_token], dim=1)\n",
    "\n",
    "    return generated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7f458678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Actual Trials\n",
      "[array(['blue', 'star', '4'], dtype='<U6'), array(['green', 'square', '1'], dtype='<U6'), array(['yellow', 'circle', '1'], dtype='<U6'), array(['yellow', 'cross', '3'], dtype='<U6'), array(['blue', 'cross', '3'], dtype='<U6'), 'SEP', 'C4', 'EOS', array(['yellow', 'circle', '4'], dtype='<U6'), 'SEP', 'C3']\n",
      "[array(['yellow', 'circle', '3'], dtype='<U6'), array(['blue', 'cross', '1'], dtype='<U6'), array(['green', 'star', '1'], dtype='<U6'), array(['yellow', 'square', '1'], dtype='<U6'), array(['yellow', 'square', '3'], dtype='<U6'), 'SEP', 'C4', 'EOS', array(['red', 'circle', '4'], dtype='<U6'), 'SEP', 'C1']\n",
      "[array(['yellow', 'star', '4'], dtype='<U6'), array(['yellow', 'square', '4'], dtype='<U6'), array(['green', 'circle', '2'], dtype='<U6'), array(['green', 'cross', '2'], dtype='<U6'), array(['green', 'cross', '1'], dtype='<U6'), 'SEP', 'C4', 'EOS', array(['blue', 'square', '1'], dtype='<U6'), 'SEP', 'C2']\n",
      "[array(['green', 'cross', '1'], dtype='<U6'), array(['yellow', 'circle', '4'], dtype='<U6'), array(['red', 'square', '1'], dtype='<U6'), array(['red', 'star', '3'], dtype='<U6'), array(['yellow', 'circle', '1'], dtype='<U6'), 'SEP', 'C2', 'EOS', array(['green', 'cross', '4'], dtype='<U6'), 'SEP', 'C1']\n",
      "[array(['yellow', 'circle', '4'], dtype='<U6'), array(['yellow', 'star', '4'], dtype='<U6'), array(['red', 'cross', '2'], dtype='<U6'), array(['blue', 'square', '1'], dtype='<U6'), array(['green', 'cross', '3'], dtype='<U6'), 'SEP', 'C3', 'EOS', array(['yellow', 'cross', '2'], dtype='<U6'), 'SEP', 'C3']\n",
      "Feature for Classification:  1 \n",
      "\n",
      "# Predicted Trials\n",
      "[array(['blue', 'star', '4'], dtype='<U6'), array(['green', 'square', '1'], dtype='<U6'), array(['yellow', 'circle', '1'], dtype='<U6'), array(['yellow', 'cross', '3'], dtype='<U6'), array(['blue', 'cross', '3'], dtype='<U6'), 'SEP', 'C4', 'EOS', array(['yellow', 'circle', '4'], dtype='<U6'), 'SEP', 'C3']\n",
      "[array(['yellow', 'circle', '3'], dtype='<U6'), array(['blue', 'cross', '1'], dtype='<U6'), array(['green', 'star', '1'], dtype='<U6'), array(['yellow', 'square', '1'], dtype='<U6'), array(['yellow', 'square', '3'], dtype='<U6'), 'SEP', 'C4', 'EOS', array(['red', 'circle', '4'], dtype='<U6'), 'SEP', 'C1']\n",
      "[array(['yellow', 'star', '4'], dtype='<U6'), array(['yellow', 'square', '4'], dtype='<U6'), array(['green', 'circle', '2'], dtype='<U6'), array(['green', 'cross', '2'], dtype='<U6'), array(['green', 'cross', '1'], dtype='<U6'), 'SEP', 'C4', 'EOS', array(['blue', 'square', '1'], dtype='<U6'), 'SEP', 'C2']\n",
      "[array(['green', 'cross', '1'], dtype='<U6'), array(['yellow', 'circle', '4'], dtype='<U6'), array(['red', 'square', '1'], dtype='<U6'), array(['red', 'star', '3'], dtype='<U6'), array(['yellow', 'circle', '1'], dtype='<U6'), 'SEP', 'C2', 'EOS', array(['green', 'cross', '4'], dtype='<U6'), 'SEP', 'C1']\n",
      "[array(['yellow', 'circle', '4'], dtype='<U6'), array(['yellow', 'star', '4'], dtype='<U6'), array(['red', 'cross', '2'], dtype='<U6'), array(['blue', 'square', '1'], dtype='<U6'), array(['green', 'cross', '3'], dtype='<U6'), 'SEP', 'C3', 'EOS', array(['yellow', 'cross', '2'], dtype='<U6'), 'SEP', 'C3']\n",
      "Feature for Classification:  1 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_seqs, targets = [], []\n",
    "for input_seq, target in test_dataset[:5]:\n",
    "    input_seqs.append(input_seq)\n",
    "    targets.append(target)\n",
    "\n",
    "input_seqs = T.stack(input_seqs).to(device)\n",
    "targets = T.stack(targets).unsqueeze(1).to(device)\n",
    "\n",
    "predictions = model_inference(transformer, input_seqs)\n",
    "\n",
    "print(\"# Actual Trials\")\n",
    "test_batch = [np.asarray(item.cpu()) for item in [input_seqs[:, :-2], T.concatenate([input_seqs[:, -2:], targets], dim=1)]]\n",
    "output = wcst.visualise_batch(test_batch)\n",
    "\n",
    "print(\"# Predicted Trials\")\n",
    "prediction_batch = [np.asarray(item.cpu()) for item in [predictions[:, :-2], predictions[:, -2:]]]\n",
    "output = wcst.visualise_batch(prediction_batch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
