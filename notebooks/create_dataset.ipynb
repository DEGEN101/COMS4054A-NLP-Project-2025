{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd555848",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import torch as T\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08945224",
   "metadata": {},
   "source": [
    "## Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25ea2ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, random_split, Subset, ConcatDataset\n",
    "\n",
    "from datasets.wcst import WCST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d2bc29",
   "metadata": {},
   "source": [
    "### 1. Dataset Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b73743b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "TOTAL_BATCHES = 500\n",
    "MIN_SWITCH_BATCHES = 5\n",
    "MAX_SWITCH_BATCHES = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e47ffd5",
   "metadata": {},
   "source": [
    "### 2. Creating Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f50d4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_wcst_dataset(wcst, total_batches=5000, batch_size=64, fixed_context=None, allow_switch=True):\n",
    "    \"\"\"\n",
    "    Create a dataset of (encoder, decoder, target) tensors.\n",
    "    \n",
    "    Args:\n",
    "        wcst: WCST environment instance\n",
    "        total_batches: total number of batches to generate\n",
    "        batch_size: number of samples per batch\n",
    "        fixed_context: category name to fix the context (e.g., 0 (color), 1 (shape), 2 (quantity))\n",
    "        allow_switch: if True, allows random context switching\n",
    "    \"\"\"\n",
    "    encoder_inputs = []\n",
    "    decoder_inputs = []\n",
    "    targets = []\n",
    "\n",
    "    batches_since_switch = 0\n",
    "    switch_threshold = random.randint(MIN_SWITCH_BATCHES, MAX_SWITCH_BATCHES)\n",
    "\n",
    "    # --- Fix context if specified ---\n",
    "    if fixed_context is not None:\n",
    "        wcst.set_context(fixed_context)\n",
    "        print(f\"[!] Fixed context mode: {wcst.category_feature}\")\n",
    "\n",
    "    for _ in range(total_batches):\n",
    "        # Switch context if allowed\n",
    "        if allow_switch and batches_since_switch >= switch_threshold:\n",
    "            wcst.context_switch()\n",
    "            print(f\"[!] Context Switched - Using Category {wcst.category_feature}\")\n",
    "            batches_since_switch = 0\n",
    "            switch_threshold = random.randint(MIN_SWITCH_BATCHES, MAX_SWITCH_BATCHES)\n",
    "\n",
    "        X_batch, t_batch = next(wcst.gen_batch())\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            encoder_input = T.tensor(np.array(X_batch[i]), dtype=T.long).flatten()\n",
    "            decoder_input = T.tensor(t_batch[i][:-1], dtype=T.long)\n",
    "            target = T.tensor([t_batch[i][-1]], dtype=T.long)\n",
    "\n",
    "            encoder_inputs.append(encoder_input)\n",
    "            decoder_inputs.append(decoder_input)\n",
    "            targets.append(target)\n",
    "\n",
    "        batches_since_switch += 1\n",
    "\n",
    "    encoder_tensor = T.stack(encoder_inputs)\n",
    "    decoder_tensor = T.stack(decoder_inputs)\n",
    "    target_tensor = T.stack(targets)\n",
    "    return TensorDataset(encoder_tensor, decoder_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8053fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[DATASET CREATION]\n",
      "\n",
      "[CONTEXT 0] Generating dataset...\n",
      "[!] Fixed context mode: 0\n",
      "\n",
      "[CONTEXT 1] Generating dataset...\n",
      "[!] Fixed context mode: 1\n",
      "\n",
      "[CONTEXT 2] Generating dataset...\n",
      "[!] Fixed context mode: 2\n",
      "\n",
      "Dataset creation complete!\n",
      "Train (0): 6376\n",
      "Train (1): 6376\n",
      "Train (2): 6376\n",
      "Validation (combined): 6372\n",
      "Test (combined): 6372\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "contexts = [0, 1, 2]\n",
    "train_datasets = {}\n",
    "\n",
    "VAL_RATIO = 0.2\n",
    "TEST_RATIO = 0.2\n",
    "\n",
    "print(\"\\n[DATASET CREATION]\")\n",
    "train_splits, val_splits, test_splits = [], [], []\n",
    "\n",
    "for ctx in contexts:\n",
    "    print(f\"\\n[CONTEXT {ctx}] Generating dataset...\")\n",
    "    wcst = WCST(BATCH_SIZE)\n",
    "    \n",
    "    # Create dataset for this fixed context\n",
    "    full_dataset = create_wcst_dataset(\n",
    "        wcst,\n",
    "        total_batches=TOTAL_BATCHES // len(contexts),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        fixed_context=ctx,\n",
    "        allow_switch=False\n",
    "    )\n",
    "\n",
    "    # Compute split sizes\n",
    "    total_size = len(full_dataset)\n",
    "    val_size = int(total_size * VAL_RATIO)\n",
    "    test_size = int(total_size * TEST_RATIO)\n",
    "    train_size = total_size - val_size - test_size\n",
    "\n",
    "    # Split into train/val/test for this context\n",
    "    train_subset, val_subset, test_subset = random_split(full_dataset, [train_size, val_size, test_size])\n",
    "\n",
    "    train_datasets[ctx] = train_subset\n",
    "    val_splits.append(val_subset)\n",
    "    test_splits.append(test_subset)\n",
    "\n",
    "# Combine validation & test splits across all contexts\n",
    "validation_dataset = ConcatDataset(val_splits)\n",
    "test_dataset = ConcatDataset(test_splits)\n",
    "\n",
    "# Logging\n",
    "print(\"\\nDataset creation complete!\")\n",
    "for ctx, ds in train_datasets.items():\n",
    "    print(f\"Train ({ctx}): {len(ds)}\")\n",
    "print(f\"Validation (combined): {len(validation_dataset)}\")\n",
    "print(f\"Test (combined): {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5322bbe9",
   "metadata": {},
   "source": [
    "### 3. Save Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b118a471",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset(dataset, filename):\n",
    "    \"\"\"Recursively save a TensorDataset, Subset, or ConcatDataset to a .pt file.\"\"\"\n",
    "\n",
    "    def extract_tensors(ds):\n",
    "        if isinstance(ds, TensorDataset):\n",
    "            return ds.tensors\n",
    "\n",
    "        elif isinstance(ds, Subset):\n",
    "            base_tensors = extract_tensors(ds.dataset)\n",
    "            idx = ds.indices\n",
    "            return tuple(t[idx] for t in base_tensors)\n",
    "\n",
    "        elif isinstance(ds, ConcatDataset):\n",
    "            parts = [extract_tensors(d) for d in ds.datasets]\n",
    "            return tuple(T.cat(p, dim=0) for p in zip(*parts))\n",
    "\n",
    "        else:\n",
    "            raise TypeError(f\"Unsupported dataset type: {type(ds)}\")\n",
    "\n",
    "    encoder_data, decoder_data, targets = extract_tensors(dataset)\n",
    "    T.save((encoder_data, decoder_data, targets), filename)\n",
    "    print(f\"Saved {filename} ({len(targets)} samples)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c84893e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ../datasets/train_context_0.pt (6376 samples)\n",
      "Saved ../datasets/train_context_1.pt (6376 samples)\n",
      "Saved ../datasets/train_context_2.pt (6376 samples)\n",
      "Saved ../datasets/validation_dataset.pt (6372 samples)\n",
      "Saved ../datasets/test_dataset.pt (6372 samples)\n"
     ]
    }
   ],
   "source": [
    "for context, dataset in train_datasets.items():\n",
    "    save_dataset(dataset, f'../datasets/train_context_{context}.pt')\n",
    "\n",
    "# --- Save validation and test datasets ---\n",
    "save_dataset(validation_dataset, '../datasets/validation_dataset.pt')\n",
    "save_dataset(test_dataset, '../datasets/test_dataset.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75246f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
