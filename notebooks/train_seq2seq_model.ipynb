{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd555848",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch as T\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "389721a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "\n",
    "device = T.device('cuda' if T.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "\n",
    "T.manual_seed(SEED)\n",
    "\n",
    "if T.cuda.is_available():\n",
    "    T.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08945224",
   "metadata": {},
   "source": [
    "## Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ea2ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from datasets.custom_dataset import CustomWCSTDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ba4f2c",
   "metadata": {},
   "source": [
    "### 1. Dataset Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95fdcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "TOTAL_BATCHES = 500\n",
    "TRAIN_TEST_SPLIT_RATIO = 0.6\n",
    "VALIDATION_TEST_SPLIT_RATIO = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e47ffd5",
   "metadata": {},
   "source": [
    "### 2. Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365835c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(TOTAL_BATCHES * TRAIN_TEST_SPLIT_RATIO)\n",
    "validation_size = int((TOTAL_BATCHES - train_size) * VALIDATION_TEST_SPLIT_RATIO)\n",
    "test_size = TOTAL_BATCHES - train_size - validation_size\n",
    "\n",
    "train_datasets = {\n",
    "    \"color\": CustomWCSTDataset(total_batches=train_size // 2, fixed_context=0, sample_batch_size=BATCH_SIZE, allow_switch=False),\n",
    "    \"shape\": CustomWCSTDataset(total_batches=train_size // 2, fixed_context=1, sample_batch_size=BATCH_SIZE, allow_switch=False),\n",
    "    \"quantity\": CustomWCSTDataset(total_batches=train_size // 2, fixed_context=2, sample_batch_size=BATCH_SIZE, allow_switch=False)\n",
    "}\n",
    "\n",
    "train_loaders = {\n",
    "    ctx: DataLoader(ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    for ctx, ds in train_datasets.items()\n",
    "}\n",
    "\n",
    "validation_dataset = CustomWCSTDataset(\n",
    "        total_batches=validation_size, sample_batch_size=BATCH_SIZE\n",
    "    )\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "test_dataset = CustomWCSTDataset(\n",
    "        total_batches=validation_size, sample_batch_size=BATCH_SIZE\n",
    "    )\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f524267",
   "metadata": {},
   "source": [
    "## Transformer Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3a8bc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.transformer import Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7739c3fe",
   "metadata": {},
   "source": [
    "### 1. Transformer Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606c1008",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCABULARY_SIZE = 70        # 64 cards + 4 categories + SEP + EOS\n",
    "EMBEDDING_SIZE = 216        # larger embedding to capture card features\n",
    "N_ATTENTION_HEADS = 6       # more heads for better multi-feature attention\n",
    "N_BLOCKS = 3                # same depth as before\n",
    "MAX_SEQUENCE_LENGTH = 10    # longer max sequence to accommodate multiple past trials\n",
    "FF_DIMS = 256               # larger feedforward layer for better representation\n",
    "DROPOUT_PROB = 0.2        # reduce dropout slightly to retain signal in small batches\n",
    "CARD_DIMS = (4, 4, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab9fc96",
   "metadata": {},
   "source": [
    "### 2. Transformer Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f25ee98",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    VOCABULARY_SIZE, VOCABULARY_SIZE, CARD_DIMS, EMBEDDING_SIZE, N_ATTENTION_HEADS,\n",
    "    N_BLOCKS, MAX_SEQUENCE_LENGTH, FF_DIMS, DROPOUT_PROB, device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7489333",
   "metadata": {},
   "source": [
    "## Training Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42f3ebdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import torch as T\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn, optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c5d23d",
   "metadata": {},
   "source": [
    "### 1. Train, Validate, Evaluate Model Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a8ee8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    train_loader: DataLoader,\n",
    "    validation_loader: DataLoader,\n",
    "    model: Transformer,\n",
    "    criterion: nn.CrossEntropyLoss,\n",
    "    optimizer: optim.Optimizer,\n",
    "    max_epochs: int = 20,\n",
    "    device: str | T.device = \"cpu\",\n",
    "    patience: int = 3,\n",
    "):\n",
    "    best_val_loss = np.inf\n",
    "    patience_counter = 0\n",
    "\n",
    "    train_losses, train_accs, train_perplexities = [], [], []\n",
    "    val_losses, val_accs, val_perplexities = [], [], []\n",
    "    best_model_state = model.state_dict()\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{max_epochs}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "        # --- Training ---\n",
    "        model.train()\n",
    "        epoch_train_losses = []\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        for batch_idx, (encoder_input, decoder_input, target) in enumerate(train_loader):\n",
    "            encoder_input, decoder_input, target = encoder_input.to(device), decoder_input.to(device), target.view(-1).to(device)\n",
    "    \n",
    "            # Forward pass\n",
    "            logits = model(encoder_input, decoder_input)  # [batch, seq_len, vocab]\n",
    "            logits = logits[:, -1, :]  # only the final step prediction [batch, vocab]\n",
    "\n",
    "            loss = criterion(logits, target)\n",
    "\n",
    "            # Backprop\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accuracy\n",
    "            preds = logits.argmax(dim=1)\n",
    "            total_correct += (preds == target).sum().item()\n",
    "            total_samples += target.size(0)\n",
    "\n",
    "            epoch_train_losses.append(loss.item())\n",
    "\n",
    "            if batch_idx % 100 == 0 or batch_idx == len(train_loader) - 1:\n",
    "                print(f\"Train Batch {batch_idx+1}/{len(train_loader)} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "        train_loss = np.mean(epoch_train_losses)\n",
    "        train_acc = total_correct / total_samples\n",
    "        train_perplexity = np.exp(train_loss)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        train_perplexities.append(train_perplexity)\n",
    "\n",
    "        print(f\"[Epoch {epoch+1}] Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Train Perplexity: {train_perplexity:.4f}\")\n",
    "\n",
    "        # --- Validation ---\n",
    "        model.eval()\n",
    "        val_batch_losses = []\n",
    "        val_correct = 0\n",
    "        val_samples = 0\n",
    "\n",
    "        with T.no_grad():\n",
    "            for encoder_input, decoder_input, target in validation_loader:\n",
    "                encoder_input, decoder_input, target = encoder_input.to(device), decoder_input.to(device), target.view(-1).to(device)\n",
    "\n",
    "                logits = model(encoder_input, decoder_input) # [batch, seq_len, vocab]\n",
    "                logits = logits[:, -1, :]  # only the final step prediction [batch, vocab]\n",
    "\n",
    "                loss = criterion(logits, target)\n",
    "\n",
    "                preds = logits.argmax(dim=1)\n",
    "                val_correct += (preds == target).sum().item()\n",
    "                val_samples += target.size(0)\n",
    "\n",
    "                val_batch_losses.append(loss.item())\n",
    "\n",
    "        val_loss = np.mean(val_batch_losses)\n",
    "        val_acc = val_correct / val_samples\n",
    "        val_perplexity = np.exp(val_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_acc)\n",
    "        val_perplexities.append(val_perplexity)\n",
    "\n",
    "        print(f\"[Epoch {epoch+1}] Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | Val Perplexity: {val_perplexity:.4f}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "        # --- Early Stopping ---\n",
    "        # if val_loss < best_val_loss:\n",
    "        #     best_val_loss = val_loss\n",
    "        #     patience_counter = 0\n",
    "        #     best_model_state = model.state_dict()\n",
    "        #     print(f\"Validation loss improved — saving model (Loss: {val_loss:.4f})\")\n",
    "        # else:\n",
    "        #     patience_counter += 1\n",
    "        #     print(f\"No improvement ({patience_counter}/{patience})\")\n",
    "        #     if patience_counter >= patience:\n",
    "        #         print(\"\\nEarly stopping triggered. Restoring best model.\")\n",
    "        #         model.load_state_dict(best_model_state)\n",
    "        #         break\n",
    "\n",
    "    print(\"\\nTraining complete\")\n",
    "\n",
    "    return {\n",
    "        \"train_losses\": train_losses,\n",
    "        \"train_accs\": train_accs,\n",
    "        \"train_perplexities\": train_perplexities,\n",
    "        \"val_losses\": val_losses,\n",
    "        \"val_accs\": val_accs,\n",
    "        \"val_perplexities\": val_perplexities,\n",
    "        \"best_val_loss\": best_val_loss,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6af9014f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_round_robin(\n",
    "    train_loaders: dict[str, DataLoader], validation_loader: DataLoader, model: nn.Module,\n",
    "    criterion: nn.CrossEntropyLoss, optimizer: optim.Optimizer, max_epochs: int = 20,\n",
    "    device: str | T.device = \"cpu\", patience: int = 3,\n",
    "):\n",
    "    best_val_loss = np.inf\n",
    "    patience_counter = 0\n",
    "    best_model_state = model.state_dict()\n",
    "\n",
    "    history = {k: [] for k in [\n",
    "        \"train_losses\", \"train_accs\", \"train_perplexities\",\n",
    "        \"val_losses\", \"val_accs\", \"val_perplexities\"\n",
    "    ]}\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        print(f\"\\n[Epoch {epoch + 1}/{max_epochs}]\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "        model.train()\n",
    "        epoch_losses, total_correct, total_samples = [], 0, 0\n",
    "\n",
    "        # --- Build a round-robin iterator across all loaders ---\n",
    "        loaders_cycle = itertools.cycle(train_loaders.items())\n",
    "        active_iters = {ctx: iter(dl) for ctx, dl in train_loaders.items()}\n",
    "\n",
    "        # Find smallest loader length to roughly balance epoch size\n",
    "        min_len = min(len(dl) for dl in train_loaders.values())\n",
    "        total_batches = min_len * len(train_loaders)\n",
    "\n",
    "        for batch_idx in range(total_batches):\n",
    "            context, _ = next(loaders_cycle)\n",
    "            loader_iter = active_iters[context]\n",
    "\n",
    "            try:\n",
    "                encoder_input, decoder_input, target = next(loader_iter)\n",
    "            except StopIteration:\n",
    "                # Restart exhausted iterator\n",
    "                active_iters[context] = iter(train_loaders[context])\n",
    "                encoder_input, decoder_input, target = next(active_iters[context])\n",
    "\n",
    "            encoder_input, decoder_input, target = (\n",
    "                encoder_input.to(device),\n",
    "                decoder_input.to(device),\n",
    "                target.view(-1).to(device)\n",
    "            )\n",
    "\n",
    "            # Forward pass\n",
    "            logits = model(encoder_input, decoder_input)\n",
    "            logits = logits[:, -1, :]  # predict final token only\n",
    "            loss = criterion(logits, target)\n",
    "\n",
    "            # Backprop\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Metrics\n",
    "            preds = logits.argmax(dim=1)\n",
    "            total_correct += (preds == target).sum().item()\n",
    "            total_samples += target.size(0)\n",
    "            epoch_losses.append(loss.item())\n",
    "\n",
    "            if batch_idx % 50 == 0 or batch_idx == total_batches - 1:\n",
    "                print(f\"[{context}] Batch {batch_idx+1}/{total_batches} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "        # --- Epoch stats ---\n",
    "        train_loss = np.mean(epoch_losses)\n",
    "        train_acc = total_correct / total_samples\n",
    "        train_perplexity = np.exp(train_loss)\n",
    "\n",
    "        print(f\"[Epoch {epoch+1}] Train Loss: {train_loss:.4f} | Acc: {train_acc:.4f} | Perplexity: {train_perplexity:.4f}\")\n",
    "\n",
    "        # --- Validation ---\n",
    "        model.eval()\n",
    "        val_losses, val_correct, val_samples = [], 0, 0\n",
    "\n",
    "        with T.no_grad():\n",
    "            for encoder_input, decoder_input, target in validation_loader:\n",
    "                encoder_input, decoder_input, target = (\n",
    "                    encoder_input.to(device),\n",
    "                    decoder_input.to(device),\n",
    "                    target.view(-1).to(device)\n",
    "                )\n",
    "                logits = model(encoder_input, decoder_input)\n",
    "                logits = logits[:, -1, :]\n",
    "                loss = criterion(logits, target)\n",
    "                preds = logits.argmax(dim=1)\n",
    "                val_correct += (preds == target).sum().item()\n",
    "                val_samples += target.size(0)\n",
    "                val_losses.append(loss.item())\n",
    "\n",
    "        val_loss = np.mean(val_losses)\n",
    "        val_acc = val_correct / val_samples\n",
    "        val_perplexity = np.exp(val_loss)\n",
    "\n",
    "        print(f\"[Epoch {epoch+1}] Val Loss: {val_loss:.4f} | Acc: {val_acc:.4f} | Perplexity: {val_perplexity:.4f}\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "        # --- Logging ---\n",
    "        history[\"train_losses\"].append(train_loss)\n",
    "        history[\"train_accs\"].append(train_acc)\n",
    "        history[\"train_perplexities\"].append(train_perplexity)\n",
    "        history[\"val_losses\"].append(val_loss)\n",
    "        history[\"val_accs\"].append(val_acc)\n",
    "        history[\"val_perplexities\"].append(val_perplexity)\n",
    "\n",
    "        # --- Early stopping ---\n",
    "        # if val_loss < best_val_loss:\n",
    "        #     best_val_loss = val_loss\n",
    "        #     best_model_state = model.state_dict()\n",
    "        #     patience_counter = 0\n",
    "        # else:\n",
    "        #     patience_counter += 1\n",
    "        #     if patience_counter >= patience:\n",
    "        #         print(\"Early stopping — restoring best model.\")\n",
    "        #         model.load_state_dict(best_model_state)\n",
    "        #         break\n",
    "\n",
    "    print(\"\\n Training complete (Round-Robin Mode)\")\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44864dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(test_loader: DataLoader, model: Transformer, criterion: nn.CrossEntropyLoss, device: str | T.device = \"cpu\"):\n",
    "\n",
    "    model.eval()\n",
    "    test_batch_losses = []\n",
    "    test_correct = 0\n",
    "    test_tokens = 0\n",
    "\n",
    "    with T.no_grad():\n",
    "        for encoder_input, decoder_input, target in test_loader:\n",
    "            encoder_input, decoder_input, target = encoder_input.to(device), decoder_input.to(device), target.view(-1).to(device)\n",
    "            \n",
    "            logits = model(encoder_input, decoder_input)[:, -1, :]\n",
    "            loss = criterion(logits, target)\n",
    "\n",
    "            preds = logits.argmax(dim=1)\n",
    "            test_correct += (preds == target).sum().item()\n",
    "            test_tokens += target.size(0)\n",
    "\n",
    "            test_batch_losses.append(loss.item())\n",
    "\n",
    "    test_loss = np.mean(test_batch_losses)\n",
    "    test_acc = test_correct / test_tokens\n",
    "    test_perplexity = np.exp(test_loss)\n",
    "\n",
    "    return {\n",
    "        \"test_loss\": test_loss,\n",
    "        \"test_acc\": test_acc,\n",
    "        \"test_perplexity\": test_perplexity\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d863c67",
   "metadata": {},
   "source": [
    "### 2. Train Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37f53def",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 3e-4\n",
    "BATCH_SIZE = 64\n",
    "WEIGHT_DECAY = 1e-2\n",
    "WARMUP_STEPS = 400\n",
    "LABEL_SMOOTHING = 0.1\n",
    "MAX_EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5c8f99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 1/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 4.5288\n",
      "[quantity] Batch 51/300 | Loss: 1.9377\n",
      "[shape] Batch 101/300 | Loss: 1.9490\n",
      "[color] Batch 151/300 | Loss: 2.0080\n",
      "[quantity] Batch 201/300 | Loss: 1.9344\n",
      "[shape] Batch 251/300 | Loss: 1.9633\n",
      "[quantity] Batch 300/300 | Loss: 1.9583\n",
      "[Epoch 1] Train Loss: 2.0070 | Acc: 0.2510 | Perplexity: 7.4411\n",
      "[Epoch 1] Val Loss: 1.9669 | Acc: 0.2472 | Perplexity: 7.1483\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 2/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.9688\n",
      "[quantity] Batch 51/300 | Loss: 2.0039\n",
      "[shape] Batch 101/300 | Loss: 1.9685\n",
      "[color] Batch 151/300 | Loss: 1.9543\n",
      "[quantity] Batch 201/300 | Loss: 1.9805\n",
      "[shape] Batch 251/300 | Loss: 1.9988\n",
      "[quantity] Batch 300/300 | Loss: 1.9976\n",
      "[Epoch 2] Train Loss: 1.9763 | Acc: 0.2509 | Perplexity: 7.2162\n",
      "[Epoch 2] Val Loss: 1.9670 | Acc: 0.2564 | Perplexity: 7.1495\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 3/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.9944\n",
      "[quantity] Batch 51/300 | Loss: 1.9581\n",
      "[shape] Batch 101/300 | Loss: 1.9655\n",
      "[color] Batch 151/300 | Loss: 1.9714\n",
      "[quantity] Batch 201/300 | Loss: 1.9680\n",
      "[shape] Batch 251/300 | Loss: 1.9596\n",
      "[quantity] Batch 300/300 | Loss: 1.9684\n",
      "[Epoch 3] Train Loss: 1.9735 | Acc: 0.2506 | Perplexity: 7.1957\n",
      "[Epoch 3] Val Loss: 1.9709 | Acc: 0.2524 | Perplexity: 7.1771\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 4/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.9637\n",
      "[quantity] Batch 51/300 | Loss: 1.9714\n",
      "[shape] Batch 101/300 | Loss: 1.9636\n",
      "[color] Batch 151/300 | Loss: 1.9692\n",
      "[quantity] Batch 201/300 | Loss: 1.9574\n",
      "[shape] Batch 251/300 | Loss: 1.9810\n",
      "[quantity] Batch 300/300 | Loss: 1.9754\n",
      "[Epoch 4] Train Loss: 1.9720 | Acc: 0.2479 | Perplexity: 7.1849\n",
      "[Epoch 4] Val Loss: 1.9710 | Acc: 0.2462 | Perplexity: 7.1779\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 5/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.9641\n",
      "[quantity] Batch 51/300 | Loss: 1.9839\n",
      "[shape] Batch 101/300 | Loss: 1.9841\n",
      "[color] Batch 151/300 | Loss: 1.9748\n",
      "[quantity] Batch 201/300 | Loss: 1.9834\n",
      "[shape] Batch 251/300 | Loss: 1.9671\n",
      "[quantity] Batch 300/300 | Loss: 2.0126\n",
      "[Epoch 5] Train Loss: 1.9691 | Acc: 0.2567 | Perplexity: 7.1643\n",
      "[Epoch 5] Val Loss: 1.9678 | Acc: 0.2575 | Perplexity: 7.1552\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 6/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.9831\n",
      "[quantity] Batch 51/300 | Loss: 1.9799\n",
      "[shape] Batch 101/300 | Loss: 1.9577\n",
      "[color] Batch 151/300 | Loss: 1.9766\n",
      "[quantity] Batch 201/300 | Loss: 1.9626\n",
      "[shape] Batch 251/300 | Loss: 1.9573\n",
      "[quantity] Batch 300/300 | Loss: 1.9803\n",
      "[Epoch 6] Train Loss: 1.9665 | Acc: 0.2567 | Perplexity: 7.1459\n",
      "[Epoch 6] Val Loss: 1.9662 | Acc: 0.2638 | Perplexity: 7.1432\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 7/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.9802\n",
      "[quantity] Batch 51/300 | Loss: 1.9726\n",
      "[shape] Batch 101/300 | Loss: 1.9748\n",
      "[color] Batch 151/300 | Loss: 1.9593\n",
      "[quantity] Batch 201/300 | Loss: 1.9496\n",
      "[shape] Batch 251/300 | Loss: 1.9865\n",
      "[quantity] Batch 300/300 | Loss: 1.9494\n",
      "[Epoch 7] Train Loss: 1.9657 | Acc: 0.2632 | Perplexity: 7.1400\n",
      "[Epoch 7] Val Loss: 1.9735 | Acc: 0.2563 | Perplexity: 7.1961\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 8/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.9590\n",
      "[quantity] Batch 51/300 | Loss: 1.9627\n",
      "[shape] Batch 101/300 | Loss: 1.9685\n",
      "[color] Batch 151/300 | Loss: 1.9663\n",
      "[quantity] Batch 201/300 | Loss: 1.9642\n",
      "[shape] Batch 251/300 | Loss: 1.9473\n",
      "[quantity] Batch 300/300 | Loss: 1.9643\n",
      "[Epoch 8] Train Loss: 1.9638 | Acc: 0.2685 | Perplexity: 7.1265\n",
      "[Epoch 8] Val Loss: 1.9718 | Acc: 0.2563 | Perplexity: 7.1836\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 9/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.9700\n",
      "[quantity] Batch 51/300 | Loss: 1.9764\n",
      "[shape] Batch 101/300 | Loss: 1.9703\n",
      "[color] Batch 151/300 | Loss: 1.9674\n",
      "[quantity] Batch 201/300 | Loss: 1.9913\n",
      "[shape] Batch 251/300 | Loss: 1.9481\n",
      "[quantity] Batch 300/300 | Loss: 1.9718\n",
      "[Epoch 9] Train Loss: 1.9615 | Acc: 0.2751 | Perplexity: 7.1098\n",
      "[Epoch 9] Val Loss: 1.9701 | Acc: 0.2546 | Perplexity: 7.1716\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 10/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.9454\n",
      "[quantity] Batch 51/300 | Loss: 1.9535\n",
      "[shape] Batch 101/300 | Loss: 1.9371\n",
      "[color] Batch 151/300 | Loss: 1.9469\n",
      "[quantity] Batch 201/300 | Loss: 1.9293\n",
      "[shape] Batch 251/300 | Loss: 2.0026\n",
      "[quantity] Batch 300/300 | Loss: 1.9776\n",
      "[Epoch 10] Train Loss: 1.9599 | Acc: 0.2806 | Perplexity: 7.0985\n",
      "[Epoch 10] Val Loss: 1.9760 | Acc: 0.2555 | Perplexity: 7.2140\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 11/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.9718\n",
      "[quantity] Batch 51/300 | Loss: 1.9255\n",
      "[shape] Batch 101/300 | Loss: 1.9143\n",
      "[color] Batch 151/300 | Loss: 1.9871\n",
      "[quantity] Batch 201/300 | Loss: 1.9517\n",
      "[shape] Batch 251/300 | Loss: 1.9651\n",
      "[quantity] Batch 300/300 | Loss: 1.9648\n",
      "[Epoch 11] Train Loss: 1.9575 | Acc: 0.2836 | Perplexity: 7.0813\n",
      "[Epoch 11] Val Loss: 1.9736 | Acc: 0.2553 | Perplexity: 7.1962\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 12/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.9375\n",
      "[quantity] Batch 51/300 | Loss: 1.9336\n",
      "[shape] Batch 101/300 | Loss: 1.9703\n",
      "[color] Batch 151/300 | Loss: 1.9569\n",
      "[quantity] Batch 201/300 | Loss: 1.9499\n",
      "[shape] Batch 251/300 | Loss: 1.9516\n",
      "[quantity] Batch 300/300 | Loss: 1.9625\n",
      "[Epoch 12] Train Loss: 1.9561 | Acc: 0.2879 | Perplexity: 7.0716\n",
      "[Epoch 12] Val Loss: 1.9837 | Acc: 0.2517 | Perplexity: 7.2693\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 13/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.9518\n",
      "[quantity] Batch 51/300 | Loss: 1.9334\n",
      "[shape] Batch 101/300 | Loss: 1.9491\n",
      "[color] Batch 151/300 | Loss: 1.9428\n",
      "[quantity] Batch 201/300 | Loss: 1.9876\n",
      "[shape] Batch 251/300 | Loss: 1.9669\n",
      "[quantity] Batch 300/300 | Loss: 1.9638\n",
      "[Epoch 13] Train Loss: 1.9540 | Acc: 0.2974 | Perplexity: 7.0570\n",
      "[Epoch 13] Val Loss: 1.9693 | Acc: 0.2497 | Perplexity: 7.1657\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 14/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.9608\n",
      "[quantity] Batch 51/300 | Loss: 1.9374\n",
      "[shape] Batch 101/300 | Loss: 1.9177\n",
      "[color] Batch 151/300 | Loss: 1.8858\n",
      "[quantity] Batch 201/300 | Loss: 1.9392\n",
      "[shape] Batch 251/300 | Loss: 1.9710\n",
      "[quantity] Batch 300/300 | Loss: 1.9507\n",
      "[Epoch 14] Train Loss: 1.9494 | Acc: 0.3032 | Perplexity: 7.0241\n",
      "[Epoch 14] Val Loss: 1.9789 | Acc: 0.2464 | Perplexity: 7.2348\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 15/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.9234\n",
      "[quantity] Batch 51/300 | Loss: 1.9448\n",
      "[shape] Batch 101/300 | Loss: 1.9426\n",
      "[color] Batch 151/300 | Loss: 1.9675\n",
      "[quantity] Batch 201/300 | Loss: 1.9582\n",
      "[shape] Batch 251/300 | Loss: 1.9407\n",
      "[quantity] Batch 300/300 | Loss: 1.9658\n",
      "[Epoch 15] Train Loss: 1.9463 | Acc: 0.3085 | Perplexity: 7.0025\n",
      "[Epoch 15] Val Loss: 1.9981 | Acc: 0.2498 | Perplexity: 7.3753\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 16/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.8405\n",
      "[quantity] Batch 51/300 | Loss: 1.9901\n",
      "[shape] Batch 101/300 | Loss: 1.9800\n",
      "[color] Batch 151/300 | Loss: 1.9271\n",
      "[quantity] Batch 201/300 | Loss: 1.9166\n",
      "[shape] Batch 251/300 | Loss: 1.9701\n",
      "[quantity] Batch 300/300 | Loss: 1.8997\n",
      "[Epoch 16] Train Loss: 1.9420 | Acc: 0.3157 | Perplexity: 6.9728\n",
      "[Epoch 16] Val Loss: 2.0007 | Acc: 0.2536 | Perplexity: 7.3943\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 17/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.9407\n",
      "[quantity] Batch 51/300 | Loss: 1.9390\n",
      "[shape] Batch 101/300 | Loss: 1.9288\n",
      "[color] Batch 151/300 | Loss: 1.9643\n",
      "[quantity] Batch 201/300 | Loss: 1.9310\n",
      "[shape] Batch 251/300 | Loss: 1.9363\n",
      "[quantity] Batch 300/300 | Loss: 1.9441\n",
      "[Epoch 17] Train Loss: 1.9373 | Acc: 0.3294 | Perplexity: 6.9401\n",
      "[Epoch 17] Val Loss: 1.9876 | Acc: 0.2514 | Perplexity: 7.2981\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 18/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.9240\n",
      "[quantity] Batch 51/300 | Loss: 1.9075\n",
      "[shape] Batch 101/300 | Loss: 1.9235\n",
      "[color] Batch 151/300 | Loss: 1.9095\n",
      "[quantity] Batch 201/300 | Loss: 1.9901\n",
      "[shape] Batch 251/300 | Loss: 1.9506\n",
      "[quantity] Batch 300/300 | Loss: 1.8797\n",
      "[Epoch 18] Train Loss: 1.9328 | Acc: 0.3322 | Perplexity: 6.9091\n",
      "[Epoch 18] Val Loss: 2.0016 | Acc: 0.2415 | Perplexity: 7.4006\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 19/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.9099\n",
      "[quantity] Batch 51/300 | Loss: 1.9640\n",
      "[shape] Batch 101/300 | Loss: 1.9066\n",
      "[color] Batch 151/300 | Loss: 1.9117\n",
      "[quantity] Batch 201/300 | Loss: 1.9455\n",
      "[shape] Batch 251/300 | Loss: 1.9001\n",
      "[quantity] Batch 300/300 | Loss: 1.9472\n",
      "[Epoch 19] Train Loss: 1.9275 | Acc: 0.3399 | Perplexity: 6.8726\n",
      "[Epoch 19] Val Loss: 1.9958 | Acc: 0.2519 | Perplexity: 7.3583\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 20/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.9102\n",
      "[quantity] Batch 51/300 | Loss: 1.8870\n",
      "[shape] Batch 101/300 | Loss: 1.8827\n",
      "[color] Batch 151/300 | Loss: 1.8883\n",
      "[quantity] Batch 201/300 | Loss: 1.9164\n",
      "[shape] Batch 251/300 | Loss: 1.9030\n",
      "[quantity] Batch 300/300 | Loss: 1.9973\n",
      "[Epoch 20] Train Loss: 1.9225 | Acc: 0.3468 | Perplexity: 6.8379\n",
      "[Epoch 20] Val Loss: 2.0292 | Acc: 0.2544 | Perplexity: 7.6083\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 21/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.8924\n",
      "[quantity] Batch 51/300 | Loss: 1.8922\n",
      "[shape] Batch 101/300 | Loss: 1.8864\n",
      "[color] Batch 151/300 | Loss: 1.9071\n",
      "[quantity] Batch 201/300 | Loss: 1.8504\n",
      "[shape] Batch 251/300 | Loss: 1.9310\n",
      "[quantity] Batch 300/300 | Loss: 1.9263\n",
      "[Epoch 21] Train Loss: 1.9173 | Acc: 0.3527 | Perplexity: 6.8024\n",
      "[Epoch 21] Val Loss: 2.0140 | Acc: 0.2465 | Perplexity: 7.4933\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 22/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.8702\n",
      "[quantity] Batch 51/300 | Loss: 1.9912\n",
      "[shape] Batch 101/300 | Loss: 1.8596\n",
      "[color] Batch 151/300 | Loss: 1.9489\n",
      "[quantity] Batch 201/300 | Loss: 1.9185\n",
      "[shape] Batch 251/300 | Loss: 1.8962\n",
      "[quantity] Batch 300/300 | Loss: 2.0221\n",
      "[Epoch 22] Train Loss: 1.9122 | Acc: 0.3522 | Perplexity: 6.7681\n",
      "[Epoch 22] Val Loss: 2.0351 | Acc: 0.2467 | Perplexity: 7.6532\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 23/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.9372\n",
      "[quantity] Batch 51/300 | Loss: 1.8975\n",
      "[shape] Batch 101/300 | Loss: 1.9176\n",
      "[color] Batch 151/300 | Loss: 1.9725\n",
      "[quantity] Batch 201/300 | Loss: 1.9310\n",
      "[shape] Batch 251/300 | Loss: 1.9288\n",
      "[quantity] Batch 300/300 | Loss: 1.9088\n",
      "[Epoch 23] Train Loss: 1.9071 | Acc: 0.3662 | Perplexity: 6.7335\n",
      "[Epoch 23] Val Loss: 2.0304 | Acc: 0.2500 | Perplexity: 7.6174\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 24/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.9516\n",
      "[quantity] Batch 51/300 | Loss: 1.8620\n",
      "[shape] Batch 101/300 | Loss: 1.8329\n",
      "[color] Batch 151/300 | Loss: 1.9292\n",
      "[quantity] Batch 201/300 | Loss: 1.8563\n",
      "[shape] Batch 251/300 | Loss: 1.9125\n",
      "[quantity] Batch 300/300 | Loss: 1.9400\n",
      "[Epoch 24] Train Loss: 1.8980 | Acc: 0.3699 | Perplexity: 6.6726\n",
      "[Epoch 24] Val Loss: 2.0485 | Acc: 0.2539 | Perplexity: 7.7560\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 25/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.7883\n",
      "[quantity] Batch 51/300 | Loss: 1.8377\n",
      "[shape] Batch 101/300 | Loss: 1.8555\n",
      "[color] Batch 151/300 | Loss: 1.8357\n",
      "[quantity] Batch 201/300 | Loss: 1.9431\n",
      "[shape] Batch 251/300 | Loss: 1.8770\n",
      "[quantity] Batch 300/300 | Loss: 1.9303\n",
      "[Epoch 25] Train Loss: 1.8923 | Acc: 0.3787 | Perplexity: 6.6348\n",
      "[Epoch 25] Val Loss: 2.0428 | Acc: 0.2451 | Perplexity: 7.7122\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 26/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.8137\n",
      "[quantity] Batch 51/300 | Loss: 1.8890\n",
      "[shape] Batch 101/300 | Loss: 1.8974\n",
      "[color] Batch 151/300 | Loss: 1.8904\n",
      "[quantity] Batch 201/300 | Loss: 1.8563\n",
      "[shape] Batch 251/300 | Loss: 1.8683\n",
      "[quantity] Batch 300/300 | Loss: 1.8994\n",
      "[Epoch 26] Train Loss: 1.8874 | Acc: 0.3847 | Perplexity: 6.6022\n",
      "[Epoch 26] Val Loss: 2.0408 | Acc: 0.2445 | Perplexity: 7.6964\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 27/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.9045\n",
      "[quantity] Batch 51/300 | Loss: 1.8933\n",
      "[shape] Batch 101/300 | Loss: 1.8827\n",
      "[color] Batch 151/300 | Loss: 1.9244\n",
      "[quantity] Batch 201/300 | Loss: 1.8901\n",
      "[shape] Batch 251/300 | Loss: 1.8453\n",
      "[quantity] Batch 300/300 | Loss: 1.8453\n",
      "[Epoch 27] Train Loss: 1.8813 | Acc: 0.3848 | Perplexity: 6.5620\n",
      "[Epoch 27] Val Loss: 2.0562 | Acc: 0.2489 | Perplexity: 7.8165\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 28/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.8239\n",
      "[quantity] Batch 51/300 | Loss: 1.9889\n",
      "[shape] Batch 101/300 | Loss: 1.8623\n",
      "[color] Batch 151/300 | Loss: 1.8586\n",
      "[quantity] Batch 201/300 | Loss: 1.8190\n",
      "[shape] Batch 251/300 | Loss: 1.9404\n",
      "[quantity] Batch 300/300 | Loss: 1.9741\n",
      "[Epoch 28] Train Loss: 1.8718 | Acc: 0.3991 | Perplexity: 6.5002\n",
      "[Epoch 28] Val Loss: 2.0707 | Acc: 0.2506 | Perplexity: 7.9302\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 29/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.8563\n",
      "[quantity] Batch 51/300 | Loss: 1.8189\n",
      "[shape] Batch 101/300 | Loss: 1.8431\n",
      "[color] Batch 151/300 | Loss: 1.7601\n",
      "[quantity] Batch 201/300 | Loss: 1.9223\n",
      "[shape] Batch 251/300 | Loss: 1.8738\n",
      "[quantity] Batch 300/300 | Loss: 2.0015\n",
      "[Epoch 29] Train Loss: 1.8631 | Acc: 0.4021 | Perplexity: 6.4434\n",
      "[Epoch 29] Val Loss: 2.0927 | Acc: 0.2508 | Perplexity: 8.1068\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 30/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.8922\n",
      "[quantity] Batch 51/300 | Loss: 1.8268\n",
      "[shape] Batch 101/300 | Loss: 1.8865\n",
      "[color] Batch 151/300 | Loss: 1.8878\n",
      "[quantity] Batch 201/300 | Loss: 1.8785\n",
      "[shape] Batch 251/300 | Loss: 1.8412\n",
      "[quantity] Batch 300/300 | Loss: 1.7857\n",
      "[Epoch 30] Train Loss: 1.8569 | Acc: 0.4087 | Perplexity: 6.4037\n",
      "[Epoch 30] Val Loss: 2.1090 | Acc: 0.2478 | Perplexity: 8.2398\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 31/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.7512\n",
      "[quantity] Batch 51/300 | Loss: 1.9181\n",
      "[shape] Batch 101/300 | Loss: 1.8862\n",
      "[color] Batch 151/300 | Loss: 1.8425\n",
      "[quantity] Batch 201/300 | Loss: 1.8960\n",
      "[shape] Batch 251/300 | Loss: 1.8787\n",
      "[quantity] Batch 300/300 | Loss: 1.8857\n",
      "[Epoch 31] Train Loss: 1.8477 | Acc: 0.4191 | Perplexity: 6.3455\n",
      "[Epoch 31] Val Loss: 2.1413 | Acc: 0.2470 | Perplexity: 8.5106\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 32/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.7675\n",
      "[quantity] Batch 51/300 | Loss: 1.8432\n",
      "[shape] Batch 101/300 | Loss: 1.7847\n",
      "[color] Batch 151/300 | Loss: 1.7998\n",
      "[quantity] Batch 201/300 | Loss: 1.9331\n",
      "[shape] Batch 251/300 | Loss: 1.7847\n",
      "[quantity] Batch 300/300 | Loss: 1.8701\n",
      "[Epoch 32] Train Loss: 1.8408 | Acc: 0.4265 | Perplexity: 6.3018\n",
      "[Epoch 32] Val Loss: 2.0877 | Acc: 0.2495 | Perplexity: 8.0665\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 33/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.7655\n",
      "[quantity] Batch 51/300 | Loss: 1.8404\n",
      "[shape] Batch 101/300 | Loss: 1.8329\n",
      "[color] Batch 151/300 | Loss: 1.8907\n",
      "[quantity] Batch 201/300 | Loss: 1.7547\n",
      "[shape] Batch 251/300 | Loss: 1.8611\n",
      "[quantity] Batch 300/300 | Loss: 1.7792\n",
      "[Epoch 33] Train Loss: 1.8293 | Acc: 0.4355 | Perplexity: 6.2297\n",
      "[Epoch 33] Val Loss: 2.2045 | Acc: 0.2481 | Perplexity: 9.0655\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 34/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.8326\n",
      "[quantity] Batch 51/300 | Loss: 1.7676\n",
      "[shape] Batch 101/300 | Loss: 1.7567\n",
      "[color] Batch 151/300 | Loss: 1.8621\n",
      "[quantity] Batch 201/300 | Loss: 1.7307\n",
      "[shape] Batch 251/300 | Loss: 1.8565\n",
      "[quantity] Batch 300/300 | Loss: 1.7306\n",
      "[Epoch 34] Train Loss: 1.8235 | Acc: 0.4372 | Perplexity: 6.1933\n",
      "[Epoch 34] Val Loss: 2.2033 | Acc: 0.2473 | Perplexity: 9.0545\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 35/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.6989\n",
      "[quantity] Batch 51/300 | Loss: 1.8141\n",
      "[shape] Batch 101/300 | Loss: 1.8131\n",
      "[color] Batch 151/300 | Loss: 1.8854\n",
      "[quantity] Batch 201/300 | Loss: 1.8515\n",
      "[shape] Batch 251/300 | Loss: 1.7920\n",
      "[quantity] Batch 300/300 | Loss: 1.7739\n",
      "[Epoch 35] Train Loss: 1.8147 | Acc: 0.4420 | Perplexity: 6.1391\n",
      "[Epoch 35] Val Loss: 2.1417 | Acc: 0.2506 | Perplexity: 8.5139\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 36/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.7711\n",
      "[quantity] Batch 51/300 | Loss: 1.8151\n",
      "[shape] Batch 101/300 | Loss: 1.7311\n",
      "[color] Batch 151/300 | Loss: 1.8230\n",
      "[quantity] Batch 201/300 | Loss: 1.7477\n",
      "[shape] Batch 251/300 | Loss: 1.8026\n",
      "[quantity] Batch 300/300 | Loss: 1.8320\n",
      "[Epoch 36] Train Loss: 1.8042 | Acc: 0.4517 | Perplexity: 6.0751\n",
      "[Epoch 36] Val Loss: 2.1929 | Acc: 0.2411 | Perplexity: 8.9615\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 37/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.8180\n",
      "[quantity] Batch 51/300 | Loss: 1.8401\n",
      "[shape] Batch 101/300 | Loss: 1.6532\n",
      "[color] Batch 151/300 | Loss: 1.8234\n",
      "[quantity] Batch 201/300 | Loss: 1.8024\n",
      "[shape] Batch 251/300 | Loss: 1.8269\n",
      "[quantity] Batch 300/300 | Loss: 1.7714\n",
      "[Epoch 37] Train Loss: 1.7984 | Acc: 0.4637 | Perplexity: 6.0398\n",
      "[Epoch 37] Val Loss: 2.1684 | Acc: 0.2459 | Perplexity: 8.7441\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 38/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.7584\n",
      "[quantity] Batch 51/300 | Loss: 1.8014\n",
      "[shape] Batch 101/300 | Loss: 1.8478\n",
      "[color] Batch 151/300 | Loss: 1.9073\n",
      "[quantity] Batch 201/300 | Loss: 1.8857\n",
      "[shape] Batch 251/300 | Loss: 1.7434\n",
      "[quantity] Batch 300/300 | Loss: 1.8968\n",
      "[Epoch 38] Train Loss: 1.7881 | Acc: 0.4640 | Perplexity: 5.9783\n",
      "[Epoch 38] Val Loss: 2.1750 | Acc: 0.2439 | Perplexity: 8.8019\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 39/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.7212\n",
      "[quantity] Batch 51/300 | Loss: 1.7630\n",
      "[shape] Batch 101/300 | Loss: 1.8510\n",
      "[color] Batch 151/300 | Loss: 1.6774\n",
      "[quantity] Batch 201/300 | Loss: 1.8180\n",
      "[shape] Batch 251/300 | Loss: 1.7745\n",
      "[quantity] Batch 300/300 | Loss: 1.8931\n",
      "[Epoch 39] Train Loss: 1.7769 | Acc: 0.4700 | Perplexity: 5.9117\n",
      "[Epoch 39] Val Loss: 2.2472 | Acc: 0.2469 | Perplexity: 9.4609\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 40/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.7061\n",
      "[quantity] Batch 51/300 | Loss: 1.7596\n",
      "[shape] Batch 101/300 | Loss: 1.8484\n",
      "[color] Batch 151/300 | Loss: 1.8645\n",
      "[quantity] Batch 201/300 | Loss: 1.8467\n",
      "[shape] Batch 251/300 | Loss: 1.7882\n",
      "[quantity] Batch 300/300 | Loss: 1.9075\n",
      "[Epoch 40] Train Loss: 1.7729 | Acc: 0.4782 | Perplexity: 5.8882\n",
      "[Epoch 40] Val Loss: 2.2453 | Acc: 0.2428 | Perplexity: 9.4429\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 41/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.9073\n",
      "[quantity] Batch 51/300 | Loss: 1.8345\n",
      "[shape] Batch 101/300 | Loss: 1.7519\n",
      "[color] Batch 151/300 | Loss: 1.7435\n",
      "[quantity] Batch 201/300 | Loss: 1.6877\n",
      "[shape] Batch 251/300 | Loss: 1.8421\n",
      "[quantity] Batch 300/300 | Loss: 1.6066\n",
      "[Epoch 41] Train Loss: 1.7669 | Acc: 0.4803 | Perplexity: 5.8529\n",
      "[Epoch 41] Val Loss: 2.2561 | Acc: 0.2486 | Perplexity: 9.5456\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 42/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.7062\n",
      "[quantity] Batch 51/300 | Loss: 1.7328\n",
      "[shape] Batch 101/300 | Loss: 1.6925\n",
      "[color] Batch 151/300 | Loss: 1.7816\n",
      "[quantity] Batch 201/300 | Loss: 1.7026\n",
      "[shape] Batch 251/300 | Loss: 1.6931\n",
      "[quantity] Batch 300/300 | Loss: 1.7632\n",
      "[Epoch 42] Train Loss: 1.7546 | Acc: 0.4937 | Perplexity: 5.7811\n",
      "[Epoch 42] Val Loss: 2.2341 | Acc: 0.2433 | Perplexity: 9.3383\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 43/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.8456\n",
      "[quantity] Batch 51/300 | Loss: 1.7186\n",
      "[shape] Batch 101/300 | Loss: 1.6965\n",
      "[color] Batch 151/300 | Loss: 1.7207\n",
      "[quantity] Batch 201/300 | Loss: 1.9343\n",
      "[shape] Batch 251/300 | Loss: 1.7481\n",
      "[quantity] Batch 300/300 | Loss: 2.0672\n",
      "[Epoch 43] Train Loss: 1.7419 | Acc: 0.5028 | Perplexity: 5.7082\n",
      "[Epoch 43] Val Loss: 2.2826 | Acc: 0.2524 | Perplexity: 9.8024\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 44/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.7548\n",
      "[quantity] Batch 51/300 | Loss: 1.6596\n",
      "[shape] Batch 101/300 | Loss: 1.7484\n",
      "[color] Batch 151/300 | Loss: 1.7001\n",
      "[quantity] Batch 201/300 | Loss: 1.6166\n",
      "[shape] Batch 251/300 | Loss: 1.8688\n",
      "[quantity] Batch 300/300 | Loss: 1.9924\n",
      "[Epoch 44] Train Loss: 1.7402 | Acc: 0.5020 | Perplexity: 5.6983\n",
      "[Epoch 44] Val Loss: 2.2530 | Acc: 0.2498 | Perplexity: 9.5163\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 45/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.7357\n",
      "[quantity] Batch 51/300 | Loss: 1.6991\n",
      "[shape] Batch 101/300 | Loss: 1.8212\n",
      "[color] Batch 151/300 | Loss: 1.7268\n",
      "[quantity] Batch 201/300 | Loss: 1.7359\n",
      "[shape] Batch 251/300 | Loss: 1.7894\n",
      "[quantity] Batch 300/300 | Loss: 1.8288\n",
      "[Epoch 45] Train Loss: 1.7253 | Acc: 0.5065 | Perplexity: 5.6142\n",
      "[Epoch 45] Val Loss: 2.2509 | Acc: 0.2517 | Perplexity: 9.4965\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 46/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.6828\n",
      "[quantity] Batch 51/300 | Loss: 1.7038\n",
      "[shape] Batch 101/300 | Loss: 1.7124\n",
      "[color] Batch 151/300 | Loss: 1.6877\n",
      "[quantity] Batch 201/300 | Loss: 1.6698\n",
      "[shape] Batch 251/300 | Loss: 1.6756\n",
      "[quantity] Batch 300/300 | Loss: 1.8012\n",
      "[Epoch 46] Train Loss: 1.7155 | Acc: 0.5187 | Perplexity: 5.5596\n",
      "[Epoch 46] Val Loss: 2.3242 | Acc: 0.2439 | Perplexity: 10.2184\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 47/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.6642\n",
      "[quantity] Batch 51/300 | Loss: 1.7480\n",
      "[shape] Batch 101/300 | Loss: 1.6650\n",
      "[color] Batch 151/300 | Loss: 1.6903\n",
      "[quantity] Batch 201/300 | Loss: 1.7624\n",
      "[shape] Batch 251/300 | Loss: 1.7614\n",
      "[quantity] Batch 300/300 | Loss: 2.0200\n",
      "[Epoch 47] Train Loss: 1.7065 | Acc: 0.5241 | Perplexity: 5.5096\n",
      "[Epoch 47] Val Loss: 2.3016 | Acc: 0.2472 | Perplexity: 9.9897\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 48/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.5965\n",
      "[quantity] Batch 51/300 | Loss: 1.8093\n",
      "[shape] Batch 101/300 | Loss: 1.6862\n",
      "[color] Batch 151/300 | Loss: 1.6947\n",
      "[quantity] Batch 201/300 | Loss: 1.6935\n",
      "[shape] Batch 251/300 | Loss: 1.6861\n",
      "[quantity] Batch 300/300 | Loss: 1.6255\n",
      "[Epoch 48] Train Loss: 1.6993 | Acc: 0.5289 | Perplexity: 5.4702\n",
      "[Epoch 48] Val Loss: 2.3340 | Acc: 0.2514 | Perplexity: 10.3191\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 49/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.6812\n",
      "[quantity] Batch 51/300 | Loss: 1.7417\n",
      "[shape] Batch 101/300 | Loss: 1.5748\n",
      "[color] Batch 151/300 | Loss: 1.7006\n",
      "[quantity] Batch 201/300 | Loss: 1.7251\n",
      "[shape] Batch 251/300 | Loss: 1.6737\n",
      "[quantity] Batch 300/300 | Loss: 1.7069\n",
      "[Epoch 49] Train Loss: 1.6961 | Acc: 0.5324 | Perplexity: 5.4526\n",
      "[Epoch 49] Val Loss: 2.3017 | Acc: 0.2473 | Perplexity: 9.9914\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 50/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.6828\n",
      "[quantity] Batch 51/300 | Loss: 1.6324\n",
      "[shape] Batch 101/300 | Loss: 1.7165\n",
      "[color] Batch 151/300 | Loss: 1.6371\n",
      "[quantity] Batch 201/300 | Loss: 1.6642\n",
      "[shape] Batch 251/300 | Loss: 1.6893\n",
      "[quantity] Batch 300/300 | Loss: 1.7207\n",
      "[Epoch 50] Train Loss: 1.6838 | Acc: 0.5397 | Perplexity: 5.3860\n",
      "[Epoch 50] Val Loss: 2.3574 | Acc: 0.2456 | Perplexity: 10.5632\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 51/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.6844\n",
      "[quantity] Batch 51/300 | Loss: 1.7619\n",
      "[shape] Batch 101/300 | Loss: 1.6213\n",
      "[color] Batch 151/300 | Loss: 1.7188\n",
      "[quantity] Batch 201/300 | Loss: 1.8349\n",
      "[shape] Batch 251/300 | Loss: 1.6925\n",
      "[quantity] Batch 300/300 | Loss: 1.6544\n",
      "[Epoch 51] Train Loss: 1.6657 | Acc: 0.5484 | Perplexity: 5.2894\n",
      "[Epoch 51] Val Loss: 2.4093 | Acc: 0.2492 | Perplexity: 11.1265\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 52/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.4873\n",
      "[quantity] Batch 51/300 | Loss: 1.6584\n",
      "[shape] Batch 101/300 | Loss: 1.5758\n",
      "[color] Batch 151/300 | Loss: 1.6485\n",
      "[quantity] Batch 201/300 | Loss: 1.7057\n",
      "[shape] Batch 251/300 | Loss: 1.6669\n",
      "[quantity] Batch 300/300 | Loss: 1.5258\n",
      "[Epoch 52] Train Loss: 1.6537 | Acc: 0.5602 | Perplexity: 5.2261\n",
      "[Epoch 52] Val Loss: 2.3806 | Acc: 0.2486 | Perplexity: 10.8110\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 53/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.6241\n",
      "[quantity] Batch 51/300 | Loss: 1.6760\n",
      "[shape] Batch 101/300 | Loss: 1.7784\n",
      "[color] Batch 151/300 | Loss: 1.7697\n",
      "[quantity] Batch 201/300 | Loss: 1.7040\n",
      "[shape] Batch 251/300 | Loss: 1.4596\n",
      "[quantity] Batch 300/300 | Loss: 1.7553\n",
      "[Epoch 53] Train Loss: 1.6496 | Acc: 0.5593 | Perplexity: 5.2050\n",
      "[Epoch 53] Val Loss: 2.4093 | Acc: 0.2465 | Perplexity: 11.1257\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 54/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.6371\n",
      "[quantity] Batch 51/300 | Loss: 1.6528\n",
      "[shape] Batch 101/300 | Loss: 1.6027\n",
      "[color] Batch 151/300 | Loss: 1.5590\n",
      "[quantity] Batch 201/300 | Loss: 1.6616\n",
      "[shape] Batch 251/300 | Loss: 1.6833\n",
      "[quantity] Batch 300/300 | Loss: 1.7256\n",
      "[Epoch 54] Train Loss: 1.6359 | Acc: 0.5683 | Perplexity: 5.1338\n",
      "[Epoch 54] Val Loss: 2.4060 | Acc: 0.2472 | Perplexity: 11.0895\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 55/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.4782\n",
      "[quantity] Batch 51/300 | Loss: 1.6754\n",
      "[shape] Batch 101/300 | Loss: 1.6375\n",
      "[color] Batch 151/300 | Loss: 1.5790\n",
      "[quantity] Batch 201/300 | Loss: 1.7877\n",
      "[shape] Batch 251/300 | Loss: 1.6225\n",
      "[quantity] Batch 300/300 | Loss: 1.5281\n",
      "[Epoch 55] Train Loss: 1.6244 | Acc: 0.5755 | Perplexity: 5.0753\n",
      "[Epoch 55] Val Loss: 2.3935 | Acc: 0.2508 | Perplexity: 10.9512\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 56/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.6013\n",
      "[quantity] Batch 51/300 | Loss: 1.6060\n",
      "[shape] Batch 101/300 | Loss: 1.5566\n",
      "[color] Batch 151/300 | Loss: 1.5288\n",
      "[quantity] Batch 201/300 | Loss: 1.6177\n",
      "[shape] Batch 251/300 | Loss: 1.7493\n",
      "[quantity] Batch 300/300 | Loss: 1.7018\n",
      "[Epoch 56] Train Loss: 1.6177 | Acc: 0.5797 | Perplexity: 5.0415\n",
      "[Epoch 56] Val Loss: 2.4081 | Acc: 0.2566 | Perplexity: 11.1124\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 57/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.4739\n",
      "[quantity] Batch 51/300 | Loss: 1.5978\n",
      "[shape] Batch 101/300 | Loss: 1.4808\n",
      "[color] Batch 151/300 | Loss: 1.6709\n",
      "[quantity] Batch 201/300 | Loss: 1.6478\n",
      "[shape] Batch 251/300 | Loss: 1.7287\n",
      "[quantity] Batch 300/300 | Loss: 1.6832\n",
      "[Epoch 57] Train Loss: 1.6071 | Acc: 0.5843 | Perplexity: 4.9884\n",
      "[Epoch 57] Val Loss: 2.4012 | Acc: 0.2553 | Perplexity: 11.0364\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 58/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.5107\n",
      "[quantity] Batch 51/300 | Loss: 1.5585\n",
      "[shape] Batch 101/300 | Loss: 1.6960\n",
      "[color] Batch 151/300 | Loss: 1.6888\n",
      "[quantity] Batch 201/300 | Loss: 1.5949\n",
      "[shape] Batch 251/300 | Loss: 1.5887\n",
      "[quantity] Batch 300/300 | Loss: 1.7213\n",
      "[Epoch 58] Train Loss: 1.5898 | Acc: 0.5958 | Perplexity: 4.9029\n",
      "[Epoch 58] Val Loss: 2.4781 | Acc: 0.2530 | Perplexity: 11.9187\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 59/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.5398\n",
      "[quantity] Batch 51/300 | Loss: 1.6087\n",
      "[shape] Batch 101/300 | Loss: 1.5632\n",
      "[color] Batch 151/300 | Loss: 1.6752\n",
      "[quantity] Batch 201/300 | Loss: 1.6557\n",
      "[shape] Batch 251/300 | Loss: 1.6179\n",
      "[quantity] Batch 300/300 | Loss: 1.6369\n",
      "[Epoch 59] Train Loss: 1.5786 | Acc: 0.6001 | Perplexity: 4.8480\n",
      "[Epoch 59] Val Loss: 2.4989 | Acc: 0.2522 | Perplexity: 12.1691\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 60/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.4967\n",
      "[quantity] Batch 51/300 | Loss: 1.7532\n",
      "[shape] Batch 101/300 | Loss: 1.6528\n",
      "[color] Batch 151/300 | Loss: 1.5567\n",
      "[quantity] Batch 201/300 | Loss: 1.6218\n",
      "[shape] Batch 251/300 | Loss: 1.4651\n",
      "[quantity] Batch 300/300 | Loss: 1.7096\n",
      "[Epoch 60] Train Loss: 1.5672 | Acc: 0.6077 | Perplexity: 4.7932\n",
      "[Epoch 60] Val Loss: 2.4618 | Acc: 0.2621 | Perplexity: 11.7259\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 61/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.6459\n",
      "[quantity] Batch 51/300 | Loss: 1.5713\n",
      "[shape] Batch 101/300 | Loss: 1.5426\n",
      "[color] Batch 151/300 | Loss: 1.6001\n",
      "[quantity] Batch 201/300 | Loss: 1.5419\n",
      "[shape] Batch 251/300 | Loss: 1.7528\n",
      "[quantity] Batch 300/300 | Loss: 1.4873\n",
      "[Epoch 61] Train Loss: 1.5606 | Acc: 0.6127 | Perplexity: 4.7619\n",
      "[Epoch 61] Val Loss: 2.5201 | Acc: 0.2528 | Perplexity: 12.4302\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 62/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.4996\n",
      "[quantity] Batch 51/300 | Loss: 1.4409\n",
      "[shape] Batch 101/300 | Loss: 1.4948\n",
      "[color] Batch 151/300 | Loss: 1.3958\n",
      "[quantity] Batch 201/300 | Loss: 1.5874\n",
      "[shape] Batch 251/300 | Loss: 1.5436\n",
      "[quantity] Batch 300/300 | Loss: 1.6244\n",
      "[Epoch 62] Train Loss: 1.5463 | Acc: 0.6226 | Perplexity: 4.6940\n",
      "[Epoch 62] Val Loss: 2.5706 | Acc: 0.2519 | Perplexity: 13.0732\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 63/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.5570\n",
      "[quantity] Batch 51/300 | Loss: 1.4948\n",
      "[shape] Batch 101/300 | Loss: 1.4937\n",
      "[color] Batch 151/300 | Loss: 1.4900\n",
      "[quantity] Batch 201/300 | Loss: 1.5480\n",
      "[shape] Batch 251/300 | Loss: 1.4978\n",
      "[quantity] Batch 300/300 | Loss: 1.7185\n",
      "[Epoch 63] Train Loss: 1.5415 | Acc: 0.6257 | Perplexity: 4.6716\n",
      "[Epoch 63] Val Loss: 2.5289 | Acc: 0.2586 | Perplexity: 12.5393\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 64/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.6430\n",
      "[quantity] Batch 51/300 | Loss: 1.4712\n",
      "[shape] Batch 101/300 | Loss: 1.4735\n",
      "[color] Batch 151/300 | Loss: 1.3798\n",
      "[quantity] Batch 201/300 | Loss: 1.5334\n",
      "[shape] Batch 251/300 | Loss: 1.4918\n",
      "[quantity] Batch 300/300 | Loss: 1.8398\n",
      "[Epoch 64] Train Loss: 1.5283 | Acc: 0.6305 | Perplexity: 4.6101\n",
      "[Epoch 64] Val Loss: 2.5788 | Acc: 0.2577 | Perplexity: 13.1815\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 65/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.4589\n",
      "[quantity] Batch 51/300 | Loss: 1.5156\n",
      "[shape] Batch 101/300 | Loss: 1.4281\n",
      "[color] Batch 151/300 | Loss: 1.5007\n",
      "[quantity] Batch 201/300 | Loss: 1.6630\n",
      "[shape] Batch 251/300 | Loss: 1.5814\n",
      "[quantity] Batch 300/300 | Loss: 1.4537\n",
      "[Epoch 65] Train Loss: 1.5121 | Acc: 0.6375 | Perplexity: 4.5363\n",
      "[Epoch 65] Val Loss: 2.5899 | Acc: 0.2555 | Perplexity: 13.3278\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 66/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.4624\n",
      "[quantity] Batch 51/300 | Loss: 1.4321\n",
      "[shape] Batch 101/300 | Loss: 1.3549\n",
      "[color] Batch 151/300 | Loss: 1.5347\n",
      "[quantity] Batch 201/300 | Loss: 1.5710\n",
      "[shape] Batch 251/300 | Loss: 1.6562\n",
      "[quantity] Batch 300/300 | Loss: 1.7455\n",
      "[Epoch 66] Train Loss: 1.5103 | Acc: 0.6409 | Perplexity: 4.5280\n",
      "[Epoch 66] Val Loss: 2.6036 | Acc: 0.2619 | Perplexity: 13.5129\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 67/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.5251\n",
      "[quantity] Batch 51/300 | Loss: 1.4528\n",
      "[shape] Batch 101/300 | Loss: 1.5847\n",
      "[color] Batch 151/300 | Loss: 1.5731\n",
      "[quantity] Batch 201/300 | Loss: 1.4741\n",
      "[shape] Batch 251/300 | Loss: 1.4365\n",
      "[quantity] Batch 300/300 | Loss: 1.5784\n",
      "[Epoch 67] Train Loss: 1.4935 | Acc: 0.6483 | Perplexity: 4.4525\n",
      "[Epoch 67] Val Loss: 2.6025 | Acc: 0.2720 | Perplexity: 13.4978\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 68/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.4800\n",
      "[quantity] Batch 51/300 | Loss: 1.5146\n",
      "[shape] Batch 101/300 | Loss: 1.4132\n",
      "[color] Batch 151/300 | Loss: 1.5088\n",
      "[quantity] Batch 201/300 | Loss: 1.4489\n",
      "[shape] Batch 251/300 | Loss: 1.4661\n",
      "[quantity] Batch 300/300 | Loss: 1.3908\n",
      "[Epoch 68] Train Loss: 1.4802 | Acc: 0.6566 | Perplexity: 4.3940\n",
      "[Epoch 68] Val Loss: 2.6564 | Acc: 0.2585 | Perplexity: 14.2450\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 69/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.4140\n",
      "[quantity] Batch 51/300 | Loss: 1.3747\n",
      "[shape] Batch 101/300 | Loss: 1.5738\n",
      "[color] Batch 151/300 | Loss: 1.5746\n",
      "[quantity] Batch 201/300 | Loss: 1.5047\n",
      "[shape] Batch 251/300 | Loss: 1.5810\n",
      "[quantity] Batch 300/300 | Loss: 1.6211\n",
      "[Epoch 69] Train Loss: 1.4697 | Acc: 0.6604 | Perplexity: 4.3479\n",
      "[Epoch 69] Val Loss: 2.6186 | Acc: 0.2594 | Perplexity: 13.7162\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 70/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.4413\n",
      "[quantity] Batch 51/300 | Loss: 1.5069\n",
      "[shape] Batch 101/300 | Loss: 1.2359\n",
      "[color] Batch 151/300 | Loss: 1.4091\n",
      "[quantity] Batch 201/300 | Loss: 1.2834\n",
      "[shape] Batch 251/300 | Loss: 1.4747\n",
      "[quantity] Batch 300/300 | Loss: 1.5905\n",
      "[Epoch 70] Train Loss: 1.4700 | Acc: 0.6618 | Perplexity: 4.3494\n",
      "[Epoch 70] Val Loss: 2.6734 | Acc: 0.2629 | Perplexity: 14.4894\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 71/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.5232\n",
      "[quantity] Batch 51/300 | Loss: 1.3002\n",
      "[shape] Batch 101/300 | Loss: 1.4181\n",
      "[color] Batch 151/300 | Loss: 1.3642\n",
      "[quantity] Batch 201/300 | Loss: 1.5236\n",
      "[shape] Batch 251/300 | Loss: 1.4480\n",
      "[quantity] Batch 300/300 | Loss: 1.5233\n",
      "[Epoch 71] Train Loss: 1.4508 | Acc: 0.6724 | Perplexity: 4.2664\n",
      "[Epoch 71] Val Loss: 2.7122 | Acc: 0.2682 | Perplexity: 15.0617\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 72/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.4708\n",
      "[quantity] Batch 51/300 | Loss: 1.3813\n",
      "[shape] Batch 101/300 | Loss: 1.4253\n",
      "[color] Batch 151/300 | Loss: 1.3964\n",
      "[quantity] Batch 201/300 | Loss: 1.2911\n",
      "[shape] Batch 251/300 | Loss: 1.2625\n",
      "[quantity] Batch 300/300 | Loss: 1.3938\n",
      "[Epoch 72] Train Loss: 1.4273 | Acc: 0.6847 | Perplexity: 4.1674\n",
      "[Epoch 72] Val Loss: 2.7441 | Acc: 0.2649 | Perplexity: 15.5505\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 73/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.4874\n",
      "[quantity] Batch 51/300 | Loss: 1.3309\n",
      "[shape] Batch 101/300 | Loss: 1.3293\n",
      "[color] Batch 151/300 | Loss: 1.3460\n",
      "[quantity] Batch 201/300 | Loss: 1.5065\n",
      "[shape] Batch 251/300 | Loss: 1.4186\n",
      "[quantity] Batch 300/300 | Loss: 1.4006\n",
      "[Epoch 73] Train Loss: 1.4249 | Acc: 0.6894 | Perplexity: 4.1576\n",
      "[Epoch 73] Val Loss: 2.7113 | Acc: 0.2702 | Perplexity: 15.0494\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 74/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.3732\n",
      "[quantity] Batch 51/300 | Loss: 1.2764\n",
      "[shape] Batch 101/300 | Loss: 1.4817\n",
      "[color] Batch 151/300 | Loss: 1.4255\n",
      "[quantity] Batch 201/300 | Loss: 1.6157\n",
      "[shape] Batch 251/300 | Loss: 1.3192\n",
      "[quantity] Batch 300/300 | Loss: 1.5524\n",
      "[Epoch 74] Train Loss: 1.4118 | Acc: 0.6896 | Perplexity: 4.1033\n",
      "[Epoch 74] Val Loss: 2.7083 | Acc: 0.2735 | Perplexity: 15.0033\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 75/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.5088\n",
      "[quantity] Batch 51/300 | Loss: 1.5673\n",
      "[shape] Batch 101/300 | Loss: 1.2816\n",
      "[color] Batch 151/300 | Loss: 1.3311\n",
      "[quantity] Batch 201/300 | Loss: 1.4264\n",
      "[shape] Batch 251/300 | Loss: 1.4461\n",
      "[quantity] Batch 300/300 | Loss: 1.3284\n",
      "[Epoch 75] Train Loss: 1.4046 | Acc: 0.6997 | Perplexity: 4.0739\n",
      "[Epoch 75] Val Loss: 2.7075 | Acc: 0.2685 | Perplexity: 14.9922\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 76/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.3625\n",
      "[quantity] Batch 51/300 | Loss: 1.3358\n",
      "[shape] Batch 101/300 | Loss: 1.4753\n",
      "[color] Batch 151/300 | Loss: 1.4309\n",
      "[quantity] Batch 201/300 | Loss: 1.5012\n",
      "[shape] Batch 251/300 | Loss: 1.4397\n",
      "[quantity] Batch 300/300 | Loss: 1.2003\n",
      "[Epoch 76] Train Loss: 1.3949 | Acc: 0.7046 | Perplexity: 4.0347\n",
      "[Epoch 76] Val Loss: 2.7223 | Acc: 0.2709 | Perplexity: 15.2150\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 77/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.2038\n",
      "[quantity] Batch 51/300 | Loss: 1.4117\n",
      "[shape] Batch 101/300 | Loss: 1.3394\n",
      "[color] Batch 151/300 | Loss: 1.4955\n",
      "[quantity] Batch 201/300 | Loss: 1.5114\n",
      "[shape] Batch 251/300 | Loss: 1.4217\n",
      "[quantity] Batch 300/300 | Loss: 1.2512\n",
      "[Epoch 77] Train Loss: 1.3841 | Acc: 0.7080 | Perplexity: 3.9912\n",
      "[Epoch 77] Val Loss: 2.6842 | Acc: 0.2792 | Perplexity: 14.6470\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 78/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.3632\n",
      "[quantity] Batch 51/300 | Loss: 1.3700\n",
      "[shape] Batch 101/300 | Loss: 1.3971\n",
      "[color] Batch 151/300 | Loss: 1.3233\n",
      "[quantity] Batch 201/300 | Loss: 1.3758\n",
      "[shape] Batch 251/300 | Loss: 1.3922\n",
      "[quantity] Batch 300/300 | Loss: 1.4050\n",
      "[Epoch 78] Train Loss: 1.3648 | Acc: 0.7150 | Perplexity: 3.9147\n",
      "[Epoch 78] Val Loss: 2.7216 | Acc: 0.2778 | Perplexity: 15.2046\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 79/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.2886\n",
      "[quantity] Batch 51/300 | Loss: 1.3187\n",
      "[shape] Batch 101/300 | Loss: 1.1787\n",
      "[color] Batch 151/300 | Loss: 1.3789\n",
      "[quantity] Batch 201/300 | Loss: 1.5696\n",
      "[shape] Batch 251/300 | Loss: 1.3677\n",
      "[quantity] Batch 300/300 | Loss: 1.4423\n",
      "[Epoch 79] Train Loss: 1.3612 | Acc: 0.7193 | Perplexity: 3.9009\n",
      "[Epoch 79] Val Loss: 2.7487 | Acc: 0.2768 | Perplexity: 15.6229\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 80/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.2501\n",
      "[quantity] Batch 51/300 | Loss: 1.2383\n",
      "[shape] Batch 101/300 | Loss: 1.4763\n",
      "[color] Batch 151/300 | Loss: 1.3314\n",
      "[quantity] Batch 201/300 | Loss: 1.3757\n",
      "[shape] Batch 251/300 | Loss: 1.3495\n",
      "[quantity] Batch 300/300 | Loss: 1.4657\n",
      "[Epoch 80] Train Loss: 1.3525 | Acc: 0.7215 | Perplexity: 3.8672\n",
      "[Epoch 80] Val Loss: 2.7633 | Acc: 0.2834 | Perplexity: 15.8520\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 81/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.1782\n",
      "[quantity] Batch 51/300 | Loss: 1.3343\n",
      "[shape] Batch 101/300 | Loss: 1.3344\n",
      "[color] Batch 151/300 | Loss: 1.4038\n",
      "[quantity] Batch 201/300 | Loss: 1.2897\n",
      "[shape] Batch 251/300 | Loss: 1.2869\n",
      "[quantity] Batch 300/300 | Loss: 1.5190\n",
      "[Epoch 81] Train Loss: 1.3459 | Acc: 0.7243 | Perplexity: 3.8418\n",
      "[Epoch 81] Val Loss: 2.7660 | Acc: 0.2782 | Perplexity: 15.8945\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 82/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.2524\n",
      "[quantity] Batch 51/300 | Loss: 1.4240\n",
      "[shape] Batch 101/300 | Loss: 1.4389\n",
      "[color] Batch 151/300 | Loss: 1.2745\n",
      "[quantity] Batch 201/300 | Loss: 1.3740\n",
      "[shape] Batch 251/300 | Loss: 1.3238\n",
      "[quantity] Batch 300/300 | Loss: 1.3551\n",
      "[Epoch 82] Train Loss: 1.3332 | Acc: 0.7344 | Perplexity: 3.7933\n",
      "[Epoch 82] Val Loss: 2.7927 | Acc: 0.2831 | Perplexity: 16.3253\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 83/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.2855\n",
      "[quantity] Batch 51/300 | Loss: 1.3070\n",
      "[shape] Batch 101/300 | Loss: 1.2805\n",
      "[color] Batch 151/300 | Loss: 1.3851\n",
      "[quantity] Batch 201/300 | Loss: 1.2756\n",
      "[shape] Batch 251/300 | Loss: 1.3629\n",
      "[quantity] Batch 300/300 | Loss: 1.1700\n",
      "[Epoch 83] Train Loss: 1.3280 | Acc: 0.7343 | Perplexity: 3.7735\n",
      "[Epoch 83] Val Loss: 2.7747 | Acc: 0.2841 | Perplexity: 16.0344\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 84/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.2707\n",
      "[quantity] Batch 51/300 | Loss: 1.2020\n",
      "[shape] Batch 101/300 | Loss: 1.3338\n",
      "[color] Batch 151/300 | Loss: 1.1492\n",
      "[quantity] Batch 201/300 | Loss: 1.3012\n",
      "[shape] Batch 251/300 | Loss: 1.3768\n",
      "[quantity] Batch 300/300 | Loss: 1.2409\n",
      "[Epoch 84] Train Loss: 1.3063 | Acc: 0.7455 | Perplexity: 3.6926\n",
      "[Epoch 84] Val Loss: 2.8023 | Acc: 0.2880 | Perplexity: 16.4826\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 85/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.1707\n",
      "[quantity] Batch 51/300 | Loss: 1.3065\n",
      "[shape] Batch 101/300 | Loss: 1.1321\n",
      "[color] Batch 151/300 | Loss: 1.3381\n",
      "[quantity] Batch 201/300 | Loss: 1.4458\n",
      "[shape] Batch 251/300 | Loss: 1.3966\n",
      "[quantity] Batch 300/300 | Loss: 1.1670\n",
      "[Epoch 85] Train Loss: 1.3024 | Acc: 0.7485 | Perplexity: 3.6782\n",
      "[Epoch 85] Val Loss: 2.8712 | Acc: 0.2913 | Perplexity: 17.6584\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 86/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.3393\n",
      "[quantity] Batch 51/300 | Loss: 1.3113\n",
      "[shape] Batch 101/300 | Loss: 1.2318\n",
      "[color] Batch 151/300 | Loss: 1.2150\n",
      "[quantity] Batch 201/300 | Loss: 1.5484\n",
      "[shape] Batch 251/300 | Loss: 1.3487\n",
      "[quantity] Batch 300/300 | Loss: 1.2461\n",
      "[Epoch 86] Train Loss: 1.2911 | Acc: 0.7526 | Perplexity: 3.6367\n",
      "[Epoch 86] Val Loss: 2.9191 | Acc: 0.2903 | Perplexity: 18.5243\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 87/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.2687\n",
      "[quantity] Batch 51/300 | Loss: 1.1731\n",
      "[shape] Batch 101/300 | Loss: 1.2838\n",
      "[color] Batch 151/300 | Loss: 1.1489\n",
      "[quantity] Batch 201/300 | Loss: 1.2930\n",
      "[shape] Batch 251/300 | Loss: 1.1657\n",
      "[quantity] Batch 300/300 | Loss: 1.2115\n",
      "[Epoch 87] Train Loss: 1.2812 | Acc: 0.7611 | Perplexity: 3.6008\n",
      "[Epoch 87] Val Loss: 2.8170 | Acc: 0.2930 | Perplexity: 16.7274\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 88/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.4389\n",
      "[quantity] Batch 51/300 | Loss: 1.2475\n",
      "[shape] Batch 101/300 | Loss: 1.2684\n",
      "[color] Batch 151/300 | Loss: 1.4365\n",
      "[quantity] Batch 201/300 | Loss: 1.1966\n",
      "[shape] Batch 251/300 | Loss: 1.1836\n",
      "[quantity] Batch 300/300 | Loss: 1.2019\n",
      "[Epoch 88] Train Loss: 1.2765 | Acc: 0.7627 | Perplexity: 3.5841\n",
      "[Epoch 88] Val Loss: 2.8092 | Acc: 0.2883 | Perplexity: 16.5964\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 89/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.2215\n",
      "[quantity] Batch 51/300 | Loss: 1.2536\n",
      "[shape] Batch 101/300 | Loss: 1.1614\n",
      "[color] Batch 151/300 | Loss: 1.2312\n",
      "[quantity] Batch 201/300 | Loss: 1.3514\n",
      "[shape] Batch 251/300 | Loss: 1.2907\n",
      "[quantity] Batch 300/300 | Loss: 1.2319\n",
      "[Epoch 89] Train Loss: 1.2707 | Acc: 0.7684 | Perplexity: 3.5634\n",
      "[Epoch 89] Val Loss: 2.8730 | Acc: 0.2963 | Perplexity: 17.6901\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 90/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.2453\n",
      "[quantity] Batch 51/300 | Loss: 1.1569\n",
      "[shape] Batch 101/300 | Loss: 1.2641\n",
      "[color] Batch 151/300 | Loss: 1.1796\n",
      "[quantity] Batch 201/300 | Loss: 1.2831\n",
      "[shape] Batch 251/300 | Loss: 1.3487\n",
      "[quantity] Batch 300/300 | Loss: 1.4735\n",
      "[Epoch 90] Train Loss: 1.2536 | Acc: 0.7731 | Perplexity: 3.5029\n",
      "[Epoch 90] Val Loss: 2.8776 | Acc: 0.2974 | Perplexity: 17.7719\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 91/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.2144\n",
      "[quantity] Batch 51/300 | Loss: 1.1626\n",
      "[shape] Batch 101/300 | Loss: 1.3996\n",
      "[color] Batch 151/300 | Loss: 1.2188\n",
      "[quantity] Batch 201/300 | Loss: 1.1756\n",
      "[shape] Batch 251/300 | Loss: 1.2179\n",
      "[quantity] Batch 300/300 | Loss: 1.2165\n",
      "[Epoch 91] Train Loss: 1.2526 | Acc: 0.7695 | Perplexity: 3.4996\n",
      "[Epoch 91] Val Loss: 2.9460 | Acc: 0.2988 | Perplexity: 19.0295\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 92/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.1746\n",
      "[quantity] Batch 51/300 | Loss: 1.4143\n",
      "[shape] Batch 101/300 | Loss: 1.3951\n",
      "[color] Batch 151/300 | Loss: 1.2173\n",
      "[quantity] Batch 201/300 | Loss: 1.1964\n",
      "[shape] Batch 251/300 | Loss: 1.2751\n",
      "[quantity] Batch 300/300 | Loss: 1.3306\n",
      "[Epoch 92] Train Loss: 1.2431 | Acc: 0.7806 | Perplexity: 3.4664\n",
      "[Epoch 92] Val Loss: 2.8863 | Acc: 0.2944 | Perplexity: 17.9272\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 93/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.1853\n",
      "[quantity] Batch 51/300 | Loss: 1.2268\n",
      "[shape] Batch 101/300 | Loss: 1.2958\n",
      "[color] Batch 151/300 | Loss: 1.3125\n",
      "[quantity] Batch 201/300 | Loss: 1.3899\n",
      "[shape] Batch 251/300 | Loss: 1.1228\n",
      "[quantity] Batch 300/300 | Loss: 1.2544\n",
      "[Epoch 93] Train Loss: 1.2359 | Acc: 0.7812 | Perplexity: 3.4416\n",
      "[Epoch 93] Val Loss: 2.8980 | Acc: 0.2914 | Perplexity: 18.1387\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 94/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.0697\n",
      "[quantity] Batch 51/300 | Loss: 1.1988\n",
      "[shape] Batch 101/300 | Loss: 1.1061\n",
      "[color] Batch 151/300 | Loss: 1.3208\n",
      "[quantity] Batch 201/300 | Loss: 1.1026\n",
      "[shape] Batch 251/300 | Loss: 1.2426\n",
      "[quantity] Batch 300/300 | Loss: 1.2692\n",
      "[Epoch 94] Train Loss: 1.2224 | Acc: 0.7850 | Perplexity: 3.3952\n",
      "[Epoch 94] Val Loss: 2.9053 | Acc: 0.3012 | Perplexity: 18.2716\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 95/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.1308\n",
      "[quantity] Batch 51/300 | Loss: 1.2910\n",
      "[shape] Batch 101/300 | Loss: 1.0806\n",
      "[color] Batch 151/300 | Loss: 1.1781\n",
      "[quantity] Batch 201/300 | Loss: 1.2535\n",
      "[shape] Batch 251/300 | Loss: 1.2057\n",
      "[quantity] Batch 300/300 | Loss: 1.0854\n",
      "[Epoch 95] Train Loss: 1.2146 | Acc: 0.7900 | Perplexity: 3.3689\n",
      "[Epoch 95] Val Loss: 2.9285 | Acc: 0.2994 | Perplexity: 18.6996\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 96/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.1716\n",
      "[quantity] Batch 51/300 | Loss: 1.0818\n",
      "[shape] Batch 101/300 | Loss: 1.1131\n",
      "[color] Batch 151/300 | Loss: 1.1464\n",
      "[quantity] Batch 201/300 | Loss: 1.2357\n",
      "[shape] Batch 251/300 | Loss: 1.1692\n",
      "[quantity] Batch 300/300 | Loss: 1.2063\n",
      "[Epoch 96] Train Loss: 1.2130 | Acc: 0.7935 | Perplexity: 3.3637\n",
      "[Epoch 96] Val Loss: 2.9075 | Acc: 0.2960 | Perplexity: 18.3115\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 97/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.2209\n",
      "[quantity] Batch 51/300 | Loss: 1.3440\n",
      "[shape] Batch 101/300 | Loss: 1.0813\n",
      "[color] Batch 151/300 | Loss: 1.1880\n",
      "[quantity] Batch 201/300 | Loss: 1.1253\n",
      "[shape] Batch 251/300 | Loss: 1.2435\n",
      "[quantity] Batch 300/300 | Loss: 1.2817\n",
      "[Epoch 97] Train Loss: 1.1997 | Acc: 0.7967 | Perplexity: 3.3191\n",
      "[Epoch 97] Val Loss: 2.8569 | Acc: 0.3018 | Perplexity: 17.4076\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 98/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.0054\n",
      "[quantity] Batch 51/300 | Loss: 1.1146\n",
      "[shape] Batch 101/300 | Loss: 1.3035\n",
      "[color] Batch 151/300 | Loss: 1.2631\n",
      "[quantity] Batch 201/300 | Loss: 1.2834\n",
      "[shape] Batch 251/300 | Loss: 1.2240\n",
      "[quantity] Batch 300/300 | Loss: 1.0960\n",
      "[Epoch 98] Train Loss: 1.1908 | Acc: 0.7985 | Perplexity: 3.2897\n",
      "[Epoch 98] Val Loss: 2.9684 | Acc: 0.3062 | Perplexity: 19.4601\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 99/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.0776\n",
      "[quantity] Batch 51/300 | Loss: 1.1874\n",
      "[shape] Batch 101/300 | Loss: 1.2068\n",
      "[color] Batch 151/300 | Loss: 1.1138\n",
      "[quantity] Batch 201/300 | Loss: 1.3266\n",
      "[shape] Batch 251/300 | Loss: 1.1291\n",
      "[quantity] Batch 300/300 | Loss: 1.2175\n",
      "[Epoch 99] Train Loss: 1.1840 | Acc: 0.8092 | Perplexity: 3.2673\n",
      "[Epoch 99] Val Loss: 2.9964 | Acc: 0.3063 | Perplexity: 20.0131\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 100/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/300 | Loss: 1.0445\n",
      "[quantity] Batch 51/300 | Loss: 1.2113\n",
      "[shape] Batch 101/300 | Loss: 1.3052\n",
      "[color] Batch 151/300 | Loss: 1.3081\n",
      "[quantity] Batch 201/300 | Loss: 1.1978\n",
      "[shape] Batch 251/300 | Loss: 1.1955\n",
      "[quantity] Batch 300/300 | Loss: 1.2663\n",
      "[Epoch 100] Train Loss: 1.1823 | Acc: 0.8079 | Perplexity: 3.2620\n",
      "[Epoch 100] Val Loss: 2.9859 | Acc: 0.3067 | Perplexity: 19.8052\n",
      "------------------------------------------------------------\n",
      "\n",
      " Training complete (Round-Robin Mode)\n"
     ]
    }
   ],
   "source": [
    "criterion =  nn.CrossEntropyLoss(label_smoothing=LABEL_SMOOTHING)\n",
    "optimizer = optim.AdamW(transformer.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=WARMUP_STEPS)  \n",
    "\n",
    "results = train_model_round_robin(\n",
    "    train_loaders, validation_loader, transformer, criterion, \n",
    "    optimizer, max_epochs=MAX_EPOCHS, device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917a1fa4",
   "metadata": {},
   "source": [
    "### 3. Test Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d144441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.9588 | Test Acc: 0.3195 | Test Perplexity: 19.2752\n"
     ]
    }
   ],
   "source": [
    "results = test_model(test_loader, transformer, criterion, device)\n",
    "print(f\"Test Loss: {results[\"test_loss\"]:.4f} | Test Acc: {results[\"test_acc\"]:.4f} | Test Perplexity: {results[\"test_perplexity\"]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28dac7b",
   "metadata": {},
   "source": [
    "## Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3c7ecc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.wcst import WCST\n",
    "wcst = WCST(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b590910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inference(model: Transformer, source_sequence, start_tokens):\n",
    "    model.eval()\n",
    "    generated = start_tokens\n",
    "    \n",
    "    with T.no_grad():\n",
    "        logits = model(source_sequence, generated)\n",
    "    \n",
    "    # Greedy Selection\n",
    "    next_token = T.argmax(logits[:, -1, :], dim=-1, keepdim=True)\n",
    "\n",
    "    generated = T.cat([generated, next_token], dim=1)\n",
    "\n",
    "    return generated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f458678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Actual Trials\n",
      "[array(['green', 'cross', '2'], dtype='<U6'), array(['green', 'star', '3'], dtype='<U6'), array(['yellow', 'star', '4'], dtype='<U6'), array(['yellow', 'circle', '1'], dtype='<U6'), array(['blue', 'cross', '1'], dtype='<U6'), 'SEP', 'C4', 'EOS', array(['red', 'square', '1'], dtype='<U6'), 'SEP', 'C4']\n",
      "[array(['blue', 'cross', '4'], dtype='<U6'), array(['blue', 'cross', '3'], dtype='<U6'), array(['green', 'square', '2'], dtype='<U6'), array(['yellow', 'square', '1'], dtype='<U6'), array(['yellow', 'star', '3'], dtype='<U6'), 'SEP', 'C2', 'EOS', array(['blue', 'circle', '4'], dtype='<U6'), 'SEP', 'C1']\n",
      "[array(['green', 'cross', '4'], dtype='<U6'), array(['blue', 'square', '2'], dtype='<U6'), array(['green', 'star', '1'], dtype='<U6'), array(['red', 'circle', '3'], dtype='<U6'), array(['blue', 'circle', '2'], dtype='<U6'), 'SEP', 'C2', 'EOS', array(['green', 'cross', '1'], dtype='<U6'), 'SEP', 'C3']\n",
      "[array(['yellow', 'square', '3'], dtype='<U6'), array(['yellow', 'cross', '4'], dtype='<U6'), array(['yellow', 'star', '2'], dtype='<U6'), array(['yellow', 'cross', '1'], dtype='<U6'), array(['yellow', 'star', '3'], dtype='<U6'), 'SEP', 'C1', 'EOS', array(['green', 'circle', '1'], dtype='<U6'), 'SEP', 'C4']\n",
      "[array(['red', 'cross', '2'], dtype='<U6'), array(['red', 'cross', '3'], dtype='<U6'), array(['red', 'star', '4'], dtype='<U6'), array(['yellow', 'cross', '1'], dtype='<U6'), array(['red', 'square', '4'], dtype='<U6'), 'SEP', 'C3', 'EOS', array(['green', 'circle', '3'], dtype='<U6'), 'SEP', 'C2']\n",
      "[array(['yellow', 'circle', '1'], dtype='<U6'), array(['blue', 'cross', '3'], dtype='<U6'), array(['green', 'circle', '4'], dtype='<U6'), array(['red', 'circle', '2'], dtype='<U6'), array(['red', 'square', '1'], dtype='<U6'), 'SEP', 'C1', 'EOS', array(['red', 'cross', '3'], dtype='<U6'), 'SEP', 'C2']\n",
      "[array(['yellow', 'square', '2'], dtype='<U6'), array(['green', 'square', '1'], dtype='<U6'), array(['red', 'square', '4'], dtype='<U6'), array(['blue', 'circle', '3'], dtype='<U6'), array(['blue', 'circle', '1'], dtype='<U6'), 'SEP', 'C2', 'EOS', array(['green', 'square', '3'], dtype='<U6'), 'SEP', 'C4']\n",
      "[array(['blue', 'square', '1'], dtype='<U6'), array(['yellow', 'circle', '2'], dtype='<U6'), array(['green', 'circle', '4'], dtype='<U6'), array(['red', 'square', '3'], dtype='<U6'), array(['blue', 'star', '4'], dtype='<U6'), 'SEP', 'C3', 'EOS', array(['red', 'circle', '1'], dtype='<U6'), 'SEP', 'C1']\n",
      "[array(['yellow', 'cross', '1'], dtype='<U6'), array(['red', 'cross', '4'], dtype='<U6'), array(['red', 'square', '2'], dtype='<U6'), array(['red', 'circle', '3'], dtype='<U6'), array(['blue', 'cross', '2'], dtype='<U6'), 'SEP', 'C3', 'EOS', array(['red', 'square', '3'], dtype='<U6'), 'SEP', 'C4']\n",
      "[array(['red', 'star', '3'], dtype='<U6'), array(['red', 'star', '1'], dtype='<U6'), array(['green', 'cross', '2'], dtype='<U6'), array(['yellow', 'square', '4'], dtype='<U6'), array(['blue', 'star', '1'], dtype='<U6'), 'SEP', 'C2', 'EOS', array(['yellow', 'square', '1'], dtype='<U6'), 'SEP', 'C2']\n",
      "Feature for Classification:  2 \n",
      "\n",
      "# Predicted Trials\n",
      "[array(['green', 'cross', '2'], dtype='<U6'), array(['green', 'star', '3'], dtype='<U6'), array(['yellow', 'star', '4'], dtype='<U6'), array(['yellow', 'circle', '1'], dtype='<U6'), array(['blue', 'cross', '1'], dtype='<U6'), 'SEP', 'C4', 'EOS', array(['red', 'square', '1'], dtype='<U6'), 'SEP', array(['red', 'square', '1'], dtype='<U6'), 'SEP', 'C4']\n",
      "[array(['blue', 'cross', '4'], dtype='<U6'), array(['blue', 'cross', '3'], dtype='<U6'), array(['green', 'square', '2'], dtype='<U6'), array(['yellow', 'square', '1'], dtype='<U6'), array(['yellow', 'star', '3'], dtype='<U6'), 'SEP', 'C2', 'EOS', array(['blue', 'circle', '4'], dtype='<U6'), 'SEP', array(['blue', 'circle', '4'], dtype='<U6'), 'SEP', 'C1']\n",
      "[array(['green', 'cross', '4'], dtype='<U6'), array(['blue', 'square', '2'], dtype='<U6'), array(['green', 'star', '1'], dtype='<U6'), array(['red', 'circle', '3'], dtype='<U6'), array(['blue', 'circle', '2'], dtype='<U6'), 'SEP', 'C2', 'EOS', array(['green', 'cross', '1'], dtype='<U6'), 'SEP', array(['green', 'cross', '1'], dtype='<U6'), 'SEP', 'C3']\n",
      "[array(['yellow', 'square', '3'], dtype='<U6'), array(['yellow', 'cross', '4'], dtype='<U6'), array(['yellow', 'star', '2'], dtype='<U6'), array(['yellow', 'cross', '1'], dtype='<U6'), array(['yellow', 'star', '3'], dtype='<U6'), 'SEP', 'C1', 'EOS', array(['green', 'circle', '1'], dtype='<U6'), 'SEP', array(['green', 'circle', '1'], dtype='<U6'), 'SEP', 'C4']\n",
      "[array(['red', 'cross', '2'], dtype='<U6'), array(['red', 'cross', '3'], dtype='<U6'), array(['red', 'star', '4'], dtype='<U6'), array(['yellow', 'cross', '1'], dtype='<U6'), array(['red', 'square', '4'], dtype='<U6'), 'SEP', 'C3', 'EOS', array(['green', 'circle', '3'], dtype='<U6'), 'SEP', array(['green', 'circle', '3'], dtype='<U6'), 'SEP', 'C2']\n",
      "[array(['yellow', 'circle', '1'], dtype='<U6'), array(['blue', 'cross', '3'], dtype='<U6'), array(['green', 'circle', '4'], dtype='<U6'), array(['red', 'circle', '2'], dtype='<U6'), array(['red', 'square', '1'], dtype='<U6'), 'SEP', 'C1', 'EOS', array(['red', 'cross', '3'], dtype='<U6'), 'SEP', array(['red', 'cross', '3'], dtype='<U6'), 'SEP', 'C2']\n",
      "[array(['yellow', 'square', '2'], dtype='<U6'), array(['green', 'square', '1'], dtype='<U6'), array(['red', 'square', '4'], dtype='<U6'), array(['blue', 'circle', '3'], dtype='<U6'), array(['blue', 'circle', '1'], dtype='<U6'), 'SEP', 'C2', 'EOS', array(['green', 'square', '3'], dtype='<U6'), 'SEP', array(['green', 'square', '3'], dtype='<U6'), 'SEP', 'C4']\n",
      "[array(['blue', 'square', '1'], dtype='<U6'), array(['yellow', 'circle', '2'], dtype='<U6'), array(['green', 'circle', '4'], dtype='<U6'), array(['red', 'square', '3'], dtype='<U6'), array(['blue', 'star', '4'], dtype='<U6'), 'SEP', 'C3', 'EOS', array(['red', 'circle', '1'], dtype='<U6'), 'SEP', array(['red', 'circle', '1'], dtype='<U6'), 'SEP', 'C1']\n",
      "[array(['yellow', 'cross', '1'], dtype='<U6'), array(['red', 'cross', '4'], dtype='<U6'), array(['red', 'square', '2'], dtype='<U6'), array(['red', 'circle', '3'], dtype='<U6'), array(['blue', 'cross', '2'], dtype='<U6'), 'SEP', 'C3', 'EOS', array(['red', 'square', '3'], dtype='<U6'), 'SEP', array(['red', 'square', '3'], dtype='<U6'), 'SEP', 'C4']\n",
      "[array(['red', 'star', '3'], dtype='<U6'), array(['red', 'star', '1'], dtype='<U6'), array(['green', 'cross', '2'], dtype='<U6'), array(['yellow', 'square', '4'], dtype='<U6'), array(['blue', 'star', '1'], dtype='<U6'), 'SEP', 'C2', 'EOS', array(['yellow', 'square', '1'], dtype='<U6'), 'SEP', array(['yellow', 'square', '1'], dtype='<U6'), 'SEP', 'C2']\n",
      "Feature for Classification:  2 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoder_input, decoder_input, target = train_datasets[\"quantity\"][:10]\n",
    "encoder_input = encoder_input.to(device)\n",
    "decoder_input = decoder_input.to(device)\n",
    "target = target.to(device)\n",
    "\n",
    "prediction = model_inference(transformer, encoder_input, decoder_input)\n",
    "\n",
    "print(\"# Actual Trials\")\n",
    "test_batch = [np.asarray(item.cpu()) for item in [encoder_input, T.concatenate([decoder_input, target], dim=1)]]\n",
    "output = wcst.visualise_batch(test_batch)\n",
    "\n",
    "print(\"# Predicted Trials\")\n",
    "prediction_batch = [np.asarray(item.cpu()) for item in [encoder_input, T.concatenate([decoder_input, prediction], dim=1)]]\n",
    "output = wcst.visualise_batch(prediction_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f73bf2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
