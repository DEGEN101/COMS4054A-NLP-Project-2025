{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd555848",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch as T\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "389721a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "SEED = 42\n",
    "\n",
    "device = T.device('cuda' if T.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "\n",
    "T.manual_seed(SEED)\n",
    "\n",
    "if T.cuda.is_available():\n",
    "    T.cuda.manual_seed_all(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08945224",
   "metadata": {},
   "source": [
    "## Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25ea2ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from datasets.custom_dataset import CustomWCSTDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ba4f2c",
   "metadata": {},
   "source": [
    "### 1. Dataset Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e95fdcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "TOTAL_BATCHES = 500\n",
    "TRAIN_TEST_SPLIT_RATIO = 0.6\n",
    "VALIDATION_TEST_SPLIT_RATIO = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e47ffd5",
   "metadata": {},
   "source": [
    "### 2. Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "365835c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset Init] Fixed context: 0\n",
      "[Dataset Init] Fixed context: 1\n",
      "[Dataset Init] Fixed context: 2\n",
      "[Dataset Init] Context switched -> 1\n",
      "[Dataset Init] Context switched -> 0\n",
      "[Dataset Init] Context switched -> 2\n",
      "[Dataset Init] Context switched -> 0\n",
      "[Dataset Init] Context switched -> 2\n",
      "[Dataset Init] Context switched -> 0\n",
      "[Dataset Init] Context switched -> 1\n",
      "[Dataset Init] Context switched -> 0\n",
      "[Dataset Init] Context switched -> 2\n",
      "[Dataset Init] Context switched -> 1\n",
      "[Dataset Init] Context switched -> 2\n",
      "[Dataset Init] Context switched -> 0\n",
      "[Dataset Init] Context switched -> 2\n",
      "[Dataset Init] Context switched -> 0\n",
      "[Dataset Init] Context switched -> 2\n",
      "[Dataset Init] Context switched -> 1\n",
      "[Dataset Init] Context switched -> 0\n",
      "[Dataset Init] Context switched -> 2\n",
      "[Dataset Init] Context switched -> 1\n",
      "[Dataset Init] Context switched -> 0\n",
      "[Dataset Init] Context switched -> 1\n",
      "[Dataset Init] Context switched -> 2\n"
     ]
    }
   ],
   "source": [
    "train_size = int(TOTAL_BATCHES * TRAIN_TEST_SPLIT_RATIO)\n",
    "validation_size = int((TOTAL_BATCHES - train_size) * VALIDATION_TEST_SPLIT_RATIO)\n",
    "test_size = TOTAL_BATCHES - train_size - validation_size\n",
    "\n",
    "train_datasets = {\n",
    "    \"color\": CustomWCSTDataset(total_batches=train_size // 2, fixed_context=0, sample_batch_size=BATCH_SIZE, allow_switch=False),\n",
    "    \"shape\": CustomWCSTDataset(total_batches=train_size // 2, fixed_context=1, sample_batch_size=BATCH_SIZE, allow_switch=False),\n",
    "    \"quantity\": CustomWCSTDataset(total_batches=train_size // 2, fixed_context=2, sample_batch_size=BATCH_SIZE, allow_switch=False)\n",
    "}\n",
    "\n",
    "train_loaders = {\n",
    "    ctx: DataLoader(ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    for ctx, ds in train_datasets.items()\n",
    "}\n",
    "\n",
    "validation_dataset = CustomWCSTDataset(\n",
    "        total_batches=validation_size, sample_batch_size=BATCH_SIZE\n",
    "    )\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "test_dataset = CustomWCSTDataset(\n",
    "        total_batches=validation_size, sample_batch_size=BATCH_SIZE\n",
    "    )\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f524267",
   "metadata": {},
   "source": [
    "## Transformer Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3a8bc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.transformer import BaselineTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7739c3fe",
   "metadata": {},
   "source": [
    "### 1. Transformer Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "606c1008",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCABULARY_SIZE = 70        # 64 cards + 4 categories + SEP + EOS\n",
    "EMBEDDING_SIZE = 256        # larger embedding to capture card features\n",
    "N_ATTENTION_HEADS = 8       # more heads for better multi-feature attention\n",
    "N_BLOCKS = 6                # same depth as before\n",
    "MAX_SEQUENCE_LENGTH = 10    # longer max sequence to accommodate multiple past trials\n",
    "FF_DIMS = 1024               # larger feedforward layer for better representation\n",
    "DROPOUT_PROB = 0.2        # reduce dropout slightly to retain signal in small batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab9fc96",
   "metadata": {},
   "source": [
    "### 2. Transformer Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f25ee98",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = BaselineTransformer(\n",
    "    VOCABULARY_SIZE, EMBEDDING_SIZE, N_ATTENTION_HEADS,\n",
    "    N_BLOCKS, MAX_SEQUENCE_LENGTH, FF_DIMS, DROPOUT_PROB, device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7489333",
   "metadata": {},
   "source": [
    "## Training Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42f3ebdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import torch as T\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn, optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c5d23d",
   "metadata": {},
   "source": [
    "### 1. Train, Validate, Evaluate Model Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a8ee8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    train_loader: DataLoader,\n",
    "    validation_loader: DataLoader,\n",
    "    model: nn.Module,\n",
    "    criterion: nn.CrossEntropyLoss,\n",
    "    optimizer: optim.Optimizer,\n",
    "    max_epochs: int = 20,\n",
    "    device: str | T.device = \"cpu\",\n",
    "):\n",
    "    best_val_loss = np.inf\n",
    "\n",
    "    train_losses, train_accs, train_perplexities = [], [], []\n",
    "    val_losses, val_accs, val_perplexities = [], [], []\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{max_epochs}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "        # --- Training ---\n",
    "        model.train()\n",
    "        epoch_train_losses = []\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        for batch_idx, (input_seq, target) in enumerate(train_loader):\n",
    "            input_seq, target = input_seq.to(device), target.view(-1).to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            logits = model(input_seq)  # [batch, seq_len, vocab]\n",
    "            logits = logits[:, -1, :]  # only the final step prediction [batch, vocab]\n",
    "\n",
    "            loss = criterion(logits, target)\n",
    "\n",
    "            # Backprop\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accuracy\n",
    "            preds = logits.argmax(dim=1)\n",
    "            total_correct += (preds == target).sum().item()\n",
    "            total_samples += target.size(0)\n",
    "\n",
    "            epoch_train_losses.append(loss.item())\n",
    "\n",
    "            if batch_idx % 100 == 0 or batch_idx == len(train_loader) - 1:\n",
    "                print(f\"Train Batch {batch_idx+1}/{len(train_loader)} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "        train_loss = np.mean(epoch_train_losses)\n",
    "        train_acc = total_correct / total_samples\n",
    "        train_perplexity = np.exp(train_loss)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        train_perplexities.append(train_perplexity)\n",
    "\n",
    "        print(f\"[Epoch {epoch+1}] Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | Train Perplexity: {train_perplexity:.4f}\")\n",
    "\n",
    "        # --- Validation ---\n",
    "        model.eval()\n",
    "        val_batch_losses = []\n",
    "        val_correct = 0\n",
    "        val_samples = 0\n",
    "\n",
    "        with T.no_grad():\n",
    "            for input_seq, target in validation_loader:\n",
    "                input_seq, target = input_seq.to(device), target.view(-1).to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                logits = model(input_seq)  # [batch, seq_len, vocab]\n",
    "                logits = logits[:, -1, :]  # only the final step prediction [batch, vocab]\n",
    "\n",
    "                loss = criterion(logits, target)\n",
    "\n",
    "                preds = logits.argmax(dim=1)\n",
    "                val_correct += (preds == target).sum().item()\n",
    "                val_samples += target.size(0)\n",
    "\n",
    "                val_batch_losses.append(loss.item())\n",
    "\n",
    "        val_loss = np.mean(val_batch_losses)\n",
    "        val_acc = val_correct / val_samples\n",
    "        val_perplexity = np.exp(val_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_acc)\n",
    "        val_perplexities.append(val_perplexity)\n",
    "\n",
    "        print(f\"[Epoch {epoch+1}] Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | Val Perplexity: {val_perplexity:.4f}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "    print(\"\\nTraining complete\")\n",
    "\n",
    "    return {\n",
    "        \"train_losses\": train_losses,\n",
    "        \"train_accs\": train_accs,\n",
    "        \"train_perplexities\": train_perplexities,\n",
    "        \"val_losses\": val_losses,\n",
    "        \"val_accs\": val_accs,\n",
    "        \"val_perplexities\": val_perplexities,\n",
    "        \"best_val_loss\": best_val_loss,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6af9014f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_round_robin(\n",
    "    train_loaders: dict[str, DataLoader], validation_loader: DataLoader, model: nn.Module,\n",
    "    criterion: nn.CrossEntropyLoss, optimizer: optim.Optimizer, max_epochs: int = 20,\n",
    "    device: str | T.device = \"cpu\"\n",
    "):\n",
    "\n",
    "    history = {k: [] for k in [\n",
    "        \"train_losses\", \"train_accs\", \"train_perplexities\",\n",
    "        \"val_losses\", \"val_accs\", \"val_perplexities\"\n",
    "    ]}\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        print(f\"\\n[Epoch {epoch + 1}/{max_epochs}]\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "        model.train()\n",
    "        epoch_losses, total_correct, total_samples = [], 0, 0\n",
    "\n",
    "        # --- Build a round-robin iterator across all loaders ---\n",
    "        loaders_cycle = itertools.cycle(train_loaders.items())\n",
    "        active_iters = {ctx: iter(dl) for ctx, dl in train_loaders.items()}\n",
    "\n",
    "        # Find smallest loader length to roughly balance epoch size\n",
    "        min_len = min(len(dl) for dl in train_loaders.values())\n",
    "        total_batches = min_len * len(train_loaders)\n",
    "\n",
    "        for batch_idx in range(total_batches):\n",
    "            context, _ = next(loaders_cycle)\n",
    "            loader_iter = active_iters[context]\n",
    "\n",
    "            try:\n",
    "                input_seq, target = next(loader_iter)\n",
    "            except StopIteration:\n",
    "                # Restart exhausted iterator\n",
    "                active_iters[context] = iter(train_loaders[context])\n",
    "                input_seq, target = next(active_iters[context])\n",
    "\n",
    "            input_seq, target = input_seq.to(device), target.view(-1).to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            logits = model(input_seq)  # [batch, seq_len, vocab]\n",
    "            logits = logits[:, -1, :]  # predict final token only\n",
    "            loss = criterion(logits, target)\n",
    "\n",
    "            # Backprop\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Metrics\n",
    "            preds = logits.argmax(dim=1)\n",
    "            total_correct += (preds == target).sum().item()\n",
    "            total_samples += target.size(0)\n",
    "            epoch_losses.append(loss.item())\n",
    "\n",
    "            if batch_idx % 50 == 0 or batch_idx == total_batches - 1:\n",
    "                print(f\"[{context}] Batch {batch_idx+1}/{total_batches} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "        # --- Epoch stats ---\n",
    "        train_loss = np.mean(epoch_losses)\n",
    "        train_acc = total_correct / total_samples\n",
    "        train_perplexity = np.exp(train_loss)\n",
    "\n",
    "        print(f\"[Epoch {epoch+1}] Train Loss: {train_loss:.4f} | Acc: {train_acc:.4f} | Perplexity: {train_perplexity:.4f}\")\n",
    "\n",
    "        # --- Validation ---\n",
    "        model.eval()\n",
    "        val_losses, val_correct, val_samples = [], 0, 0\n",
    "\n",
    "        with T.no_grad():\n",
    "            for input_seq, target in validation_loader:\n",
    "                input_seq, target = input_seq.to(device), target.view(-1).to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                logits = model(input_seq)  # [batch, seq_len, vocab]\n",
    "\n",
    "                logits = logits[:, -1, :]\n",
    "                loss = criterion(logits, target)\n",
    "                \n",
    "                preds = logits.argmax(dim=1)\n",
    "                val_correct += (preds == target).sum().item()\n",
    "                val_samples += target.size(0)\n",
    "                val_losses.append(loss.item())\n",
    "\n",
    "        val_loss = np.mean(val_losses)\n",
    "        val_acc = val_correct / val_samples\n",
    "        val_perplexity = np.exp(val_loss)\n",
    "\n",
    "        print(f\"[Epoch {epoch+1}] Val Loss: {val_loss:.4f} | Acc: {val_acc:.4f} | Perplexity: {val_perplexity:.4f}\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "        # --- Logging ---\n",
    "        history[\"train_losses\"].append(train_loss)\n",
    "        history[\"train_accs\"].append(train_acc)\n",
    "        history[\"train_perplexities\"].append(train_perplexity)\n",
    "        history[\"val_losses\"].append(val_loss)\n",
    "        history[\"val_accs\"].append(val_acc)\n",
    "        history[\"val_perplexities\"].append(val_perplexity)\n",
    "\n",
    "    print(\"\\n Training complete (Round-Robin Mode)\")\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44864dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(test_loader: DataLoader, model: nn.Module, criterion: nn.CrossEntropyLoss, device: str | T.device = \"cpu\"):\n",
    "\n",
    "    model.eval()\n",
    "    test_batch_losses = []\n",
    "    test_correct = 0\n",
    "    test_tokens = 0\n",
    "\n",
    "    with T.no_grad():\n",
    "        for input_seq, target in test_loader:\n",
    "            input_seq, target = input_seq.to(device), target.view(-1).to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            logits = model(input_seq)  # [batch, seq_len, vocab]\n",
    "            logits = logits[:, -1, :]\n",
    "            loss = criterion(logits, target)\n",
    "\n",
    "            preds = logits.argmax(dim=1)\n",
    "            test_correct += (preds == target).sum().item()\n",
    "            test_tokens += target.size(0)\n",
    "\n",
    "            test_batch_losses.append(loss.item())\n",
    "\n",
    "    test_loss = np.mean(test_batch_losses)\n",
    "    test_acc = test_correct / test_tokens\n",
    "    test_perplexity = np.exp(test_loss)\n",
    "\n",
    "    return {\n",
    "        \"test_loss\": test_loss,\n",
    "        \"test_acc\": test_acc,\n",
    "        \"test_perplexity\": test_perplexity\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d863c67",
   "metadata": {},
   "source": [
    "### 2. Train Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37f53def",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 3e-4\n",
    "BATCH_SIZE = 64\n",
    "WEIGHT_DECAY = 1e-2\n",
    "WARMUP_STEPS = 400\n",
    "LABEL_SMOOTHING = 0.1\n",
    "MAX_EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5c8f99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Epoch 1/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 166.9722\n",
      "[quantity] Batch 51/450 | Loss: 5.6897\n",
      "[shape] Batch 101/450 | Loss: 4.6771\n",
      "[color] Batch 151/450 | Loss: 3.7778\n",
      "[quantity] Batch 201/450 | Loss: 3.0461\n",
      "[shape] Batch 251/450 | Loss: 3.7780\n",
      "[color] Batch 301/450 | Loss: 3.6043\n",
      "[quantity] Batch 351/450 | Loss: 2.9756\n",
      "[shape] Batch 401/450 | Loss: 3.3916\n",
      "[quantity] Batch 450/450 | Loss: 2.8597\n",
      "[Epoch 1] Train Loss: 4.6711 | Acc: 0.2398 | Perplexity: 106.8181\n",
      "[Epoch 1] Val Loss: 2.5603 | Acc: 0.2648 | Perplexity: 12.9402\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 2/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 3.2678\n",
      "[quantity] Batch 51/450 | Loss: 2.9988\n",
      "[shape] Batch 101/450 | Loss: 2.7506\n",
      "[color] Batch 151/450 | Loss: 2.9123\n",
      "[quantity] Batch 201/450 | Loss: 2.4650\n",
      "[shape] Batch 251/450 | Loss: 2.6952\n",
      "[color] Batch 301/450 | Loss: 2.6478\n",
      "[quantity] Batch 351/450 | Loss: 2.7567\n",
      "[shape] Batch 401/450 | Loss: 2.5144\n",
      "[quantity] Batch 450/450 | Loss: 2.4999\n",
      "[Epoch 2] Train Loss: 2.6902 | Acc: 0.2432 | Perplexity: 14.7342\n",
      "[Epoch 2] Val Loss: 2.3421 | Acc: 0.2453 | Perplexity: 10.4029\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 3/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 2.6124\n",
      "[quantity] Batch 51/450 | Loss: 2.4712\n",
      "[shape] Batch 101/450 | Loss: 2.9856\n",
      "[color] Batch 151/450 | Loss: 2.4983\n",
      "[quantity] Batch 201/450 | Loss: 2.8261\n",
      "[shape] Batch 251/450 | Loss: 2.3825\n",
      "[color] Batch 301/450 | Loss: 2.2959\n",
      "[quantity] Batch 351/450 | Loss: 2.4469\n",
      "[shape] Batch 401/450 | Loss: 2.5029\n",
      "[quantity] Batch 450/450 | Loss: 2.3823\n",
      "[Epoch 3] Train Loss: 2.4548 | Acc: 0.2467 | Perplexity: 11.6435\n",
      "[Epoch 3] Val Loss: 2.1535 | Acc: 0.2648 | Perplexity: 8.6147\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 4/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 2.2491\n",
      "[quantity] Batch 51/450 | Loss: 2.3963\n",
      "[shape] Batch 101/450 | Loss: 2.3750\n",
      "[color] Batch 151/450 | Loss: 2.2172\n",
      "[quantity] Batch 201/450 | Loss: 2.4971\n",
      "[shape] Batch 251/450 | Loss: 2.4806\n",
      "[color] Batch 301/450 | Loss: 2.5890\n",
      "[quantity] Batch 351/450 | Loss: 2.3005\n",
      "[shape] Batch 401/450 | Loss: 2.0627\n",
      "[quantity] Batch 450/450 | Loss: 2.3614\n",
      "[Epoch 4] Train Loss: 2.3623 | Acc: 0.2485 | Perplexity: 10.6153\n",
      "[Epoch 4] Val Loss: 2.0538 | Acc: 0.2430 | Perplexity: 7.7975\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 5/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 2.2581\n",
      "[quantity] Batch 51/450 | Loss: 2.3784\n",
      "[shape] Batch 101/450 | Loss: 2.2753\n",
      "[color] Batch 151/450 | Loss: 2.1491\n",
      "[quantity] Batch 201/450 | Loss: 2.2394\n",
      "[shape] Batch 251/450 | Loss: 2.2460\n",
      "[color] Batch 301/450 | Loss: 2.1304\n",
      "[quantity] Batch 351/450 | Loss: 2.2179\n",
      "[shape] Batch 401/450 | Loss: 2.2331\n",
      "[quantity] Batch 450/450 | Loss: 2.1816\n",
      "[Epoch 5] Train Loss: 2.2462 | Acc: 0.2526 | Perplexity: 9.4515\n",
      "[Epoch 5] Val Loss: 2.2386 | Acc: 0.2430 | Perplexity: 9.3800\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 6/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 2.3152\n",
      "[quantity] Batch 51/450 | Loss: 2.1195\n",
      "[shape] Batch 101/450 | Loss: 2.1504\n",
      "[color] Batch 151/450 | Loss: 2.2047\n",
      "[quantity] Batch 201/450 | Loss: 2.3257\n",
      "[shape] Batch 251/450 | Loss: 2.1189\n",
      "[color] Batch 301/450 | Loss: 2.1856\n",
      "[quantity] Batch 351/450 | Loss: 2.0715\n",
      "[shape] Batch 401/450 | Loss: 2.1054\n",
      "[quantity] Batch 450/450 | Loss: 2.1273\n",
      "[Epoch 6] Train Loss: 2.2010 | Acc: 0.2503 | Perplexity: 9.0337\n",
      "[Epoch 6] Val Loss: 2.0453 | Acc: 0.2409 | Perplexity: 7.7318\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 7/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 2.0781\n",
      "[quantity] Batch 51/450 | Loss: 2.1343\n",
      "[shape] Batch 101/450 | Loss: 2.2553\n",
      "[color] Batch 151/450 | Loss: 2.2441\n",
      "[quantity] Batch 201/450 | Loss: 2.2175\n",
      "[shape] Batch 251/450 | Loss: 2.0942\n",
      "[color] Batch 301/450 | Loss: 2.1386\n",
      "[quantity] Batch 351/450 | Loss: 2.0881\n",
      "[shape] Batch 401/450 | Loss: 2.1814\n",
      "[quantity] Batch 450/450 | Loss: 2.1153\n",
      "[Epoch 7] Train Loss: 2.1496 | Acc: 0.2482 | Perplexity: 8.5812\n",
      "[Epoch 7] Val Loss: 2.0391 | Acc: 0.2648 | Perplexity: 7.6836\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 8/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 2.2202\n",
      "[quantity] Batch 51/450 | Loss: 2.0318\n",
      "[shape] Batch 101/450 | Loss: 2.1944\n",
      "[color] Batch 151/450 | Loss: 2.3769\n",
      "[quantity] Batch 201/450 | Loss: 2.1208\n",
      "[shape] Batch 251/450 | Loss: 2.0132\n",
      "[color] Batch 301/450 | Loss: 2.0686\n",
      "[quantity] Batch 351/450 | Loss: 2.0959\n",
      "[shape] Batch 401/450 | Loss: 2.2131\n",
      "[quantity] Batch 450/450 | Loss: 2.0603\n",
      "[Epoch 8] Train Loss: 2.1198 | Acc: 0.2519 | Perplexity: 8.3298\n",
      "[Epoch 8] Val Loss: 1.9877 | Acc: 0.2495 | Perplexity: 7.2989\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 9/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.9766\n",
      "[quantity] Batch 51/450 | Loss: 2.0357\n",
      "[shape] Batch 101/450 | Loss: 2.0556\n",
      "[color] Batch 151/450 | Loss: 2.0987\n",
      "[quantity] Batch 201/450 | Loss: 2.0475\n",
      "[shape] Batch 251/450 | Loss: 2.0789\n",
      "[color] Batch 301/450 | Loss: 2.0186\n",
      "[quantity] Batch 351/450 | Loss: 2.1181\n",
      "[shape] Batch 401/450 | Loss: 2.0269\n",
      "[quantity] Batch 450/450 | Loss: 2.1506\n",
      "[Epoch 9] Train Loss: 2.0879 | Acc: 0.2520 | Perplexity: 8.0680\n",
      "[Epoch 9] Val Loss: 2.0568 | Acc: 0.2453 | Perplexity: 7.8211\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 10/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 2.0573\n",
      "[quantity] Batch 51/450 | Loss: 2.0247\n",
      "[shape] Batch 101/450 | Loss: 2.0317\n",
      "[color] Batch 151/450 | Loss: 2.1332\n",
      "[quantity] Batch 201/450 | Loss: 1.9921\n",
      "[shape] Batch 251/450 | Loss: 2.0842\n",
      "[color] Batch 301/450 | Loss: 2.0142\n",
      "[quantity] Batch 351/450 | Loss: 2.0669\n",
      "[shape] Batch 401/450 | Loss: 1.9682\n",
      "[quantity] Batch 450/450 | Loss: 2.0397\n",
      "[Epoch 10] Train Loss: 2.0782 | Acc: 0.2515 | Perplexity: 7.9902\n",
      "[Epoch 10] Val Loss: 1.9795 | Acc: 0.2545 | Perplexity: 7.2394\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 11/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 2.0344\n",
      "[quantity] Batch 51/450 | Loss: 2.0503\n",
      "[shape] Batch 101/450 | Loss: 2.1136\n",
      "[color] Batch 151/450 | Loss: 2.1095\n",
      "[quantity] Batch 201/450 | Loss: 2.0621\n",
      "[shape] Batch 251/450 | Loss: 2.0181\n",
      "[color] Batch 301/450 | Loss: 2.0273\n",
      "[quantity] Batch 351/450 | Loss: 2.0292\n",
      "[shape] Batch 401/450 | Loss: 2.1343\n",
      "[quantity] Batch 450/450 | Loss: 2.0153\n",
      "[Epoch 11] Train Loss: 2.0490 | Acc: 0.2525 | Perplexity: 7.7598\n",
      "[Epoch 11] Val Loss: 2.1239 | Acc: 0.2430 | Perplexity: 8.3639\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 12/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 2.2093\n",
      "[quantity] Batch 51/450 | Loss: 2.0469\n",
      "[shape] Batch 101/450 | Loss: 2.0307\n",
      "[color] Batch 151/450 | Loss: 2.0820\n",
      "[quantity] Batch 201/450 | Loss: 2.0262\n",
      "[shape] Batch 251/450 | Loss: 2.0250\n",
      "[color] Batch 301/450 | Loss: 2.0122\n",
      "[quantity] Batch 351/450 | Loss: 2.0465\n",
      "[shape] Batch 401/450 | Loss: 2.0424\n",
      "[quantity] Batch 450/450 | Loss: 2.1002\n",
      "[Epoch 12] Train Loss: 2.0430 | Acc: 0.2533 | Perplexity: 7.7139\n",
      "[Epoch 12] Val Loss: 2.0151 | Acc: 0.2434 | Perplexity: 7.5016\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 13/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 2.0447\n",
      "[quantity] Batch 51/450 | Loss: 2.0896\n",
      "[shape] Batch 101/450 | Loss: 2.0103\n",
      "[color] Batch 151/450 | Loss: 1.9804\n",
      "[quantity] Batch 201/450 | Loss: 1.9945\n",
      "[shape] Batch 251/450 | Loss: 2.0127\n",
      "[color] Batch 301/450 | Loss: 2.0007\n",
      "[quantity] Batch 351/450 | Loss: 2.0213\n",
      "[shape] Batch 401/450 | Loss: 1.9989\n",
      "[quantity] Batch 450/450 | Loss: 1.9527\n",
      "[Epoch 13] Train Loss: 2.0257 | Acc: 0.2538 | Perplexity: 7.5812\n",
      "[Epoch 13] Val Loss: 1.9780 | Acc: 0.2467 | Perplexity: 7.2283\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 14/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 2.0196\n",
      "[quantity] Batch 51/450 | Loss: 2.0380\n",
      "[shape] Batch 101/450 | Loss: 2.0262\n",
      "[color] Batch 151/450 | Loss: 1.9986\n",
      "[quantity] Batch 201/450 | Loss: 2.0922\n",
      "[shape] Batch 251/450 | Loss: 2.0495\n",
      "[color] Batch 301/450 | Loss: 1.9665\n",
      "[quantity] Batch 351/450 | Loss: 2.0775\n",
      "[shape] Batch 401/450 | Loss: 2.0496\n",
      "[quantity] Batch 450/450 | Loss: 2.0276\n",
      "[Epoch 14] Train Loss: 2.0198 | Acc: 0.2545 | Perplexity: 7.5367\n",
      "[Epoch 14] Val Loss: 1.9653 | Acc: 0.2506 | Perplexity: 7.1369\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 15/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.9468\n",
      "[quantity] Batch 51/450 | Loss: 2.1031\n",
      "[shape] Batch 101/450 | Loss: 1.9441\n",
      "[color] Batch 151/450 | Loss: 2.0019\n",
      "[quantity] Batch 201/450 | Loss: 1.9885\n",
      "[shape] Batch 251/450 | Loss: 2.0193\n",
      "[color] Batch 301/450 | Loss: 2.0043\n",
      "[quantity] Batch 351/450 | Loss: 1.9835\n",
      "[shape] Batch 401/450 | Loss: 2.0033\n",
      "[quantity] Batch 450/450 | Loss: 1.9942\n",
      "[Epoch 15] Train Loss: 2.0107 | Acc: 0.2622 | Perplexity: 7.4687\n",
      "[Epoch 15] Val Loss: 1.9886 | Acc: 0.2453 | Perplexity: 7.3055\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 16/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 2.0002\n",
      "[quantity] Batch 51/450 | Loss: 1.9688\n",
      "[shape] Batch 101/450 | Loss: 1.9794\n",
      "[color] Batch 151/450 | Loss: 1.9540\n",
      "[quantity] Batch 201/450 | Loss: 1.9845\n",
      "[shape] Batch 251/450 | Loss: 2.0725\n",
      "[color] Batch 301/450 | Loss: 2.0206\n",
      "[quantity] Batch 351/450 | Loss: 2.0157\n",
      "[shape] Batch 401/450 | Loss: 1.9909\n",
      "[quantity] Batch 450/450 | Loss: 1.9987\n",
      "[Epoch 16] Train Loss: 2.0034 | Acc: 0.2568 | Perplexity: 7.4144\n",
      "[Epoch 16] Val Loss: 1.9823 | Acc: 0.2616 | Perplexity: 7.2592\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 17/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.9475\n",
      "[quantity] Batch 51/450 | Loss: 2.0026\n",
      "[shape] Batch 101/450 | Loss: 2.0122\n",
      "[color] Batch 151/450 | Loss: 2.0242\n",
      "[quantity] Batch 201/450 | Loss: 1.9698\n",
      "[shape] Batch 251/450 | Loss: 2.0015\n",
      "[color] Batch 301/450 | Loss: 1.9816\n",
      "[quantity] Batch 351/450 | Loss: 1.9588\n",
      "[shape] Batch 401/450 | Loss: 2.0003\n",
      "[quantity] Batch 450/450 | Loss: 1.9899\n",
      "[Epoch 17] Train Loss: 1.9984 | Acc: 0.2639 | Perplexity: 7.3769\n",
      "[Epoch 17] Val Loss: 1.9776 | Acc: 0.2448 | Perplexity: 7.2251\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 18/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 2.0011\n",
      "[quantity] Batch 51/450 | Loss: 1.9615\n",
      "[shape] Batch 101/450 | Loss: 1.9920\n",
      "[color] Batch 151/450 | Loss: 1.9588\n",
      "[quantity] Batch 201/450 | Loss: 1.9939\n",
      "[shape] Batch 251/450 | Loss: 1.9339\n",
      "[color] Batch 301/450 | Loss: 2.0154\n",
      "[quantity] Batch 351/450 | Loss: 2.0427\n",
      "[shape] Batch 401/450 | Loss: 1.9818\n",
      "[quantity] Batch 450/450 | Loss: 1.9666\n",
      "[Epoch 18] Train Loss: 1.9951 | Acc: 0.2610 | Perplexity: 7.3533\n",
      "[Epoch 18] Val Loss: 1.9753 | Acc: 0.2441 | Perplexity: 7.2091\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 19/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.9469\n",
      "[quantity] Batch 51/450 | Loss: 1.9637\n",
      "[shape] Batch 101/450 | Loss: 2.0379\n",
      "[color] Batch 151/450 | Loss: 2.0332\n",
      "[quantity] Batch 201/450 | Loss: 1.9612\n",
      "[shape] Batch 251/450 | Loss: 1.9728\n",
      "[color] Batch 301/450 | Loss: 1.9906\n",
      "[quantity] Batch 351/450 | Loss: 1.9581\n",
      "[shape] Batch 401/450 | Loss: 1.9767\n",
      "[quantity] Batch 450/450 | Loss: 2.0070\n",
      "[Epoch 19] Train Loss: 1.9902 | Acc: 0.2623 | Perplexity: 7.3167\n",
      "[Epoch 19] Val Loss: 1.9763 | Acc: 0.2655 | Perplexity: 7.2163\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 20/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.9331\n",
      "[quantity] Batch 51/450 | Loss: 1.9730\n",
      "[shape] Batch 101/450 | Loss: 2.0212\n",
      "[color] Batch 151/450 | Loss: 1.9871\n",
      "[quantity] Batch 201/450 | Loss: 1.9472\n",
      "[shape] Batch 251/450 | Loss: 1.9817\n",
      "[color] Batch 301/450 | Loss: 1.9775\n",
      "[quantity] Batch 351/450 | Loss: 1.9785\n",
      "[shape] Batch 401/450 | Loss: 1.9652\n",
      "[quantity] Batch 450/450 | Loss: 1.9954\n",
      "[Epoch 20] Train Loss: 1.9840 | Acc: 0.2632 | Perplexity: 7.2721\n",
      "[Epoch 20] Val Loss: 1.9815 | Acc: 0.2542 | Perplexity: 7.2533\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 21/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.9540\n",
      "[quantity] Batch 51/450 | Loss: 1.9847\n",
      "[shape] Batch 101/450 | Loss: 1.9973\n",
      "[color] Batch 151/450 | Loss: 1.9727\n",
      "[quantity] Batch 201/450 | Loss: 1.9296\n",
      "[shape] Batch 251/450 | Loss: 1.9490\n",
      "[color] Batch 301/450 | Loss: 1.9671\n",
      "[quantity] Batch 351/450 | Loss: 1.9431\n",
      "[shape] Batch 401/450 | Loss: 2.0105\n",
      "[quantity] Batch 450/450 | Loss: 2.0000\n",
      "[Epoch 21] Train Loss: 1.9812 | Acc: 0.2680 | Perplexity: 7.2517\n",
      "[Epoch 21] Val Loss: 1.9695 | Acc: 0.2511 | Perplexity: 7.1674\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 22/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.9560\n",
      "[quantity] Batch 51/450 | Loss: 1.9963\n",
      "[shape] Batch 101/450 | Loss: 1.9722\n",
      "[color] Batch 151/450 | Loss: 1.9556\n",
      "[quantity] Batch 201/450 | Loss: 2.0074\n",
      "[shape] Batch 251/450 | Loss: 1.9492\n",
      "[color] Batch 301/450 | Loss: 1.9892\n",
      "[quantity] Batch 351/450 | Loss: 1.9705\n",
      "[shape] Batch 401/450 | Loss: 1.9692\n",
      "[quantity] Batch 450/450 | Loss: 1.9901\n",
      "[Epoch 22] Train Loss: 1.9806 | Acc: 0.2705 | Perplexity: 7.2469\n",
      "[Epoch 22] Val Loss: 1.9796 | Acc: 0.2564 | Perplexity: 7.2402\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 23/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.9864\n",
      "[quantity] Batch 51/450 | Loss: 1.9927\n",
      "[shape] Batch 101/450 | Loss: 1.9523\n",
      "[color] Batch 151/450 | Loss: 1.9850\n",
      "[quantity] Batch 201/450 | Loss: 1.9354\n",
      "[shape] Batch 251/450 | Loss: 1.9719\n",
      "[color] Batch 301/450 | Loss: 1.9825\n",
      "[quantity] Batch 351/450 | Loss: 1.9587\n",
      "[shape] Batch 401/450 | Loss: 1.9973\n",
      "[quantity] Batch 450/450 | Loss: 1.9731\n",
      "[Epoch 23] Train Loss: 1.9743 | Acc: 0.2812 | Perplexity: 7.2015\n",
      "[Epoch 23] Val Loss: 1.9743 | Acc: 0.2500 | Perplexity: 7.2017\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 24/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.9399\n",
      "[quantity] Batch 51/450 | Loss: 1.9392\n",
      "[shape] Batch 101/450 | Loss: 1.9648\n",
      "[color] Batch 151/450 | Loss: 1.9644\n",
      "[quantity] Batch 201/450 | Loss: 1.9937\n",
      "[shape] Batch 251/450 | Loss: 1.9600\n",
      "[color] Batch 301/450 | Loss: 1.9802\n",
      "[quantity] Batch 351/450 | Loss: 1.9462\n",
      "[shape] Batch 401/450 | Loss: 1.9405\n",
      "[quantity] Batch 450/450 | Loss: 1.9529\n",
      "[Epoch 24] Train Loss: 1.9662 | Acc: 0.2892 | Perplexity: 7.1438\n",
      "[Epoch 24] Val Loss: 2.0007 | Acc: 0.2470 | Perplexity: 7.3939\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 25/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.9394\n",
      "[quantity] Batch 51/450 | Loss: 1.9567\n",
      "[shape] Batch 101/450 | Loss: 1.9981\n",
      "[color] Batch 151/450 | Loss: 1.9513\n",
      "[quantity] Batch 201/450 | Loss: 1.9583\n",
      "[shape] Batch 251/450 | Loss: 2.0047\n",
      "[color] Batch 301/450 | Loss: 1.9657\n",
      "[quantity] Batch 351/450 | Loss: 1.9735\n",
      "[shape] Batch 401/450 | Loss: 1.9516\n",
      "[quantity] Batch 450/450 | Loss: 1.9497\n",
      "[Epoch 25] Train Loss: 1.9607 | Acc: 0.3016 | Perplexity: 7.1045\n",
      "[Epoch 25] Val Loss: 1.9875 | Acc: 0.2491 | Perplexity: 7.2973\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 26/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.9435\n",
      "[quantity] Batch 51/450 | Loss: 1.9368\n",
      "[shape] Batch 101/450 | Loss: 2.0269\n",
      "[color] Batch 151/450 | Loss: 1.9208\n",
      "[quantity] Batch 201/450 | Loss: 1.9284\n",
      "[shape] Batch 251/450 | Loss: 1.9152\n",
      "[color] Batch 301/450 | Loss: 1.9365\n",
      "[quantity] Batch 351/450 | Loss: 1.9742\n",
      "[shape] Batch 401/450 | Loss: 1.9513\n",
      "[quantity] Batch 450/450 | Loss: 1.9160\n",
      "[Epoch 26] Train Loss: 1.9548 | Acc: 0.3122 | Perplexity: 7.0625\n",
      "[Epoch 26] Val Loss: 2.0072 | Acc: 0.2539 | Perplexity: 7.4428\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 27/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.9183\n",
      "[quantity] Batch 51/450 | Loss: 1.9085\n",
      "[shape] Batch 101/450 | Loss: 1.9139\n",
      "[color] Batch 151/450 | Loss: 2.0380\n",
      "[quantity] Batch 201/450 | Loss: 1.9420\n",
      "[shape] Batch 251/450 | Loss: 1.9969\n",
      "[color] Batch 301/450 | Loss: 2.0485\n",
      "[quantity] Batch 351/450 | Loss: 2.0082\n",
      "[shape] Batch 401/450 | Loss: 1.9534\n",
      "[quantity] Batch 450/450 | Loss: 1.9097\n",
      "[Epoch 27] Train Loss: 1.9506 | Acc: 0.3192 | Perplexity: 7.0326\n",
      "[Epoch 27] Val Loss: 2.0176 | Acc: 0.2528 | Perplexity: 7.5202\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 28/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.9503\n",
      "[quantity] Batch 51/450 | Loss: 1.9366\n",
      "[shape] Batch 101/450 | Loss: 1.9477\n",
      "[color] Batch 151/450 | Loss: 1.9616\n",
      "[quantity] Batch 201/450 | Loss: 1.9035\n",
      "[shape] Batch 251/450 | Loss: 1.9217\n",
      "[color] Batch 301/450 | Loss: 2.0105\n",
      "[quantity] Batch 351/450 | Loss: 1.9630\n",
      "[shape] Batch 401/450 | Loss: 1.8777\n",
      "[quantity] Batch 450/450 | Loss: 1.9437\n",
      "[Epoch 28] Train Loss: 1.9415 | Acc: 0.3277 | Perplexity: 6.9694\n",
      "[Epoch 28] Val Loss: 2.0161 | Acc: 0.2545 | Perplexity: 7.5088\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 29/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.9038\n",
      "[quantity] Batch 51/450 | Loss: 2.0204\n",
      "[shape] Batch 101/450 | Loss: 1.8715\n",
      "[color] Batch 151/450 | Loss: 1.9160\n",
      "[quantity] Batch 201/450 | Loss: 1.9517\n",
      "[shape] Batch 251/450 | Loss: 1.9558\n",
      "[color] Batch 301/450 | Loss: 1.9290\n",
      "[quantity] Batch 351/450 | Loss: 1.9097\n",
      "[shape] Batch 401/450 | Loss: 1.9601\n",
      "[quantity] Batch 450/450 | Loss: 1.9043\n",
      "[Epoch 29] Train Loss: 1.9368 | Acc: 0.3339 | Perplexity: 6.9368\n",
      "[Epoch 29] Val Loss: 2.0335 | Acc: 0.2506 | Perplexity: 7.6411\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 30/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.9404\n",
      "[quantity] Batch 51/450 | Loss: 1.8506\n",
      "[shape] Batch 101/450 | Loss: 1.9199\n",
      "[color] Batch 151/450 | Loss: 1.9199\n",
      "[quantity] Batch 201/450 | Loss: 1.9578\n",
      "[shape] Batch 251/450 | Loss: 1.9130\n",
      "[color] Batch 301/450 | Loss: 1.9848\n",
      "[quantity] Batch 351/450 | Loss: 1.9670\n",
      "[shape] Batch 401/450 | Loss: 1.9559\n",
      "[quantity] Batch 450/450 | Loss: 1.9440\n",
      "[Epoch 30] Train Loss: 1.9382 | Acc: 0.3409 | Perplexity: 6.9463\n",
      "[Epoch 30] Val Loss: 2.0321 | Acc: 0.2425 | Perplexity: 7.6304\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 31/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.9233\n",
      "[quantity] Batch 51/450 | Loss: 1.9397\n",
      "[shape] Batch 101/450 | Loss: 1.9206\n",
      "[color] Batch 151/450 | Loss: 1.8998\n",
      "[quantity] Batch 201/450 | Loss: 1.9401\n",
      "[shape] Batch 251/450 | Loss: 2.0095\n",
      "[color] Batch 301/450 | Loss: 1.8622\n",
      "[quantity] Batch 351/450 | Loss: 1.9742\n",
      "[shape] Batch 401/450 | Loss: 1.8849\n",
      "[quantity] Batch 450/450 | Loss: 1.8213\n",
      "[Epoch 31] Train Loss: 1.9284 | Acc: 0.3465 | Perplexity: 6.8785\n",
      "[Epoch 31] Val Loss: 2.0582 | Acc: 0.2462 | Perplexity: 7.8320\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 32/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.9077\n",
      "[quantity] Batch 51/450 | Loss: 1.8761\n",
      "[shape] Batch 101/450 | Loss: 1.8832\n",
      "[color] Batch 151/450 | Loss: 1.8314\n",
      "[quantity] Batch 201/450 | Loss: 1.9874\n",
      "[shape] Batch 251/450 | Loss: 1.9268\n",
      "[color] Batch 301/450 | Loss: 1.9376\n",
      "[quantity] Batch 351/450 | Loss: 1.8624\n",
      "[shape] Batch 401/450 | Loss: 1.8775\n",
      "[quantity] Batch 450/450 | Loss: 1.8549\n",
      "[Epoch 32] Train Loss: 1.9185 | Acc: 0.3582 | Perplexity: 6.8108\n",
      "[Epoch 32] Val Loss: 2.0583 | Acc: 0.2452 | Perplexity: 7.8327\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 33/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.9587\n",
      "[quantity] Batch 51/450 | Loss: 1.9322\n",
      "[shape] Batch 101/450 | Loss: 1.8937\n",
      "[color] Batch 151/450 | Loss: 1.8587\n",
      "[quantity] Batch 201/450 | Loss: 1.9424\n",
      "[shape] Batch 251/450 | Loss: 1.9624\n",
      "[color] Batch 301/450 | Loss: 1.9142\n",
      "[quantity] Batch 351/450 | Loss: 1.9092\n",
      "[shape] Batch 401/450 | Loss: 1.8931\n",
      "[quantity] Batch 450/450 | Loss: 1.9876\n",
      "[Epoch 33] Train Loss: 1.9087 | Acc: 0.3679 | Perplexity: 6.7440\n",
      "[Epoch 33] Val Loss: 2.0552 | Acc: 0.2519 | Perplexity: 7.8084\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 34/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.8403\n",
      "[quantity] Batch 51/450 | Loss: 2.0092\n",
      "[shape] Batch 101/450 | Loss: 1.9370\n",
      "[color] Batch 151/450 | Loss: 1.8112\n",
      "[quantity] Batch 201/450 | Loss: 1.8860\n",
      "[shape] Batch 251/450 | Loss: 1.8587\n",
      "[color] Batch 301/450 | Loss: 1.9629\n",
      "[quantity] Batch 351/450 | Loss: 1.9559\n",
      "[shape] Batch 401/450 | Loss: 1.9789\n",
      "[quantity] Batch 450/450 | Loss: 1.9340\n",
      "[Epoch 34] Train Loss: 1.9061 | Acc: 0.3669 | Perplexity: 6.7271\n",
      "[Epoch 34] Val Loss: 2.0448 | Acc: 0.2503 | Perplexity: 7.7275\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 35/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.8945\n",
      "[quantity] Batch 51/450 | Loss: 1.8680\n",
      "[shape] Batch 101/450 | Loss: 1.8970\n",
      "[color] Batch 151/450 | Loss: 1.9284\n",
      "[quantity] Batch 201/450 | Loss: 1.8998\n",
      "[shape] Batch 251/450 | Loss: 1.8348\n",
      "[color] Batch 301/450 | Loss: 1.8678\n",
      "[quantity] Batch 351/450 | Loss: 1.9048\n",
      "[shape] Batch 401/450 | Loss: 1.9358\n",
      "[quantity] Batch 450/450 | Loss: 1.8762\n",
      "[Epoch 35] Train Loss: 1.8995 | Acc: 0.3745 | Perplexity: 6.6824\n",
      "[Epoch 35] Val Loss: 2.0751 | Acc: 0.2439 | Perplexity: 7.9652\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 36/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.9048\n",
      "[quantity] Batch 51/450 | Loss: 1.8962\n",
      "[shape] Batch 101/450 | Loss: 1.8573\n",
      "[color] Batch 151/450 | Loss: 1.8363\n",
      "[quantity] Batch 201/450 | Loss: 1.9367\n",
      "[shape] Batch 251/450 | Loss: 1.9064\n",
      "[color] Batch 301/450 | Loss: 1.8085\n",
      "[quantity] Batch 351/450 | Loss: 1.8710\n",
      "[shape] Batch 401/450 | Loss: 2.0227\n",
      "[quantity] Batch 450/450 | Loss: 1.9028\n",
      "[Epoch 36] Train Loss: 1.8923 | Acc: 0.3838 | Perplexity: 6.6348\n",
      "[Epoch 36] Val Loss: 2.0711 | Acc: 0.2520 | Perplexity: 7.9332\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 37/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.8118\n",
      "[quantity] Batch 51/450 | Loss: 1.9630\n",
      "[shape] Batch 101/450 | Loss: 1.9440\n",
      "[color] Batch 151/450 | Loss: 1.8275\n",
      "[quantity] Batch 201/450 | Loss: 1.8928\n",
      "[shape] Batch 251/450 | Loss: 1.8757\n",
      "[color] Batch 301/450 | Loss: 1.8728\n",
      "[quantity] Batch 351/450 | Loss: 1.9429\n",
      "[shape] Batch 401/450 | Loss: 1.8718\n",
      "[quantity] Batch 450/450 | Loss: 1.9287\n",
      "[Epoch 37] Train Loss: 1.8849 | Acc: 0.3921 | Perplexity: 6.5858\n",
      "[Epoch 37] Val Loss: 2.0837 | Acc: 0.2464 | Perplexity: 8.0345\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 38/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.8493\n",
      "[quantity] Batch 51/450 | Loss: 1.9075\n",
      "[shape] Batch 101/450 | Loss: 1.7969\n",
      "[color] Batch 151/450 | Loss: 1.8544\n",
      "[quantity] Batch 201/450 | Loss: 1.8467\n",
      "[shape] Batch 251/450 | Loss: 1.9806\n",
      "[color] Batch 301/450 | Loss: 1.8604\n",
      "[quantity] Batch 351/450 | Loss: 1.8469\n",
      "[shape] Batch 401/450 | Loss: 1.8860\n",
      "[quantity] Batch 450/450 | Loss: 1.9050\n",
      "[Epoch 38] Train Loss: 1.8837 | Acc: 0.3918 | Perplexity: 6.5781\n",
      "[Epoch 38] Val Loss: 2.0673 | Acc: 0.2534 | Perplexity: 7.9035\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 39/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.9188\n",
      "[quantity] Batch 51/450 | Loss: 1.9166\n",
      "[shape] Batch 101/450 | Loss: 1.8736\n",
      "[color] Batch 151/450 | Loss: 1.8453\n",
      "[quantity] Batch 201/450 | Loss: 1.8211\n",
      "[shape] Batch 251/450 | Loss: 1.8988\n",
      "[color] Batch 301/450 | Loss: 1.8164\n",
      "[quantity] Batch 351/450 | Loss: 1.9394\n",
      "[shape] Batch 401/450 | Loss: 1.8549\n",
      "[quantity] Batch 450/450 | Loss: 1.8259\n",
      "[Epoch 39] Train Loss: 1.8753 | Acc: 0.4011 | Perplexity: 6.5229\n",
      "[Epoch 39] Val Loss: 2.1030 | Acc: 0.2530 | Perplexity: 8.1906\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 40/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.8659\n",
      "[quantity] Batch 51/450 | Loss: 1.8422\n",
      "[shape] Batch 101/450 | Loss: 1.9411\n",
      "[color] Batch 151/450 | Loss: 1.9234\n",
      "[quantity] Batch 201/450 | Loss: 1.7925\n",
      "[shape] Batch 251/450 | Loss: 1.9155\n",
      "[color] Batch 301/450 | Loss: 1.8408\n",
      "[quantity] Batch 351/450 | Loss: 1.8525\n",
      "[shape] Batch 401/450 | Loss: 1.8515\n",
      "[quantity] Batch 450/450 | Loss: 1.9062\n",
      "[Epoch 40] Train Loss: 1.8642 | Acc: 0.4069 | Perplexity: 6.4510\n",
      "[Epoch 40] Val Loss: 2.0914 | Acc: 0.2608 | Perplexity: 8.0960\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 41/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.8687\n",
      "[quantity] Batch 51/450 | Loss: 1.7715\n",
      "[shape] Batch 101/450 | Loss: 1.9077\n",
      "[color] Batch 151/450 | Loss: 1.9582\n",
      "[quantity] Batch 201/450 | Loss: 1.8286\n",
      "[shape] Batch 251/450 | Loss: 1.8217\n",
      "[color] Batch 301/450 | Loss: 1.8068\n",
      "[quantity] Batch 351/450 | Loss: 1.9931\n",
      "[shape] Batch 401/450 | Loss: 1.9423\n",
      "[quantity] Batch 450/450 | Loss: 1.8850\n",
      "[Epoch 41] Train Loss: 1.8537 | Acc: 0.4207 | Perplexity: 6.3836\n",
      "[Epoch 41] Val Loss: 2.1218 | Acc: 0.2497 | Perplexity: 8.3459\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 42/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.9074\n",
      "[quantity] Batch 51/450 | Loss: 1.8301\n",
      "[shape] Batch 101/450 | Loss: 1.8221\n",
      "[color] Batch 151/450 | Loss: 1.7995\n",
      "[quantity] Batch 201/450 | Loss: 1.8544\n",
      "[shape] Batch 251/450 | Loss: 1.7706\n",
      "[color] Batch 301/450 | Loss: 1.8148\n",
      "[quantity] Batch 351/450 | Loss: 1.9051\n",
      "[shape] Batch 401/450 | Loss: 2.0221\n",
      "[quantity] Batch 450/450 | Loss: 1.8255\n",
      "[Epoch 42] Train Loss: 1.8489 | Acc: 0.4249 | Perplexity: 6.3526\n",
      "[Epoch 42] Val Loss: 2.1263 | Acc: 0.2630 | Perplexity: 8.3841\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 43/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.9074\n",
      "[quantity] Batch 51/450 | Loss: 1.8998\n",
      "[shape] Batch 101/450 | Loss: 1.7891\n",
      "[color] Batch 151/450 | Loss: 1.7955\n",
      "[quantity] Batch 201/450 | Loss: 1.8379\n",
      "[shape] Batch 251/450 | Loss: 1.9067\n",
      "[color] Batch 301/450 | Loss: 1.9242\n",
      "[quantity] Batch 351/450 | Loss: 1.7403\n",
      "[shape] Batch 401/450 | Loss: 1.7393\n",
      "[quantity] Batch 450/450 | Loss: 1.9558\n",
      "[Epoch 43] Train Loss: 1.8359 | Acc: 0.4383 | Perplexity: 6.2706\n",
      "[Epoch 43] Val Loss: 2.1639 | Acc: 0.2584 | Perplexity: 8.7047\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 44/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.7035\n",
      "[quantity] Batch 51/450 | Loss: 1.8468\n",
      "[shape] Batch 101/450 | Loss: 1.7563\n",
      "[color] Batch 151/450 | Loss: 1.9524\n",
      "[quantity] Batch 201/450 | Loss: 1.9375\n",
      "[shape] Batch 251/450 | Loss: 1.7548\n",
      "[color] Batch 301/450 | Loss: 1.9037\n",
      "[quantity] Batch 351/450 | Loss: 1.7696\n",
      "[shape] Batch 401/450 | Loss: 1.8179\n",
      "[quantity] Batch 450/450 | Loss: 1.7673\n",
      "[Epoch 44] Train Loss: 1.8309 | Acc: 0.4358 | Perplexity: 6.2392\n",
      "[Epoch 44] Val Loss: 2.1666 | Acc: 0.2598 | Perplexity: 8.7286\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 45/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.8058\n",
      "[quantity] Batch 51/450 | Loss: 1.9341\n",
      "[shape] Batch 101/450 | Loss: 1.8981\n",
      "[color] Batch 151/450 | Loss: 1.6864\n",
      "[quantity] Batch 201/450 | Loss: 1.7460\n",
      "[shape] Batch 251/450 | Loss: 1.8524\n",
      "[color] Batch 301/450 | Loss: 1.8812\n",
      "[quantity] Batch 351/450 | Loss: 1.8595\n",
      "[shape] Batch 401/450 | Loss: 1.8167\n",
      "[quantity] Batch 450/450 | Loss: 1.9786\n",
      "[Epoch 45] Train Loss: 1.8194 | Acc: 0.4455 | Perplexity: 6.1682\n",
      "[Epoch 45] Val Loss: 2.1841 | Acc: 0.2491 | Perplexity: 8.8830\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 46/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.9215\n",
      "[quantity] Batch 51/450 | Loss: 1.7309\n",
      "[shape] Batch 101/450 | Loss: 1.8614\n",
      "[color] Batch 151/450 | Loss: 1.7870\n",
      "[quantity] Batch 201/450 | Loss: 1.7962\n",
      "[shape] Batch 251/450 | Loss: 1.8053\n",
      "[color] Batch 301/450 | Loss: 1.7494\n",
      "[quantity] Batch 351/450 | Loss: 1.8100\n",
      "[shape] Batch 401/450 | Loss: 1.7885\n",
      "[quantity] Batch 450/450 | Loss: 1.8020\n",
      "[Epoch 46] Train Loss: 1.8112 | Acc: 0.4518 | Perplexity: 6.1180\n",
      "[Epoch 46] Val Loss: 2.1857 | Acc: 0.2567 | Perplexity: 8.8966\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 47/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.7905\n",
      "[quantity] Batch 51/450 | Loss: 1.7946\n",
      "[shape] Batch 101/450 | Loss: 1.7903\n",
      "[color] Batch 151/450 | Loss: 1.9611\n",
      "[quantity] Batch 201/450 | Loss: 1.7906\n",
      "[shape] Batch 251/450 | Loss: 1.6512\n",
      "[color] Batch 301/450 | Loss: 1.6965\n",
      "[quantity] Batch 351/450 | Loss: 1.7906\n",
      "[shape] Batch 401/450 | Loss: 1.7835\n",
      "[quantity] Batch 450/450 | Loss: 1.8191\n",
      "[Epoch 47] Train Loss: 1.8046 | Acc: 0.4636 | Perplexity: 6.0777\n",
      "[Epoch 47] Val Loss: 2.1678 | Acc: 0.2558 | Perplexity: 8.7395\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 48/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.9090\n",
      "[quantity] Batch 51/450 | Loss: 1.8127\n",
      "[shape] Batch 101/450 | Loss: 1.6973\n",
      "[color] Batch 151/450 | Loss: 1.8514\n",
      "[quantity] Batch 201/450 | Loss: 1.8180\n",
      "[shape] Batch 251/450 | Loss: 1.7428\n",
      "[color] Batch 301/450 | Loss: 1.7376\n",
      "[quantity] Batch 351/450 | Loss: 1.8254\n",
      "[shape] Batch 401/450 | Loss: 1.8282\n",
      "[quantity] Batch 450/450 | Loss: 1.7525\n",
      "[Epoch 48] Train Loss: 1.7930 | Acc: 0.4686 | Perplexity: 6.0076\n",
      "[Epoch 48] Val Loss: 2.1745 | Acc: 0.2566 | Perplexity: 8.7976\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 49/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.7746\n",
      "[quantity] Batch 51/450 | Loss: 1.8624\n",
      "[shape] Batch 101/450 | Loss: 1.8002\n",
      "[color] Batch 151/450 | Loss: 1.7673\n",
      "[quantity] Batch 201/450 | Loss: 1.7497\n",
      "[shape] Batch 251/450 | Loss: 1.8213\n",
      "[color] Batch 301/450 | Loss: 1.7468\n",
      "[quantity] Batch 351/450 | Loss: 1.7562\n",
      "[shape] Batch 401/450 | Loss: 1.8269\n",
      "[quantity] Batch 450/450 | Loss: 1.7023\n",
      "[Epoch 49] Train Loss: 1.7840 | Acc: 0.4716 | Perplexity: 5.9536\n",
      "[Epoch 49] Val Loss: 2.2186 | Acc: 0.2648 | Perplexity: 9.1943\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 50/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.7279\n",
      "[quantity] Batch 51/450 | Loss: 1.6694\n",
      "[shape] Batch 101/450 | Loss: 1.7896\n",
      "[color] Batch 151/450 | Loss: 1.6803\n",
      "[quantity] Batch 201/450 | Loss: 1.6381\n",
      "[shape] Batch 251/450 | Loss: 1.6870\n",
      "[color] Batch 301/450 | Loss: 1.7601\n",
      "[quantity] Batch 351/450 | Loss: 1.6873\n",
      "[shape] Batch 401/450 | Loss: 1.8127\n",
      "[quantity] Batch 450/450 | Loss: 1.7958\n",
      "[Epoch 50] Train Loss: 1.7715 | Acc: 0.4860 | Perplexity: 5.8798\n",
      "[Epoch 50] Val Loss: 2.2179 | Acc: 0.2587 | Perplexity: 9.1880\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 51/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.7216\n",
      "[quantity] Batch 51/450 | Loss: 1.7104\n",
      "[shape] Batch 101/450 | Loss: 1.6109\n",
      "[color] Batch 151/450 | Loss: 1.6985\n",
      "[quantity] Batch 201/450 | Loss: 1.7820\n",
      "[shape] Batch 251/450 | Loss: 1.7456\n",
      "[color] Batch 301/450 | Loss: 1.8054\n",
      "[quantity] Batch 351/450 | Loss: 1.7116\n",
      "[shape] Batch 401/450 | Loss: 1.7951\n",
      "[quantity] Batch 450/450 | Loss: 1.8655\n",
      "[Epoch 51] Train Loss: 1.7723 | Acc: 0.4835 | Perplexity: 5.8844\n",
      "[Epoch 51] Val Loss: 2.2700 | Acc: 0.2509 | Perplexity: 9.6793\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 52/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.9921\n",
      "[quantity] Batch 51/450 | Loss: 1.9261\n",
      "[shape] Batch 101/450 | Loss: 1.7718\n",
      "[color] Batch 151/450 | Loss: 1.7821\n",
      "[quantity] Batch 201/450 | Loss: 1.9485\n",
      "[shape] Batch 251/450 | Loss: 1.8249\n",
      "[color] Batch 301/450 | Loss: 1.7686\n",
      "[quantity] Batch 351/450 | Loss: 1.8670\n",
      "[shape] Batch 401/450 | Loss: 1.8626\n",
      "[quantity] Batch 450/450 | Loss: 1.7471\n",
      "[Epoch 52] Train Loss: 1.8425 | Acc: 0.4338 | Perplexity: 6.3122\n",
      "[Epoch 52] Val Loss: 2.1026 | Acc: 0.2623 | Perplexity: 8.1875\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 53/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.8888\n",
      "[quantity] Batch 51/450 | Loss: 1.8165\n",
      "[shape] Batch 101/450 | Loss: 1.8116\n",
      "[color] Batch 151/450 | Loss: 1.7298\n",
      "[quantity] Batch 201/450 | Loss: 1.8911\n",
      "[shape] Batch 251/450 | Loss: 1.8811\n",
      "[color] Batch 301/450 | Loss: 1.8321\n",
      "[quantity] Batch 351/450 | Loss: 1.8010\n",
      "[shape] Batch 401/450 | Loss: 1.7869\n",
      "[quantity] Batch 450/450 | Loss: 1.7413\n",
      "[Epoch 53] Train Loss: 1.8276 | Acc: 0.4407 | Perplexity: 6.2189\n",
      "[Epoch 53] Val Loss: 2.1833 | Acc: 0.2564 | Perplexity: 8.8760\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 54/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.9341\n",
      "[quantity] Batch 51/450 | Loss: 1.8079\n",
      "[shape] Batch 101/450 | Loss: 1.8835\n",
      "[color] Batch 151/450 | Loss: 1.7916\n",
      "[quantity] Batch 201/450 | Loss: 1.6095\n",
      "[shape] Batch 251/450 | Loss: 1.9739\n",
      "[color] Batch 301/450 | Loss: 1.8245\n",
      "[quantity] Batch 351/450 | Loss: 1.7322\n",
      "[shape] Batch 401/450 | Loss: 1.8022\n",
      "[quantity] Batch 450/450 | Loss: 1.7978\n",
      "[Epoch 54] Train Loss: 1.7948 | Acc: 0.4728 | Perplexity: 6.0181\n",
      "[Epoch 54] Val Loss: 2.2199 | Acc: 0.2559 | Perplexity: 9.2060\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 55/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.8020\n",
      "[quantity] Batch 51/450 | Loss: 1.8052\n",
      "[shape] Batch 101/450 | Loss: 1.8110\n",
      "[color] Batch 151/450 | Loss: 1.7675\n",
      "[quantity] Batch 201/450 | Loss: 1.8190\n",
      "[shape] Batch 251/450 | Loss: 1.8062\n",
      "[color] Batch 301/450 | Loss: 1.7931\n",
      "[quantity] Batch 351/450 | Loss: 1.7390\n",
      "[shape] Batch 401/450 | Loss: 1.8300\n",
      "[quantity] Batch 450/450 | Loss: 1.6891\n",
      "[Epoch 55] Train Loss: 1.7759 | Acc: 0.4832 | Perplexity: 5.9053\n",
      "[Epoch 55] Val Loss: 2.2328 | Acc: 0.2606 | Perplexity: 9.3262\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 56/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.7697\n",
      "[quantity] Batch 51/450 | Loss: 1.7500\n",
      "[shape] Batch 101/450 | Loss: 1.7250\n",
      "[color] Batch 151/450 | Loss: 1.7620\n",
      "[quantity] Batch 201/450 | Loss: 1.6267\n",
      "[shape] Batch 251/450 | Loss: 1.7488\n",
      "[color] Batch 301/450 | Loss: 1.8445\n",
      "[quantity] Batch 351/450 | Loss: 1.7719\n",
      "[shape] Batch 401/450 | Loss: 1.6807\n",
      "[quantity] Batch 450/450 | Loss: 1.7394\n",
      "[Epoch 56] Train Loss: 1.7660 | Acc: 0.4888 | Perplexity: 5.8477\n",
      "[Epoch 56] Val Loss: 2.2514 | Acc: 0.2605 | Perplexity: 9.5012\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 57/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.7775\n",
      "[quantity] Batch 51/450 | Loss: 1.7353\n",
      "[shape] Batch 101/450 | Loss: 1.7878\n",
      "[color] Batch 151/450 | Loss: 1.6924\n",
      "[quantity] Batch 201/450 | Loss: 1.5422\n",
      "[shape] Batch 251/450 | Loss: 1.6826\n",
      "[color] Batch 301/450 | Loss: 1.7870\n",
      "[quantity] Batch 351/450 | Loss: 1.8080\n",
      "[shape] Batch 401/450 | Loss: 1.7318\n",
      "[quantity] Batch 450/450 | Loss: 1.7209\n",
      "[Epoch 57] Train Loss: 1.7481 | Acc: 0.5004 | Perplexity: 5.7435\n",
      "[Epoch 57] Val Loss: 2.2925 | Acc: 0.2581 | Perplexity: 9.8998\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 58/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.8381\n",
      "[quantity] Batch 51/450 | Loss: 1.8228\n",
      "[shape] Batch 101/450 | Loss: 1.6319\n",
      "[color] Batch 151/450 | Loss: 1.6240\n",
      "[quantity] Batch 201/450 | Loss: 1.6604\n",
      "[shape] Batch 251/450 | Loss: 1.6909\n",
      "[color] Batch 301/450 | Loss: 1.7709\n",
      "[quantity] Batch 351/450 | Loss: 1.7975\n",
      "[shape] Batch 401/450 | Loss: 1.6182\n",
      "[quantity] Batch 450/450 | Loss: 1.7498\n",
      "[Epoch 58] Train Loss: 1.7387 | Acc: 0.5059 | Perplexity: 5.6898\n",
      "[Epoch 58] Val Loss: 2.2901 | Acc: 0.2619 | Perplexity: 9.8763\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 59/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.8124\n",
      "[quantity] Batch 51/450 | Loss: 1.6454\n",
      "[shape] Batch 101/450 | Loss: 1.7697\n",
      "[color] Batch 151/450 | Loss: 1.6290\n",
      "[quantity] Batch 201/450 | Loss: 1.6651\n",
      "[shape] Batch 251/450 | Loss: 1.8403\n",
      "[color] Batch 301/450 | Loss: 1.7646\n",
      "[quantity] Batch 351/450 | Loss: 1.6726\n",
      "[shape] Batch 401/450 | Loss: 1.7036\n",
      "[quantity] Batch 450/450 | Loss: 1.8576\n",
      "[Epoch 59] Train Loss: 1.7270 | Acc: 0.5149 | Perplexity: 5.6235\n",
      "[Epoch 59] Val Loss: 2.3079 | Acc: 0.2553 | Perplexity: 10.0529\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 60/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.7298\n",
      "[quantity] Batch 51/450 | Loss: 1.6090\n",
      "[shape] Batch 101/450 | Loss: 1.6532\n",
      "[color] Batch 151/450 | Loss: 1.6308\n",
      "[quantity] Batch 201/450 | Loss: 1.5700\n",
      "[shape] Batch 251/450 | Loss: 1.8854\n",
      "[color] Batch 301/450 | Loss: 1.6863\n",
      "[quantity] Batch 351/450 | Loss: 1.7636\n",
      "[shape] Batch 401/450 | Loss: 1.7691\n",
      "[quantity] Batch 450/450 | Loss: 1.6215\n",
      "[Epoch 60] Train Loss: 1.7300 | Acc: 0.5150 | Perplexity: 5.6404\n",
      "[Epoch 60] Val Loss: 2.2438 | Acc: 0.2598 | Perplexity: 9.4291\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 61/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.6507\n",
      "[quantity] Batch 51/450 | Loss: 1.5967\n",
      "[shape] Batch 101/450 | Loss: 1.6764\n",
      "[color] Batch 151/450 | Loss: 1.8836\n",
      "[quantity] Batch 201/450 | Loss: 1.7521\n",
      "[shape] Batch 251/450 | Loss: 1.6275\n",
      "[color] Batch 301/450 | Loss: 1.7178\n",
      "[quantity] Batch 351/450 | Loss: 1.8427\n",
      "[shape] Batch 401/450 | Loss: 1.8225\n",
      "[quantity] Batch 450/450 | Loss: 1.7054\n",
      "[Epoch 61] Train Loss: 1.7212 | Acc: 0.5164 | Perplexity: 5.5912\n",
      "[Epoch 61] Val Loss: 2.3066 | Acc: 0.2583 | Perplexity: 10.0407\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 62/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.6809\n",
      "[quantity] Batch 51/450 | Loss: 1.6642\n",
      "[shape] Batch 101/450 | Loss: 1.6913\n",
      "[color] Batch 151/450 | Loss: 1.7024\n",
      "[quantity] Batch 201/450 | Loss: 1.7764\n",
      "[shape] Batch 251/450 | Loss: 1.5623\n",
      "[color] Batch 301/450 | Loss: 1.7295\n",
      "[quantity] Batch 351/450 | Loss: 1.7240\n",
      "[shape] Batch 401/450 | Loss: 1.7297\n",
      "[quantity] Batch 450/450 | Loss: 1.8319\n",
      "[Epoch 62] Train Loss: 1.7087 | Acc: 0.5225 | Perplexity: 5.5219\n",
      "[Epoch 62] Val Loss: 2.2879 | Acc: 0.2595 | Perplexity: 9.8538\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 63/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.5725\n",
      "[quantity] Batch 51/450 | Loss: 1.7197\n",
      "[shape] Batch 101/450 | Loss: 1.7212\n",
      "[color] Batch 151/450 | Loss: 1.6993\n",
      "[quantity] Batch 201/450 | Loss: 1.7834\n",
      "[shape] Batch 251/450 | Loss: 1.7461\n",
      "[color] Batch 301/450 | Loss: 1.7337\n",
      "[quantity] Batch 351/450 | Loss: 1.6812\n",
      "[shape] Batch 401/450 | Loss: 1.6734\n",
      "[quantity] Batch 450/450 | Loss: 1.7602\n",
      "[Epoch 63] Train Loss: 1.6979 | Acc: 0.5339 | Perplexity: 5.4623\n",
      "[Epoch 63] Val Loss: 2.3242 | Acc: 0.2655 | Perplexity: 10.2188\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 64/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.6706\n",
      "[quantity] Batch 51/450 | Loss: 1.7252\n",
      "[shape] Batch 101/450 | Loss: 1.5760\n",
      "[color] Batch 151/450 | Loss: 1.8266\n",
      "[quantity] Batch 201/450 | Loss: 1.7125\n",
      "[shape] Batch 251/450 | Loss: 1.7783\n",
      "[color] Batch 301/450 | Loss: 1.7003\n",
      "[quantity] Batch 351/450 | Loss: 1.7397\n",
      "[shape] Batch 401/450 | Loss: 1.7394\n",
      "[quantity] Batch 450/450 | Loss: 1.6796\n",
      "[Epoch 64] Train Loss: 1.6945 | Acc: 0.5318 | Perplexity: 5.4442\n",
      "[Epoch 64] Val Loss: 2.3688 | Acc: 0.2672 | Perplexity: 10.6847\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 65/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.7150\n",
      "[quantity] Batch 51/450 | Loss: 1.5823\n",
      "[shape] Batch 101/450 | Loss: 1.6634\n",
      "[color] Batch 151/450 | Loss: 1.6814\n",
      "[quantity] Batch 201/450 | Loss: 1.7589\n",
      "[shape] Batch 251/450 | Loss: 1.7092\n",
      "[color] Batch 301/450 | Loss: 1.8894\n",
      "[quantity] Batch 351/450 | Loss: 1.6525\n",
      "[shape] Batch 401/450 | Loss: 1.7722\n",
      "[quantity] Batch 450/450 | Loss: 1.6965\n",
      "[Epoch 65] Train Loss: 1.6859 | Acc: 0.5398 | Perplexity: 5.3975\n",
      "[Epoch 65] Val Loss: 2.3372 | Acc: 0.2677 | Perplexity: 10.3524\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 66/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.6881\n",
      "[quantity] Batch 51/450 | Loss: 1.6532\n",
      "[shape] Batch 101/450 | Loss: 1.6526\n",
      "[color] Batch 151/450 | Loss: 1.6432\n",
      "[quantity] Batch 201/450 | Loss: 1.7563\n",
      "[shape] Batch 251/450 | Loss: 1.8205\n",
      "[color] Batch 301/450 | Loss: 1.6679\n",
      "[quantity] Batch 351/450 | Loss: 1.7163\n",
      "[shape] Batch 401/450 | Loss: 1.7258\n",
      "[quantity] Batch 450/450 | Loss: 1.6824\n",
      "[Epoch 66] Train Loss: 1.7190 | Acc: 0.5230 | Perplexity: 5.5787\n",
      "[Epoch 66] Val Loss: 2.2805 | Acc: 0.2633 | Perplexity: 9.7813\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 67/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.6210\n",
      "[quantity] Batch 51/450 | Loss: 1.7573\n",
      "[shape] Batch 101/450 | Loss: 1.7229\n",
      "[color] Batch 151/450 | Loss: 1.7062\n",
      "[quantity] Batch 201/450 | Loss: 1.5840\n",
      "[shape] Batch 251/450 | Loss: 1.5705\n",
      "[color] Batch 301/450 | Loss: 1.6853\n",
      "[quantity] Batch 351/450 | Loss: 1.7091\n",
      "[shape] Batch 401/450 | Loss: 1.7884\n",
      "[quantity] Batch 450/450 | Loss: 1.7608\n",
      "[Epoch 67] Train Loss: 1.6998 | Acc: 0.5308 | Perplexity: 5.4727\n",
      "[Epoch 67] Val Loss: 2.3156 | Acc: 0.2642 | Perplexity: 10.1306\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 68/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.7060\n",
      "[quantity] Batch 51/450 | Loss: 1.6284\n",
      "[shape] Batch 101/450 | Loss: 1.7380\n",
      "[color] Batch 151/450 | Loss: 1.7638\n",
      "[quantity] Batch 201/450 | Loss: 1.7340\n",
      "[shape] Batch 251/450 | Loss: 1.6212\n",
      "[color] Batch 301/450 | Loss: 1.5487\n",
      "[quantity] Batch 351/450 | Loss: 1.7098\n",
      "[shape] Batch 401/450 | Loss: 1.5548\n",
      "[quantity] Batch 450/450 | Loss: 1.6279\n",
      "[Epoch 68] Train Loss: 1.6791 | Acc: 0.5432 | Perplexity: 5.3607\n",
      "[Epoch 68] Val Loss: 2.3246 | Acc: 0.2559 | Perplexity: 10.2227\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 69/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.7038\n",
      "[quantity] Batch 51/450 | Loss: 1.7707\n",
      "[shape] Batch 101/450 | Loss: 1.6753\n",
      "[color] Batch 151/450 | Loss: 1.7730\n",
      "[quantity] Batch 201/450 | Loss: 1.8386\n",
      "[shape] Batch 251/450 | Loss: 1.6436\n",
      "[color] Batch 301/450 | Loss: 1.7718\n",
      "[quantity] Batch 351/450 | Loss: 1.6234\n",
      "[shape] Batch 401/450 | Loss: 1.6869\n",
      "[quantity] Batch 450/450 | Loss: 1.5685\n",
      "[Epoch 69] Train Loss: 1.6694 | Acc: 0.5503 | Perplexity: 5.3090\n",
      "[Epoch 69] Val Loss: 2.3854 | Acc: 0.2633 | Perplexity: 10.8631\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 70/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.5494\n",
      "[quantity] Batch 51/450 | Loss: 1.6334\n",
      "[shape] Batch 101/450 | Loss: 1.5848\n",
      "[color] Batch 151/450 | Loss: 1.7330\n",
      "[quantity] Batch 201/450 | Loss: 1.8115\n",
      "[shape] Batch 251/450 | Loss: 1.5753\n",
      "[color] Batch 301/450 | Loss: 1.5563\n",
      "[quantity] Batch 351/450 | Loss: 1.5872\n",
      "[shape] Batch 401/450 | Loss: 1.7334\n",
      "[quantity] Batch 450/450 | Loss: 1.7052\n",
      "[Epoch 70] Train Loss: 1.6558 | Acc: 0.5617 | Perplexity: 5.2372\n",
      "[Epoch 70] Val Loss: 2.3523 | Acc: 0.2642 | Perplexity: 10.5102\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 71/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.5295\n",
      "[quantity] Batch 51/450 | Loss: 1.5299\n",
      "[shape] Batch 101/450 | Loss: 1.6114\n",
      "[color] Batch 151/450 | Loss: 1.5574\n",
      "[quantity] Batch 201/450 | Loss: 1.5540\n",
      "[shape] Batch 251/450 | Loss: 1.6510\n",
      "[color] Batch 301/450 | Loss: 1.6021\n",
      "[quantity] Batch 351/450 | Loss: 1.6531\n",
      "[shape] Batch 401/450 | Loss: 1.6293\n",
      "[quantity] Batch 450/450 | Loss: 1.6823\n",
      "[Epoch 71] Train Loss: 1.6508 | Acc: 0.5641 | Perplexity: 5.2112\n",
      "[Epoch 71] Val Loss: 2.3921 | Acc: 0.2677 | Perplexity: 10.9362\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 72/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.5693\n",
      "[quantity] Batch 51/450 | Loss: 1.6426\n",
      "[shape] Batch 101/450 | Loss: 1.6076\n",
      "[color] Batch 151/450 | Loss: 1.6906\n",
      "[quantity] Batch 201/450 | Loss: 1.6044\n",
      "[shape] Batch 251/450 | Loss: 1.5479\n",
      "[color] Batch 301/450 | Loss: 1.6107\n",
      "[quantity] Batch 351/450 | Loss: 1.8031\n",
      "[shape] Batch 401/450 | Loss: 1.6363\n",
      "[quantity] Batch 450/450 | Loss: 1.5275\n",
      "[Epoch 72] Train Loss: 1.6366 | Acc: 0.5705 | Perplexity: 5.1376\n",
      "[Epoch 72] Val Loss: 2.4273 | Acc: 0.2628 | Perplexity: 11.3281\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 73/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.5310\n",
      "[quantity] Batch 51/450 | Loss: 1.5890\n",
      "[shape] Batch 101/450 | Loss: 1.4380\n",
      "[color] Batch 151/450 | Loss: 1.7104\n",
      "[quantity] Batch 201/450 | Loss: 1.6542\n",
      "[shape] Batch 251/450 | Loss: 1.6195\n",
      "[color] Batch 301/450 | Loss: 1.7311\n",
      "[quantity] Batch 351/450 | Loss: 1.5105\n",
      "[shape] Batch 401/450 | Loss: 1.6852\n",
      "[quantity] Batch 450/450 | Loss: 1.7704\n",
      "[Epoch 73] Train Loss: 1.6312 | Acc: 0.5747 | Perplexity: 5.1098\n",
      "[Epoch 73] Val Loss: 2.4134 | Acc: 0.2642 | Perplexity: 11.1720\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 74/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.5297\n",
      "[quantity] Batch 51/450 | Loss: 1.7326\n",
      "[shape] Batch 101/450 | Loss: 1.6446\n",
      "[color] Batch 151/450 | Loss: 1.6924\n",
      "[quantity] Batch 201/450 | Loss: 1.4268\n",
      "[shape] Batch 251/450 | Loss: 1.4792\n",
      "[color] Batch 301/450 | Loss: 1.6755\n",
      "[quantity] Batch 351/450 | Loss: 1.7342\n",
      "[shape] Batch 401/450 | Loss: 1.6709\n",
      "[quantity] Batch 450/450 | Loss: 1.5602\n",
      "[Epoch 74] Train Loss: 1.6215 | Acc: 0.5803 | Perplexity: 5.0605\n",
      "[Epoch 74] Val Loss: 2.4079 | Acc: 0.2656 | Perplexity: 11.1101\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 75/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.5189\n",
      "[quantity] Batch 51/450 | Loss: 1.5475\n",
      "[shape] Batch 101/450 | Loss: 1.5754\n",
      "[color] Batch 151/450 | Loss: 1.5411\n",
      "[quantity] Batch 201/450 | Loss: 1.6386\n",
      "[shape] Batch 251/450 | Loss: 1.6470\n",
      "[color] Batch 301/450 | Loss: 1.6595\n",
      "[quantity] Batch 351/450 | Loss: 1.6379\n",
      "[shape] Batch 401/450 | Loss: 1.6295\n",
      "[quantity] Batch 450/450 | Loss: 1.5908\n",
      "[Epoch 75] Train Loss: 1.6109 | Acc: 0.5840 | Perplexity: 5.0073\n",
      "[Epoch 75] Val Loss: 2.4298 | Acc: 0.2650 | Perplexity: 11.3572\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 76/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.6411\n",
      "[quantity] Batch 51/450 | Loss: 1.5298\n",
      "[shape] Batch 101/450 | Loss: 1.5191\n",
      "[color] Batch 151/450 | Loss: 1.7696\n",
      "[quantity] Batch 201/450 | Loss: 1.5111\n",
      "[shape] Batch 251/450 | Loss: 1.5556\n",
      "[color] Batch 301/450 | Loss: 1.4940\n",
      "[quantity] Batch 351/450 | Loss: 1.4996\n",
      "[shape] Batch 401/450 | Loss: 1.5273\n",
      "[quantity] Batch 450/450 | Loss: 1.5800\n",
      "[Epoch 76] Train Loss: 1.5996 | Acc: 0.5938 | Perplexity: 4.9508\n",
      "[Epoch 76] Val Loss: 2.4749 | Acc: 0.2703 | Perplexity: 11.8809\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 77/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.5469\n",
      "[quantity] Batch 51/450 | Loss: 1.4338\n",
      "[shape] Batch 101/450 | Loss: 1.6351\n",
      "[color] Batch 151/450 | Loss: 1.5800\n",
      "[quantity] Batch 201/450 | Loss: 1.6070\n",
      "[shape] Batch 251/450 | Loss: 1.7608\n",
      "[color] Batch 301/450 | Loss: 1.8618\n",
      "[quantity] Batch 351/450 | Loss: 1.6810\n",
      "[shape] Batch 401/450 | Loss: 1.7034\n",
      "[quantity] Batch 450/450 | Loss: 1.6666\n",
      "[Epoch 77] Train Loss: 1.5935 | Acc: 0.5967 | Perplexity: 4.9209\n",
      "[Epoch 77] Val Loss: 2.4532 | Acc: 0.2697 | Perplexity: 11.6253\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 78/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.4677\n",
      "[quantity] Batch 51/450 | Loss: 1.5619\n",
      "[shape] Batch 101/450 | Loss: 1.5777\n",
      "[color] Batch 151/450 | Loss: 1.5060\n",
      "[quantity] Batch 201/450 | Loss: 1.6272\n",
      "[shape] Batch 251/450 | Loss: 1.4975\n",
      "[color] Batch 301/450 | Loss: 1.5116\n",
      "[quantity] Batch 351/450 | Loss: 1.5657\n",
      "[shape] Batch 401/450 | Loss: 1.7197\n",
      "[quantity] Batch 450/450 | Loss: 1.6182\n",
      "[Epoch 78] Train Loss: 1.5779 | Acc: 0.6069 | Perplexity: 4.8450\n",
      "[Epoch 78] Val Loss: 2.5083 | Acc: 0.2684 | Perplexity: 12.2844\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 79/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.5555\n",
      "[quantity] Batch 51/450 | Loss: 1.3499\n",
      "[shape] Batch 101/450 | Loss: 1.4521\n",
      "[color] Batch 151/450 | Loss: 1.6293\n",
      "[quantity] Batch 201/450 | Loss: 1.4948\n",
      "[shape] Batch 251/450 | Loss: 1.5606\n",
      "[color] Batch 301/450 | Loss: 1.4943\n",
      "[quantity] Batch 351/450 | Loss: 1.5061\n",
      "[shape] Batch 401/450 | Loss: 1.5904\n",
      "[quantity] Batch 450/450 | Loss: 1.6317\n",
      "[Epoch 79] Train Loss: 1.5743 | Acc: 0.6060 | Perplexity: 4.8276\n",
      "[Epoch 79] Val Loss: 2.4751 | Acc: 0.2739 | Perplexity: 11.8830\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 80/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.5240\n",
      "[quantity] Batch 51/450 | Loss: 1.6190\n",
      "[shape] Batch 101/450 | Loss: 1.7422\n",
      "[color] Batch 151/450 | Loss: 1.5463\n",
      "[quantity] Batch 201/450 | Loss: 1.5281\n",
      "[shape] Batch 251/450 | Loss: 1.6493\n",
      "[color] Batch 301/450 | Loss: 1.5514\n",
      "[quantity] Batch 351/450 | Loss: 1.5795\n",
      "[shape] Batch 401/450 | Loss: 1.4909\n",
      "[quantity] Batch 450/450 | Loss: 1.6053\n",
      "[Epoch 80] Train Loss: 1.5614 | Acc: 0.6156 | Perplexity: 4.7655\n",
      "[Epoch 80] Val Loss: 2.5061 | Acc: 0.2667 | Perplexity: 12.2575\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 81/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.4304\n",
      "[quantity] Batch 51/450 | Loss: 1.7814\n",
      "[shape] Batch 101/450 | Loss: 1.5104\n",
      "[color] Batch 151/450 | Loss: 1.4117\n",
      "[quantity] Batch 201/450 | Loss: 1.6185\n",
      "[shape] Batch 251/450 | Loss: 1.4943\n",
      "[color] Batch 301/450 | Loss: 1.6734\n",
      "[quantity] Batch 351/450 | Loss: 1.4287\n",
      "[shape] Batch 401/450 | Loss: 1.5897\n",
      "[quantity] Batch 450/450 | Loss: 1.4545\n",
      "[Epoch 81] Train Loss: 1.5559 | Acc: 0.6172 | Perplexity: 4.7392\n",
      "[Epoch 81] Val Loss: 2.5053 | Acc: 0.2739 | Perplexity: 12.2471\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 82/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.5471\n",
      "[quantity] Batch 51/450 | Loss: 1.5466\n",
      "[shape] Batch 101/450 | Loss: 1.6528\n",
      "[color] Batch 151/450 | Loss: 1.5172\n",
      "[quantity] Batch 201/450 | Loss: 1.6380\n",
      "[shape] Batch 251/450 | Loss: 1.5742\n",
      "[color] Batch 301/450 | Loss: 1.5513\n",
      "[quantity] Batch 351/450 | Loss: 1.7145\n",
      "[shape] Batch 401/450 | Loss: 1.4327\n",
      "[quantity] Batch 450/450 | Loss: 1.4708\n",
      "[Epoch 82] Train Loss: 1.5453 | Acc: 0.6256 | Perplexity: 4.6892\n",
      "[Epoch 82] Val Loss: 2.5441 | Acc: 0.2717 | Perplexity: 12.7318\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 83/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.3805\n",
      "[quantity] Batch 51/450 | Loss: 1.6238\n",
      "[shape] Batch 101/450 | Loss: 1.4978\n",
      "[color] Batch 151/450 | Loss: 1.5516\n",
      "[quantity] Batch 201/450 | Loss: 1.3859\n",
      "[shape] Batch 251/450 | Loss: 1.5710\n",
      "[color] Batch 301/450 | Loss: 1.5703\n",
      "[quantity] Batch 351/450 | Loss: 1.6380\n",
      "[shape] Batch 401/450 | Loss: 1.4924\n",
      "[quantity] Batch 450/450 | Loss: 1.5073\n",
      "[Epoch 83] Train Loss: 1.5346 | Acc: 0.6303 | Perplexity: 4.6393\n",
      "[Epoch 83] Val Loss: 2.5872 | Acc: 0.2725 | Perplexity: 13.2927\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 84/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.2955\n",
      "[quantity] Batch 51/450 | Loss: 1.4073\n",
      "[shape] Batch 101/450 | Loss: 1.5797\n",
      "[color] Batch 151/450 | Loss: 1.2859\n",
      "[quantity] Batch 201/450 | Loss: 1.3746\n",
      "[shape] Batch 251/450 | Loss: 1.4325\n",
      "[color] Batch 301/450 | Loss: 1.4810\n",
      "[quantity] Batch 351/450 | Loss: 1.5431\n",
      "[shape] Batch 401/450 | Loss: 1.7398\n",
      "[quantity] Batch 450/450 | Loss: 1.8066\n",
      "[Epoch 84] Train Loss: 1.5200 | Acc: 0.6395 | Perplexity: 4.5724\n",
      "[Epoch 84] Val Loss: 2.5593 | Acc: 0.2731 | Perplexity: 12.9265\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 85/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.4072\n",
      "[quantity] Batch 51/450 | Loss: 1.4674\n",
      "[shape] Batch 101/450 | Loss: 1.4808\n",
      "[color] Batch 151/450 | Loss: 1.4617\n",
      "[quantity] Batch 201/450 | Loss: 1.4405\n",
      "[shape] Batch 251/450 | Loss: 1.6153\n",
      "[color] Batch 301/450 | Loss: 1.5207\n",
      "[quantity] Batch 351/450 | Loss: 1.6323\n",
      "[shape] Batch 401/450 | Loss: 1.4506\n",
      "[quantity] Batch 450/450 | Loss: 1.6198\n",
      "[Epoch 85] Train Loss: 1.5174 | Acc: 0.6393 | Perplexity: 4.5602\n",
      "[Epoch 85] Val Loss: 2.5535 | Acc: 0.2695 | Perplexity: 12.8524\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 86/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.5168\n",
      "[quantity] Batch 51/450 | Loss: 1.5335\n",
      "[shape] Batch 101/450 | Loss: 1.4833\n",
      "[color] Batch 151/450 | Loss: 1.5608\n",
      "[quantity] Batch 201/450 | Loss: 1.4948\n",
      "[shape] Batch 251/450 | Loss: 1.5576\n",
      "[color] Batch 301/450 | Loss: 1.4518\n",
      "[quantity] Batch 351/450 | Loss: 1.4948\n",
      "[shape] Batch 401/450 | Loss: 1.4824\n",
      "[quantity] Batch 450/450 | Loss: 1.4602\n",
      "[Epoch 86] Train Loss: 1.5074 | Acc: 0.6419 | Perplexity: 4.5150\n",
      "[Epoch 86] Val Loss: 2.6232 | Acc: 0.2762 | Perplexity: 13.7795\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 87/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.4881\n",
      "[quantity] Batch 51/450 | Loss: 1.3624\n",
      "[shape] Batch 101/450 | Loss: 1.3925\n",
      "[color] Batch 151/450 | Loss: 1.5202\n",
      "[quantity] Batch 201/450 | Loss: 1.5422\n",
      "[shape] Batch 251/450 | Loss: 1.4208\n",
      "[color] Batch 301/450 | Loss: 1.5281\n",
      "[quantity] Batch 351/450 | Loss: 1.5644\n",
      "[shape] Batch 401/450 | Loss: 1.4949\n",
      "[quantity] Batch 450/450 | Loss: 1.4805\n",
      "[Epoch 87] Train Loss: 1.4883 | Acc: 0.6542 | Perplexity: 4.4297\n",
      "[Epoch 87] Val Loss: 2.6029 | Acc: 0.2716 | Perplexity: 13.5023\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 88/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.3812\n",
      "[quantity] Batch 51/450 | Loss: 1.4525\n",
      "[shape] Batch 101/450 | Loss: 1.4965\n",
      "[color] Batch 151/450 | Loss: 1.4099\n",
      "[quantity] Batch 201/450 | Loss: 1.4704\n",
      "[shape] Batch 251/450 | Loss: 1.5393\n",
      "[color] Batch 301/450 | Loss: 1.5213\n",
      "[quantity] Batch 351/450 | Loss: 1.4680\n",
      "[shape] Batch 401/450 | Loss: 1.6294\n",
      "[quantity] Batch 450/450 | Loss: 1.4492\n",
      "[Epoch 88] Train Loss: 1.4893 | Acc: 0.6525 | Perplexity: 4.4340\n",
      "[Epoch 88] Val Loss: 2.6666 | Acc: 0.2769 | Perplexity: 14.3914\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 89/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.3379\n",
      "[quantity] Batch 51/450 | Loss: 1.5343\n",
      "[shape] Batch 101/450 | Loss: 1.5096\n",
      "[color] Batch 151/450 | Loss: 1.4458\n",
      "[quantity] Batch 201/450 | Loss: 1.4965\n",
      "[shape] Batch 251/450 | Loss: 1.5165\n",
      "[color] Batch 301/450 | Loss: 1.5734\n",
      "[quantity] Batch 351/450 | Loss: 1.3332\n",
      "[shape] Batch 401/450 | Loss: 1.4279\n",
      "[quantity] Batch 450/450 | Loss: 1.4861\n",
      "[Epoch 89] Train Loss: 1.4830 | Acc: 0.6572 | Perplexity: 4.4061\n",
      "[Epoch 89] Val Loss: 2.6543 | Acc: 0.2767 | Perplexity: 14.2155\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 90/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.3070\n",
      "[quantity] Batch 51/450 | Loss: 1.3708\n",
      "[shape] Batch 101/450 | Loss: 1.3263\n",
      "[color] Batch 151/450 | Loss: 1.3822\n",
      "[quantity] Batch 201/450 | Loss: 1.5551\n",
      "[shape] Batch 251/450 | Loss: 1.4850\n",
      "[color] Batch 301/450 | Loss: 1.4708\n",
      "[quantity] Batch 351/450 | Loss: 1.6598\n",
      "[shape] Batch 401/450 | Loss: 1.4349\n",
      "[quantity] Batch 450/450 | Loss: 1.4803\n",
      "[Epoch 90] Train Loss: 1.4669 | Acc: 0.6670 | Perplexity: 4.3360\n",
      "[Epoch 90] Val Loss: 2.6552 | Acc: 0.2761 | Perplexity: 14.2277\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 91/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.3309\n",
      "[quantity] Batch 51/450 | Loss: 1.4391\n",
      "[shape] Batch 101/450 | Loss: 1.3859\n",
      "[color] Batch 151/450 | Loss: 1.3760\n",
      "[quantity] Batch 201/450 | Loss: 1.3492\n",
      "[shape] Batch 251/450 | Loss: 1.4018\n",
      "[color] Batch 301/450 | Loss: 1.6148\n",
      "[quantity] Batch 351/450 | Loss: 1.4551\n",
      "[shape] Batch 401/450 | Loss: 1.5969\n",
      "[quantity] Batch 450/450 | Loss: 1.4469\n",
      "[Epoch 91] Train Loss: 1.4546 | Acc: 0.6724 | Perplexity: 4.2827\n",
      "[Epoch 91] Val Loss: 2.7288 | Acc: 0.2731 | Perplexity: 15.3140\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 92/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.4630\n",
      "[quantity] Batch 51/450 | Loss: 1.4423\n",
      "[shape] Batch 101/450 | Loss: 1.3468\n",
      "[color] Batch 151/450 | Loss: 1.4261\n",
      "[quantity] Batch 201/450 | Loss: 1.4621\n",
      "[shape] Batch 251/450 | Loss: 1.3593\n",
      "[color] Batch 301/450 | Loss: 1.3614\n",
      "[quantity] Batch 351/450 | Loss: 1.5201\n",
      "[shape] Batch 401/450 | Loss: 1.5261\n",
      "[quantity] Batch 450/450 | Loss: 1.5497\n",
      "[Epoch 92] Train Loss: 1.4523 | Acc: 0.6741 | Perplexity: 4.2730\n",
      "[Epoch 92] Val Loss: 2.6801 | Acc: 0.2670 | Perplexity: 14.5869\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 93/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.3288\n",
      "[quantity] Batch 51/450 | Loss: 1.3238\n",
      "[shape] Batch 101/450 | Loss: 1.3447\n",
      "[color] Batch 151/450 | Loss: 1.6040\n",
      "[quantity] Batch 201/450 | Loss: 1.4470\n",
      "[shape] Batch 251/450 | Loss: 1.4474\n",
      "[color] Batch 301/450 | Loss: 1.4149\n",
      "[quantity] Batch 351/450 | Loss: 1.6340\n",
      "[shape] Batch 401/450 | Loss: 1.5827\n",
      "[quantity] Batch 450/450 | Loss: 1.4551\n",
      "[Epoch 93] Train Loss: 1.4393 | Acc: 0.6826 | Perplexity: 4.2177\n",
      "[Epoch 93] Val Loss: 2.6528 | Acc: 0.2737 | Perplexity: 14.1940\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 94/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.3003\n",
      "[quantity] Batch 51/450 | Loss: 1.5793\n",
      "[shape] Batch 101/450 | Loss: 1.3470\n",
      "[color] Batch 151/450 | Loss: 1.3590\n",
      "[quantity] Batch 201/450 | Loss: 1.5105\n",
      "[shape] Batch 251/450 | Loss: 1.4155\n",
      "[color] Batch 301/450 | Loss: 1.4118\n",
      "[quantity] Batch 351/450 | Loss: 1.5003\n",
      "[shape] Batch 401/450 | Loss: 1.5997\n",
      "[quantity] Batch 450/450 | Loss: 1.4528\n",
      "[Epoch 94] Train Loss: 1.4354 | Acc: 0.6851 | Perplexity: 4.2015\n",
      "[Epoch 94] Val Loss: 2.7290 | Acc: 0.2769 | Perplexity: 15.3173\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 95/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.3589\n",
      "[quantity] Batch 51/450 | Loss: 1.3077\n",
      "[shape] Batch 101/450 | Loss: 1.4991\n",
      "[color] Batch 151/450 | Loss: 1.4987\n",
      "[quantity] Batch 201/450 | Loss: 1.3928\n",
      "[shape] Batch 251/450 | Loss: 1.4319\n",
      "[color] Batch 301/450 | Loss: 1.4977\n",
      "[quantity] Batch 351/450 | Loss: 1.4958\n",
      "[shape] Batch 401/450 | Loss: 1.5302\n",
      "[quantity] Batch 450/450 | Loss: 1.4542\n",
      "[Epoch 95] Train Loss: 1.4243 | Acc: 0.6876 | Perplexity: 4.1551\n",
      "[Epoch 95] Val Loss: 2.7420 | Acc: 0.2834 | Perplexity: 15.5176\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 96/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.3651\n",
      "[quantity] Batch 51/450 | Loss: 1.4552\n",
      "[shape] Batch 101/450 | Loss: 1.5379\n",
      "[color] Batch 151/450 | Loss: 1.3971\n",
      "[quantity] Batch 201/450 | Loss: 1.2746\n",
      "[shape] Batch 251/450 | Loss: 1.4714\n",
      "[color] Batch 301/450 | Loss: 1.5420\n",
      "[quantity] Batch 351/450 | Loss: 1.4085\n",
      "[shape] Batch 401/450 | Loss: 1.5457\n",
      "[quantity] Batch 450/450 | Loss: 1.5000\n",
      "[Epoch 96] Train Loss: 1.4213 | Acc: 0.6899 | Perplexity: 4.1427\n",
      "[Epoch 96] Val Loss: 2.7238 | Acc: 0.2808 | Perplexity: 15.2383\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 97/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.4034\n",
      "[quantity] Batch 51/450 | Loss: 1.5716\n",
      "[shape] Batch 101/450 | Loss: 1.3855\n",
      "[color] Batch 151/450 | Loss: 1.2905\n",
      "[quantity] Batch 201/450 | Loss: 1.2436\n",
      "[shape] Batch 251/450 | Loss: 1.4422\n",
      "[color] Batch 301/450 | Loss: 1.4799\n",
      "[quantity] Batch 351/450 | Loss: 1.3418\n",
      "[shape] Batch 401/450 | Loss: 1.4197\n",
      "[quantity] Batch 450/450 | Loss: 1.5806\n",
      "[Epoch 97] Train Loss: 1.4037 | Acc: 0.7005 | Perplexity: 4.0701\n",
      "[Epoch 97] Val Loss: 2.7389 | Acc: 0.2817 | Perplexity: 15.4703\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 98/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.3841\n",
      "[quantity] Batch 51/450 | Loss: 1.3458\n",
      "[shape] Batch 101/450 | Loss: 1.2405\n",
      "[color] Batch 151/450 | Loss: 1.3074\n",
      "[quantity] Batch 201/450 | Loss: 1.3929\n",
      "[shape] Batch 251/450 | Loss: 1.4814\n",
      "[color] Batch 301/450 | Loss: 1.3749\n",
      "[quantity] Batch 351/450 | Loss: 1.2892\n",
      "[shape] Batch 401/450 | Loss: 1.3982\n",
      "[quantity] Batch 450/450 | Loss: 1.2713\n",
      "[Epoch 98] Train Loss: 1.3945 | Acc: 0.7042 | Perplexity: 4.0328\n",
      "[Epoch 98] Val Loss: 2.8784 | Acc: 0.2809 | Perplexity: 17.7866\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 99/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.2830\n",
      "[quantity] Batch 51/450 | Loss: 1.2973\n",
      "[shape] Batch 101/450 | Loss: 1.4711\n",
      "[color] Batch 151/450 | Loss: 1.2763\n",
      "[quantity] Batch 201/450 | Loss: 1.3013\n",
      "[shape] Batch 251/450 | Loss: 1.3551\n",
      "[color] Batch 301/450 | Loss: 1.4502\n",
      "[quantity] Batch 351/450 | Loss: 1.4743\n",
      "[shape] Batch 401/450 | Loss: 1.4905\n",
      "[quantity] Batch 450/450 | Loss: 1.5329\n",
      "[Epoch 99] Train Loss: 1.3945 | Acc: 0.7046 | Perplexity: 4.0329\n",
      "[Epoch 99] Val Loss: 2.7484 | Acc: 0.2823 | Perplexity: 15.6170\n",
      "------------------------------------------------------------\n",
      "\n",
      "[Epoch 100/100]\n",
      "------------------------------------------------------------\n",
      "[color] Batch 1/450 | Loss: 1.3361\n",
      "[quantity] Batch 51/450 | Loss: 1.3813\n",
      "[shape] Batch 101/450 | Loss: 1.5237\n",
      "[color] Batch 151/450 | Loss: 1.5048\n",
      "[quantity] Batch 201/450 | Loss: 1.5244\n",
      "[shape] Batch 251/450 | Loss: 1.3583\n",
      "[color] Batch 301/450 | Loss: 1.2390\n",
      "[quantity] Batch 351/450 | Loss: 1.3757\n",
      "[shape] Batch 401/450 | Loss: 1.3323\n",
      "[quantity] Batch 450/450 | Loss: 1.4853\n",
      "[Epoch 100] Train Loss: 1.3829 | Acc: 0.7121 | Perplexity: 3.9862\n",
      "[Epoch 100] Val Loss: 2.7846 | Acc: 0.2822 | Perplexity: 16.1937\n",
      "------------------------------------------------------------\n",
      "\n",
      " Training complete (Round-Robin Mode)\n"
     ]
    }
   ],
   "source": [
    "criterion =  nn.CrossEntropyLoss(label_smoothing=LABEL_SMOOTHING)\n",
    "optimizer = optim.AdamW(transformer.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=WARMUP_STEPS)  \n",
    "\n",
    "results = train_model_round_robin(\n",
    "    train_loaders, validation_loader, transformer, criterion, \n",
    "    optimizer, max_epochs=MAX_EPOCHS, device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917a1fa4",
   "metadata": {},
   "source": [
    "### 3. Test Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d144441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.7677 | Acc: 0.2711 | Perplexity: 15.9220\n"
     ]
    }
   ],
   "source": [
    "test_results = test_model(test_loader, transformer, criterion, device)\n",
    "print(f\"Test Loss: {test_results[\"test_loss\"]:.4f} | Acc: {test_results[\"test_acc\"]:.4f} | Perplexity: {test_results[\"test_perplexity\"]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd64748",
   "metadata": {},
   "source": [
    "### 4. Save Training & Test Results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "77ce1186",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ab7906b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results = {**results, **test_results}\n",
    "\n",
    "with open(f\"./../results/baseline_transformer_performace.json\", \"w\") as file:\n",
    "    json.dump(final_results, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28dac7b",
   "metadata": {},
   "source": [
    "## Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c3c7ecc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.wcst import WCST\n",
    "wcst = WCST(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b590910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inference(model: nn.Module, source_sequence, start_tokens):\n",
    "    model.eval()\n",
    "    generated = start_tokens\n",
    "    \n",
    "    with T.no_grad():\n",
    "        full_sequence = T.cat([source_sequence, generated], dim=1)\n",
    "        logits = model(full_sequence)\n",
    "    \n",
    "    # Greedy Selection\n",
    "    next_token = T.argmax(logits[:, -1, :], dim=-1, keepdim=True)\n",
    "\n",
    "    generated = T.cat([generated, next_token], dim=1)\n",
    "\n",
    "    return generated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7f458678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Actual Trials\n",
      "[array(['red', 'cross', '2'], dtype='<U6'), array(['blue', 'star', '1'], dtype='<U6'), array(['yellow', 'square', '3'], dtype='<U6'), array(['yellow', 'circle', '4'], dtype='<U6'), array(['green', 'square', '4'], dtype='<U6'), 'SEP', 'C4', 'EOS', array(['red', 'square', '4'], dtype='<U6'), 'SEP', 'C4']\n",
      "[array(['blue', 'star', '1'], dtype='<U6'), array(['green', 'square', '2'], dtype='<U6'), array(['red', 'cross', '4'], dtype='<U6'), array(['green', 'star', '3'], dtype='<U6'), array(['blue', 'circle', '1'], dtype='<U6'), 'SEP', 'C1', 'EOS', array(['red', 'cross', '1'], dtype='<U6'), 'SEP', 'C1']\n",
      "[array(['blue', 'star', '1'], dtype='<U6'), array(['yellow', 'square', '2'], dtype='<U6'), array(['yellow', 'cross', '3'], dtype='<U6'), array(['yellow', 'square', '4'], dtype='<U6'), array(['red', 'star', '2'], dtype='<U6'), 'SEP', 'C2', 'EOS', array(['blue', 'cross', '4'], dtype='<U6'), 'SEP', 'C4']\n",
      "[array(['green', 'square', '1'], dtype='<U6'), array(['yellow', 'star', '4'], dtype='<U6'), array(['yellow', 'circle', '2'], dtype='<U6'), array(['blue', 'circle', '3'], dtype='<U6'), array(['red', 'circle', '3'], dtype='<U6'), 'SEP', 'C4', 'EOS', array(['green', 'square', '4'], dtype='<U6'), 'SEP', 'C2']\n",
      "[array(['yellow', 'star', '2'], dtype='<U6'), array(['red', 'square', '1'], dtype='<U6'), array(['yellow', 'star', '4'], dtype='<U6'), array(['yellow', 'square', '3'], dtype='<U6'), array(['blue', 'cross', '3'], dtype='<U6'), 'SEP', 'C4', 'EOS', array(['yellow', 'circle', '4'], dtype='<U6'), 'SEP', 'C3']\n",
      "[array(['red', 'circle', '3'], dtype='<U6'), array(['green', 'square', '2'], dtype='<U6'), array(['blue', 'cross', '1'], dtype='<U6'), array(['green', 'cross', '4'], dtype='<U6'), array(['yellow', 'cross', '2'], dtype='<U6'), 'SEP', 'C2', 'EOS', array(['blue', 'square', '1'], dtype='<U6'), 'SEP', 'C3']\n",
      "[array(['red', 'square', '4'], dtype='<U6'), array(['green', 'circle', '2'], dtype='<U6'), array(['red', 'square', '1'], dtype='<U6'), array(['yellow', 'cross', '3'], dtype='<U6'), array(['yellow', 'circle', '4'], dtype='<U6'), 'SEP', 'C1', 'EOS', array(['yellow', 'circle', '3'], dtype='<U6'), 'SEP', 'C4']\n",
      "[array(['green', 'cross', '4'], dtype='<U6'), array(['blue', 'circle', '1'], dtype='<U6'), array(['yellow', 'circle', '3'], dtype='<U6'), array(['green', 'circle', '2'], dtype='<U6'), array(['blue', 'star', '3'], dtype='<U6'), 'SEP', 'C3', 'EOS', array(['red', 'star', '4'], dtype='<U6'), 'SEP', 'C1']\n",
      "[array(['green', 'square', '2'], dtype='<U6'), array(['blue', 'circle', '4'], dtype='<U6'), array(['yellow', 'star', '1'], dtype='<U6'), array(['green', 'cross', '3'], dtype='<U6'), array(['blue', 'star', '1'], dtype='<U6'), 'SEP', 'C3', 'EOS', array(['red', 'circle', '2'], dtype='<U6'), 'SEP', 'C1']\n",
      "[array(['red', 'star', '1'], dtype='<U6'), array(['blue', 'cross', '4'], dtype='<U6'), array(['red', 'circle', '2'], dtype='<U6'), array(['yellow', 'square', '3'], dtype='<U6'), array(['blue', 'square', '1'], dtype='<U6'), 'SEP', 'C1', 'EOS', array(['blue', 'star', '2'], dtype='<U6'), 'SEP', 'C3']\n",
      "Feature for Classification:  1 \n",
      "\n",
      "# Predicted Trials\n",
      "[array(['red', 'cross', '2'], dtype='<U6'), array(['blue', 'star', '1'], dtype='<U6'), array(['yellow', 'square', '3'], dtype='<U6'), array(['yellow', 'circle', '4'], dtype='<U6'), array(['green', 'square', '4'], dtype='<U6'), 'SEP', 'C4', 'EOS', array(['red', 'square', '4'], dtype='<U6'), 'SEP', array(['red', 'square', '4'], dtype='<U6'), 'SEP', 'C4']\n",
      "[array(['blue', 'star', '1'], dtype='<U6'), array(['green', 'square', '2'], dtype='<U6'), array(['red', 'cross', '4'], dtype='<U6'), array(['green', 'star', '3'], dtype='<U6'), array(['blue', 'circle', '1'], dtype='<U6'), 'SEP', 'C1', 'EOS', array(['red', 'cross', '1'], dtype='<U6'), 'SEP', array(['red', 'cross', '1'], dtype='<U6'), 'SEP', 'C3']\n",
      "[array(['blue', 'star', '1'], dtype='<U6'), array(['yellow', 'square', '2'], dtype='<U6'), array(['yellow', 'cross', '3'], dtype='<U6'), array(['yellow', 'square', '4'], dtype='<U6'), array(['red', 'star', '2'], dtype='<U6'), 'SEP', 'C2', 'EOS', array(['blue', 'cross', '4'], dtype='<U6'), 'SEP', array(['blue', 'cross', '4'], dtype='<U6'), 'SEP', 'C4']\n",
      "[array(['green', 'square', '1'], dtype='<U6'), array(['yellow', 'star', '4'], dtype='<U6'), array(['yellow', 'circle', '2'], dtype='<U6'), array(['blue', 'circle', '3'], dtype='<U6'), array(['red', 'circle', '3'], dtype='<U6'), 'SEP', 'C4', 'EOS', array(['green', 'square', '4'], dtype='<U6'), 'SEP', array(['green', 'square', '4'], dtype='<U6'), 'SEP', 'C2']\n",
      "[array(['yellow', 'star', '2'], dtype='<U6'), array(['red', 'square', '1'], dtype='<U6'), array(['yellow', 'star', '4'], dtype='<U6'), array(['yellow', 'square', '3'], dtype='<U6'), array(['blue', 'cross', '3'], dtype='<U6'), 'SEP', 'C4', 'EOS', array(['yellow', 'circle', '4'], dtype='<U6'), 'SEP', array(['yellow', 'circle', '4'], dtype='<U6'), 'SEP', 'C3']\n",
      "[array(['red', 'circle', '3'], dtype='<U6'), array(['green', 'square', '2'], dtype='<U6'), array(['blue', 'cross', '1'], dtype='<U6'), array(['green', 'cross', '4'], dtype='<U6'), array(['yellow', 'cross', '2'], dtype='<U6'), 'SEP', 'C2', 'EOS', array(['blue', 'square', '1'], dtype='<U6'), 'SEP', array(['blue', 'square', '1'], dtype='<U6'), 'SEP', 'C3']\n",
      "[array(['red', 'square', '4'], dtype='<U6'), array(['green', 'circle', '2'], dtype='<U6'), array(['red', 'square', '1'], dtype='<U6'), array(['yellow', 'cross', '3'], dtype='<U6'), array(['yellow', 'circle', '4'], dtype='<U6'), 'SEP', 'C1', 'EOS', array(['yellow', 'circle', '3'], dtype='<U6'), 'SEP', array(['yellow', 'circle', '3'], dtype='<U6'), 'SEP', 'C2']\n",
      "[array(['green', 'cross', '4'], dtype='<U6'), array(['blue', 'circle', '1'], dtype='<U6'), array(['yellow', 'circle', '3'], dtype='<U6'), array(['green', 'circle', '2'], dtype='<U6'), array(['blue', 'star', '3'], dtype='<U6'), 'SEP', 'C3', 'EOS', array(['red', 'star', '4'], dtype='<U6'), 'SEP', array(['red', 'star', '4'], dtype='<U6'), 'SEP', 'C1']\n",
      "[array(['green', 'square', '2'], dtype='<U6'), array(['blue', 'circle', '4'], dtype='<U6'), array(['yellow', 'star', '1'], dtype='<U6'), array(['green', 'cross', '3'], dtype='<U6'), array(['blue', 'star', '1'], dtype='<U6'), 'SEP', 'C3', 'EOS', array(['red', 'circle', '2'], dtype='<U6'), 'SEP', array(['red', 'circle', '2'], dtype='<U6'), 'SEP', 'C2']\n",
      "[array(['red', 'star', '1'], dtype='<U6'), array(['blue', 'cross', '4'], dtype='<U6'), array(['red', 'circle', '2'], dtype='<U6'), array(['yellow', 'square', '3'], dtype='<U6'), array(['blue', 'square', '1'], dtype='<U6'), 'SEP', 'C1', 'EOS', array(['blue', 'star', '2'], dtype='<U6'), 'SEP', array(['blue', 'star', '2'], dtype='<U6'), 'SEP', 'C3']\n",
      "Feature for Classification:  1 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_seq, target = train_datasets[\"quantity\"][:10]\n",
    "encoder_input = encoder_input.to(device)\n",
    "decoder_input = decoder_input.to(device)\n",
    "target = target.to(device)\n",
    "\n",
    "prediction = model_inference(transformer, encoder_input, decoder_input)\n",
    "\n",
    "print(\"# Actual Trials\")\n",
    "test_batch = [np.asarray(item.cpu()) for item in [encoder_input, T.concatenate([decoder_input, target], dim=1)]]\n",
    "output = wcst.visualise_batch(test_batch)\n",
    "\n",
    "print(\"# Predicted Trials\")\n",
    "prediction_batch = [np.asarray(item.cpu()) for item in [encoder_input, T.concatenate([decoder_input, prediction], dim=1)]]\n",
    "output = wcst.visualise_batch(prediction_batch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
