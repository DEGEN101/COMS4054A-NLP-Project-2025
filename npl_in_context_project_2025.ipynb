{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd555848",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "389721a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = T.device('cuda' if T.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08945224",
   "metadata": {},
   "source": [
    "## Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25ea2ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[17. 21. 42. 46. 20. 68. 65. 69.]\n",
      " [18. 20. 59. 62. 41. 68. 66. 69.]\n",
      " [27.  6. 17. 63. 38. 68. 65. 69.]] [[ 4. 68. 65.]\n",
      " [ 4. 68. 65.]\n",
      " [58. 68. 64.]]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "\n",
    "from wcst import WCST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d2bc29",
   "metadata": {},
   "source": [
    "### 1. Dataset Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b73743b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "BATCHES_PER_CONTEXT = 20\n",
    "N_CONTEXT_SWITCHES = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e47ffd5",
   "metadata": {},
   "source": [
    "### 2. Creating Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "365835c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "wcst = WCST(BATCH_SIZE)\n",
    "data = []\n",
    "targets = []\n",
    "\n",
    "for i in range(N_CONTEXT_SWITCHES):\n",
    "    for j in range(BATCHES_PER_CONTEXT):\n",
    "        X, t = next(wcst.gen_batch())\n",
    "        for k in range(BATCH_SIZE):\n",
    "            data.append(X[k])\n",
    "            targets.append(t[k])\n",
    "    wcst.context_switch()\n",
    "\n",
    "data = T.tensor(np.array(data), dtype=T.long).to(device=device)\n",
    "targets = T.tensor(np.array(targets), dtype=T.long).to(device=device)\n",
    "dataset = TensorDataset(data, targets)\n",
    "\n",
    "train_dataset, validation_dataset, test_dataset = random_split(\n",
    "    dataset, [0.6, 0.2, 0.2]\n",
    ")\n",
    "\n",
    "train_dataset_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "validation_dataset_loader = DataLoader(validation_dataset, batch_size=BATCH_SIZE)\n",
    "test_dataset_loader  = DataLoader(test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f524267",
   "metadata": {},
   "source": [
    "## Transformer Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3a8bc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from toy_transformer import Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7739c3fe",
   "metadata": {},
   "source": [
    "### 1. Transformer Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "606c1008",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCABULARY_SIZE = len(wcst.card_indices) + len(wcst.categories) + 2 # Cards + Categories + 'SEP' + 'EOS'\n",
    "EMBEDDING_SIZE = 32\n",
    "N_ATTENTION_HEADS = 4\n",
    "N_BLOCKS = 3\n",
    "MAX_SEQUENCE_LENGTH = 10\n",
    "FF_DIMS = 64\n",
    "DROPOUT_PROB = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab9fc96",
   "metadata": {},
   "source": [
    "### 2. Transformer Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f25ee98",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    VOCABULARY_SIZE, VOCABULARY_SIZE, EMBEDDING_SIZE, N_ATTENTION_HEADS,\n",
    "    N_BLOCKS, MAX_SEQUENCE_LENGTH, FF_DIMS, DROPOUT_PROB, device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7489333",
   "metadata": {},
   "source": [
    "## Training Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42f3ebdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c5d23d",
   "metadata": {},
   "source": [
    "### 1. Train, Validate, Evaluate Model Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a8ee8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    train_loader: DataLoader, validation_loader: DataLoader, model: Transformer, criterion: nn.CrossEntropyLoss, \n",
    "    optimizer: optim.Optimizer, max_epochs: int = 20, device: str | T.device = \"cpu\", patience: int = 3\n",
    "    ):\n",
    "    \n",
    "    best_val_loss = np.inf\n",
    "    patience_counter = 0\n",
    "\n",
    "    train_losses, train_accs = [], []\n",
    "    test_losses, test_accs = [], []\n",
    "    best_model_state = model.state_dict()\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{max_epochs}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "        # --- Training ---\n",
    "        model.train()\n",
    "        epoch_train_losses = []\n",
    "        total_correct = 0\n",
    "        total_tokens = 0\n",
    "\n",
    "        for batch_idx, (X, target) in enumerate(train_loader):\n",
    "            encoder_input, target = X.to(device), target.to(device)\n",
    "\n",
    "            # Decoder inputs/targets (shifted)\n",
    "            decoder_input = target[:, :-1]\n",
    "            decoder_target = target[:, 1:].reshape(-1)\n",
    "\n",
    "            # Forward pass\n",
    "            logits = model(encoder_input, decoder_input).reshape(-1, VOCABULARY_SIZE)\n",
    "            loss = criterion(logits, decoder_target)\n",
    "\n",
    "            # Backprop\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accuracy\n",
    "            preds = logits.argmax(dim=1)\n",
    "            total_correct += (preds == decoder_target).sum().item()\n",
    "            total_tokens += decoder_target.size(0)\n",
    "\n",
    "            epoch_train_losses.append(loss.item())\n",
    "\n",
    "            if batch_idx % 10 == 0 or batch_idx == len(train_loader) - 1:\n",
    "                print(f\"Train Batch {batch_idx+1}/{len(train_loader)} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "        train_loss = np.mean(epoch_train_losses)\n",
    "        train_acc = total_correct / total_tokens\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "\n",
    "        print(f\"[Epoch {epoch+1}] Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "\n",
    "        # --- Validation ---\n",
    "        model.eval()\n",
    "        test_batch_losses = []\n",
    "        test_correct = 0\n",
    "        test_tokens = 0\n",
    "\n",
    "        with T.no_grad():\n",
    "            for X, target in validation_loader:\n",
    "                encoder_input, target = X.to(device), target.to(device)\n",
    "                decoder_input = target[:, :-1]\n",
    "                decoder_target = target[:, 1:].reshape(-1)\n",
    "\n",
    "                logits = model(encoder_input, decoder_input).reshape(-1, VOCABULARY_SIZE)\n",
    "                loss = criterion(logits, decoder_target)\n",
    "\n",
    "                preds = logits.argmax(dim=1)\n",
    "                test_correct += (preds == decoder_target).sum().item()\n",
    "                test_tokens += decoder_target.size(0)\n",
    "\n",
    "                test_batch_losses.append(loss.item())\n",
    "\n",
    "        val_loss = np.mean(test_batch_losses)\n",
    "        val_acc = test_correct / test_tokens\n",
    "        test_losses.append(val_loss)\n",
    "        test_accs.append(val_acc)\n",
    "\n",
    "        print(f\"[Epoch {epoch+1}] Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "        # --- Early Stopping ---\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            best_model_state = model.state_dict()\n",
    "            print(f\"Validation loss improved — saving model (Loss: {val_loss:.4f})\")\n",
    "        # else:\n",
    "        #     patience_counter += 1\n",
    "        #     print(f\"No improvement ({patience_counter}/{patience})\")\n",
    "        #     if patience_counter >= patience:\n",
    "        #         print(\"\\nEarly stopping triggered. Restoring best model.\")\n",
    "        #         model.load_state_dict(best_model_state)\n",
    "        #         break\n",
    "\n",
    "    print(\"\\nTraining complete\")\n",
    "\n",
    "    return {\n",
    "        \"train_losses\": train_losses,\n",
    "        \"train_accs\": train_accs,\n",
    "        \"test_losses\": test_losses,\n",
    "        \"test_accs\": test_accs,\n",
    "        \"best_val_loss\": best_val_loss,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44864dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(test_loader: DataLoader, model: Transformer, criterion: nn.CrossEntropyLoss, device: str | T.device = \"cpu\"):\n",
    "    test_losses, test_accs = [], []\n",
    "\n",
    "    model.eval()\n",
    "    test_batch_losses = []\n",
    "    test_correct = 0\n",
    "    test_tokens = 0\n",
    "\n",
    "    with T.no_grad():\n",
    "        for X, target in test_loader:\n",
    "            encoder_input, target = X.to(device), target.to(device)\n",
    "            decoder_input = target[:, :-1]\n",
    "            decoder_target = target[:, 1:].reshape(-1)\n",
    "\n",
    "            logits = model(encoder_input, decoder_input).reshape(-1, VOCABULARY_SIZE)\n",
    "            loss = criterion(logits, decoder_target)\n",
    "\n",
    "            preds = logits.argmax(dim=1)\n",
    "            test_correct += (preds == decoder_target).sum().item()\n",
    "            test_tokens += decoder_target.size(0)\n",
    "\n",
    "            test_batch_losses.append(loss.item())\n",
    "\n",
    "    test_loss = np.mean(test_batch_losses)\n",
    "    test_acc = test_correct / test_tokens\n",
    "    test_losses.append(test_loss)\n",
    "    test_accs.append(test_acc)\n",
    "\n",
    "    return {\n",
    "        \"test_losses\": test_losses,\n",
    "        \"test_accs\": test_accs,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d863c67",
   "metadata": {},
   "source": [
    "### 2. Train Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59271964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Hyperparameters\n",
    "MAX_EPOCHS = 60\n",
    "LEARNING_RATE = 1e-3\n",
    "BETAS = (0.9, 0.98)\n",
    "EPSILON = 1e-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5c8f99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/60\n",
      "----------------------------------------\n",
      "Train Batch 1/12 | Loss: 4.5562\n",
      "Train Batch 11/12 | Loss: 2.6112\n",
      "Train Batch 12/12 | Loss: 2.4872\n",
      "[Epoch 1] Train Loss: 3.2559 | Train Acc: 0.3802\n",
      "[Epoch 1] Val Loss: 2.4023 | Val Acc: 0.5000\n",
      "----------------------------------------\n",
      "Validation loss improved — saving model (Loss: 2.4023)\n",
      "\n",
      "Epoch 2/60\n",
      "----------------------------------------\n",
      "Train Batch 1/12 | Loss: 2.5367\n",
      "Train Batch 11/12 | Loss: 1.9600\n",
      "Train Batch 12/12 | Loss: 1.9604\n",
      "[Epoch 2] Train Loss: 2.1991 | Train Acc: 0.5013\n",
      "[Epoch 2] Val Loss: 1.8479 | Val Acc: 0.5000\n",
      "----------------------------------------\n",
      "Validation loss improved — saving model (Loss: 1.8479)\n",
      "\n",
      "Epoch 3/60\n",
      "----------------------------------------\n",
      "Train Batch 1/12 | Loss: 1.9235\n",
      "Train Batch 11/12 | Loss: 1.4828\n",
      "Train Batch 12/12 | Loss: 1.4303\n",
      "[Epoch 3] Train Loss: 1.6638 | Train Acc: 0.5768\n",
      "[Epoch 3] Val Loss: 1.3714 | Val Acc: 0.6172\n",
      "----------------------------------------\n",
      "Validation loss improved — saving model (Loss: 1.3714)\n",
      "\n",
      "Epoch 4/60\n",
      "----------------------------------------\n",
      "Train Batch 1/12 | Loss: 1.4150\n",
      "Train Batch 11/12 | Loss: 1.1889\n",
      "Train Batch 12/12 | Loss: 1.1734\n",
      "[Epoch 4] Train Loss: 1.2948 | Train Acc: 0.6276\n",
      "[Epoch 4] Val Loss: 1.1170 | Val Acc: 0.6328\n",
      "----------------------------------------\n",
      "Validation loss improved — saving model (Loss: 1.1170)\n",
      "\n",
      "Epoch 5/60\n",
      "----------------------------------------\n",
      "Train Batch 1/12 | Loss: 1.1720\n",
      "Train Batch 11/12 | Loss: 1.0207\n",
      "Train Batch 12/12 | Loss: 0.9832\n",
      "[Epoch 5] Train Loss: 1.0763 | Train Acc: 0.6484\n",
      "[Epoch 5] Val Loss: 0.9733 | Val Acc: 0.6367\n",
      "----------------------------------------\n",
      "Validation loss improved — saving model (Loss: 0.9733)\n",
      "\n",
      "Epoch 6/60\n",
      "----------------------------------------\n",
      "Train Batch 1/12 | Loss: 0.9755\n",
      "Train Batch 11/12 | Loss: 0.9390\n",
      "Train Batch 12/12 | Loss: 0.9256\n",
      "[Epoch 6] Train Loss: 0.9473 | Train Acc: 0.6341\n",
      "[Epoch 6] Val Loss: 0.8799 | Val Acc: 0.6484\n",
      "----------------------------------------\n",
      "Validation loss improved — saving model (Loss: 0.8799)\n",
      "\n",
      "Epoch 7/60\n",
      "----------------------------------------\n",
      "Train Batch 1/12 | Loss: 0.9019\n",
      "Train Batch 11/12 | Loss: 0.8394\n",
      "Train Batch 12/12 | Loss: 0.8596\n",
      "[Epoch 7] Train Loss: 0.8769 | Train Acc: 0.6341\n",
      "[Epoch 7] Val Loss: 0.8284 | Val Acc: 0.6328\n",
      "----------------------------------------\n",
      "Validation loss improved — saving model (Loss: 0.8284)\n",
      "\n",
      "Epoch 8/60\n",
      "----------------------------------------\n",
      "Train Batch 1/12 | Loss: 0.8160\n",
      "Train Batch 11/12 | Loss: 0.8061\n",
      "Train Batch 12/12 | Loss: 0.7970\n",
      "[Epoch 8] Train Loss: 0.8238 | Train Acc: 0.6354\n",
      "[Epoch 8] Val Loss: 0.7906 | Val Acc: 0.6484\n",
      "----------------------------------------\n",
      "Validation loss improved — saving model (Loss: 0.7906)\n",
      "\n",
      "Epoch 9/60\n",
      "----------------------------------------\n",
      "Train Batch 1/12 | Loss: 0.8046\n",
      "Train Batch 11/12 | Loss: 0.7885\n",
      "Train Batch 12/12 | Loss: 0.7794\n",
      "[Epoch 9] Train Loss: 0.7892 | Train Acc: 0.6406\n",
      "[Epoch 9] Val Loss: 0.7688 | Val Acc: 0.6445\n",
      "----------------------------------------\n",
      "Validation loss improved — saving model (Loss: 0.7688)\n",
      "\n",
      "Epoch 10/60\n",
      "----------------------------------------\n",
      "Train Batch 1/12 | Loss: 0.7679\n",
      "Train Batch 11/12 | Loss: 0.7568\n",
      "Train Batch 12/12 | Loss: 0.7647\n",
      "[Epoch 10] Train Loss: 0.7691 | Train Acc: 0.6367\n",
      "[Epoch 10] Val Loss: 0.7537 | Val Acc: 0.6445\n",
      "----------------------------------------\n",
      "Validation loss improved — saving model (Loss: 0.7537)\n",
      "\n",
      "Epoch 11/60\n",
      "----------------------------------------\n",
      "Train Batch 1/12 | Loss: 0.7484\n",
      "Train Batch 11/12 | Loss: 0.7440\n",
      "Train Batch 12/12 | Loss: 0.7390\n",
      "[Epoch 11] Train Loss: 0.7538 | Train Acc: 0.6549\n",
      "[Epoch 11] Val Loss: 0.7414 | Val Acc: 0.6367\n",
      "----------------------------------------\n",
      "Validation loss improved — saving model (Loss: 0.7414)\n",
      "\n",
      "Epoch 12/60\n",
      "----------------------------------------\n",
      "Train Batch 1/12 | Loss: 0.7406\n",
      "Train Batch 11/12 | Loss: 0.7477\n",
      "Train Batch 12/12 | Loss: 0.7448\n",
      "[Epoch 12] Train Loss: 0.7377 | Train Acc: 0.6523\n",
      "[Epoch 12] Val Loss: 0.7310 | Val Acc: 0.6445\n",
      "----------------------------------------\n",
      "Validation loss improved — saving model (Loss: 0.7310)\n",
      "\n",
      "Epoch 13/60\n",
      "----------------------------------------\n",
      "Train Batch 1/12 | Loss: 0.7328\n",
      "Train Batch 11/12 | Loss: 0.7277\n",
      "Train Batch 12/12 | Loss: 0.7247\n",
      "[Epoch 13] Train Loss: 0.7332 | Train Acc: 0.6393\n",
      "[Epoch 13] Val Loss: 0.7266 | Val Acc: 0.6406\n",
      "----------------------------------------\n",
      "Validation loss improved — saving model (Loss: 0.7266)\n",
      "\n",
      "Epoch 14/60\n",
      "----------------------------------------\n",
      "Train Batch 1/12 | Loss: 0.7436\n",
      "Train Batch 11/12 | Loss: 0.7151\n",
      "Train Batch 12/12 | Loss: 0.6893\n",
      "[Epoch 14] Train Loss: 0.7202 | Train Acc: 0.6615\n",
      "[Epoch 14] Val Loss: 0.7218 | Val Acc: 0.6484\n",
      "----------------------------------------\n",
      "Validation loss improved — saving model (Loss: 0.7218)\n",
      "\n",
      "Epoch 15/60\n",
      "----------------------------------------\n",
      "Train Batch 1/12 | Loss: 0.7166\n",
      "Train Batch 11/12 | Loss: 0.6934\n",
      "Train Batch 12/12 | Loss: 0.7148\n",
      "[Epoch 15] Train Loss: 0.7098 | Train Acc: 0.6667\n",
      "[Epoch 15] Val Loss: 0.7237 | Val Acc: 0.6445\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 16/60\n",
      "----------------------------------------\n",
      "Train Batch 1/12 | Loss: 0.6941\n",
      "Train Batch 11/12 | Loss: 0.7215\n",
      "Train Batch 12/12 | Loss: 0.6686\n",
      "[Epoch 16] Train Loss: 0.6960 | Train Acc: 0.6797\n",
      "[Epoch 16] Val Loss: 0.7236 | Val Acc: 0.6328\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 17/60\n",
      "----------------------------------------\n",
      "Train Batch 1/12 | Loss: 0.6809\n",
      "Train Batch 11/12 | Loss: 0.6330\n",
      "Train Batch 12/12 | Loss: 0.6940\n",
      "[Epoch 17] Train Loss: 0.6870 | Train Acc: 0.6810\n",
      "[Epoch 17] Val Loss: 0.7323 | Val Acc: 0.6523\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 18/60\n",
      "----------------------------------------\n",
      "Train Batch 1/12 | Loss: 0.6388\n",
      "Train Batch 11/12 | Loss: 0.6975\n",
      "Train Batch 12/12 | Loss: 0.6801\n",
      "[Epoch 18] Train Loss: 0.6676 | Train Acc: 0.6992\n",
      "[Epoch 18] Val Loss: 0.7550 | Val Acc: 0.6133\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 19/60\n",
      "----------------------------------------\n",
      "Train Batch 1/12 | Loss: 0.7132\n",
      "Train Batch 11/12 | Loss: 0.7008\n",
      "Train Batch 12/12 | Loss: 0.6227\n",
      "[Epoch 19] Train Loss: 0.6412 | Train Acc: 0.7279\n",
      "[Epoch 19] Val Loss: 0.7511 | Val Acc: 0.6836\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 20/60\n",
      "----------------------------------------\n",
      "Train Batch 1/12 | Loss: 0.6439\n",
      "Train Batch 11/12 | Loss: 0.6433\n",
      "Train Batch 12/12 | Loss: 0.5858\n",
      "[Epoch 20] Train Loss: 0.6260 | Train Acc: 0.7318\n",
      "[Epoch 20] Val Loss: 0.7709 | Val Acc: 0.6367\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 21/60\n",
      "----------------------------------------\n",
      "Train Batch 1/12 | Loss: 0.6069\n",
      "Train Batch 11/12 | Loss: 0.5847\n",
      "Train Batch 12/12 | Loss: 0.5898\n",
      "[Epoch 21] Train Loss: 0.6054 | Train Acc: 0.7396\n",
      "[Epoch 21] Val Loss: 0.7988 | Val Acc: 0.6289\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 22/60\n",
      "----------------------------------------\n",
      "Train Batch 1/12 | Loss: 0.4972\n",
      "Train Batch 11/12 | Loss: 0.6162\n",
      "Train Batch 12/12 | Loss: 0.5701\n",
      "[Epoch 22] Train Loss: 0.5876 | Train Acc: 0.7552\n",
      "[Epoch 22] Val Loss: 0.8260 | Val Acc: 0.6172\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 23/60\n",
      "----------------------------------------\n",
      "Train Batch 1/12 | Loss: 0.5389\n",
      "Train Batch 11/12 | Loss: 0.6242\n",
      "Train Batch 12/12 | Loss: 0.6455\n",
      "[Epoch 23] Train Loss: 0.5686 | Train Acc: 0.7344\n",
      "[Epoch 23] Val Loss: 0.8171 | Val Acc: 0.6523\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 24/60\n",
      "----------------------------------------\n",
      "Train Batch 1/12 | Loss: 0.5043\n",
      "Train Batch 11/12 | Loss: 0.5277\n",
      "Train Batch 12/12 | Loss: 0.5916\n",
      "[Epoch 24] Train Loss: 0.5377 | Train Acc: 0.7852\n",
      "[Epoch 24] Val Loss: 0.8686 | Val Acc: 0.6133\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 25/60\n",
      "----------------------------------------\n",
      "Train Batch 1/12 | Loss: 0.4918\n",
      "Train Batch 11/12 | Loss: 0.5472\n",
      "Train Batch 12/12 | Loss: 0.4634\n",
      "[Epoch 25] Train Loss: 0.5153 | Train Acc: 0.7917\n",
      "[Epoch 25] Val Loss: 0.9271 | Val Acc: 0.6133\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 26/60\n",
      "----------------------------------------\n",
      "Train Batch 1/12 | Loss: 0.5025\n",
      "Train Batch 11/12 | Loss: 0.5513\n",
      "Train Batch 12/12 | Loss: 0.5346\n",
      "[Epoch 26] Train Loss: 0.5363 | Train Acc: 0.7695\n",
      "[Epoch 26] Val Loss: 0.8780 | Val Acc: 0.6445\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 27/60\n",
      "----------------------------------------\n",
      "Train Batch 1/12 | Loss: 0.5381\n",
      "Train Batch 11/12 | Loss: 0.5085\n",
      "Train Batch 12/12 | Loss: 0.6447\n",
      "[Epoch 27] Train Loss: 0.5034 | Train Acc: 0.7969\n",
      "[Epoch 27] Val Loss: 0.9030 | Val Acc: 0.6367\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 28/60\n",
      "----------------------------------------\n",
      "Train Batch 1/12 | Loss: 0.4249\n",
      "Train Batch 11/12 | Loss: 0.5090\n",
      "Train Batch 12/12 | Loss: 0.5078\n",
      "[Epoch 28] Train Loss: 0.4762 | Train Acc: 0.8242\n",
      "[Epoch 28] Val Loss: 0.9363 | Val Acc: 0.6523\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 29/60\n",
      "----------------------------------------\n",
      "Train Batch 1/12 | Loss: 0.4625\n",
      "Train Batch 11/12 | Loss: 0.5484\n",
      "Train Batch 12/12 | Loss: 0.3695\n",
      "[Epoch 29] Train Loss: 0.4563 | Train Acc: 0.8177\n",
      "[Epoch 29] Val Loss: 0.9307 | Val Acc: 0.6445\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 30/60\n",
      "----------------------------------------\n",
      "Train Batch 1/12 | Loss: 0.4058\n",
      "Train Batch 11/12 | Loss: 0.4623\n",
      "Train Batch 12/12 | Loss: 0.3257\n",
      "[Epoch 30] Train Loss: 0.4401 | Train Acc: 0.8333\n",
      "[Epoch 30] Val Loss: 0.9509 | Val Acc: 0.6289\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 31/60\n",
      "----------------------------------------\n",
      "Train Batch 1/12 | Loss: 0.4187\n",
      "Train Batch 11/12 | Loss: 0.4342\n",
      "Train Batch 12/12 | Loss: 0.4106\n",
      "[Epoch 31] Train Loss: 0.4028 | Train Acc: 0.8385\n",
      "[Epoch 31] Val Loss: 1.0137 | Val Acc: 0.6367\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 32/60\n",
      "----------------------------------------\n",
      "Train Batch 1/12 | Loss: 0.3489\n",
      "Train Batch 11/12 | Loss: 0.3514\n",
      "Train Batch 12/12 | Loss: 0.4683\n",
      "[Epoch 32] Train Loss: 0.3835 | Train Acc: 0.8398\n",
      "[Epoch 32] Val Loss: 1.0669 | Val Acc: 0.6250\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 33/60\n",
      "----------------------------------------\n",
      "Train Batch 1/12 | Loss: 0.3978\n",
      "Train Batch 11/12 | Loss: 0.3617\n",
      "Train Batch 12/12 | Loss: 0.4485\n",
      "[Epoch 33] Train Loss: 0.3879 | Train Acc: 0.8359\n",
      "[Epoch 33] Val Loss: 1.0413 | Val Acc: 0.6367\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 34/60\n",
      "----------------------------------------\n",
      "Train Batch 1/12 | Loss: 0.3905\n",
      "Train Batch 11/12 | Loss: 0.2650\n",
      "Train Batch 12/12 | Loss: 0.4365\n",
      "[Epoch 34] Train Loss: 0.3488 | Train Acc: 0.8633\n",
      "[Epoch 34] Val Loss: 1.0522 | Val Acc: 0.6523\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 35/60\n",
      "----------------------------------------\n",
      "Train Batch 1/12 | Loss: 0.3141\n",
      "Train Batch 11/12 | Loss: 0.3469\n",
      "Train Batch 12/12 | Loss: 0.3757\n",
      "[Epoch 35] Train Loss: 0.3486 | Train Acc: 0.8672\n",
      "[Epoch 35] Val Loss: 1.1152 | Val Acc: 0.6211\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 36/60\n",
      "----------------------------------------\n",
      "Train Batch 1/12 | Loss: 0.3842\n",
      "Train Batch 11/12 | Loss: 0.3417\n",
      "Train Batch 12/12 | Loss: 0.5212\n",
      "[Epoch 36] Train Loss: 0.3471 | Train Acc: 0.8711\n",
      "[Epoch 36] Val Loss: 1.1207 | Val Acc: 0.6406\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 37/60\n",
      "----------------------------------------\n",
      "Train Batch 1/12 | Loss: 0.2293\n",
      "Train Batch 11/12 | Loss: 0.2938\n",
      "Train Batch 12/12 | Loss: 0.4989\n",
      "[Epoch 37] Train Loss: 0.3304 | Train Acc: 0.8646\n",
      "[Epoch 37] Val Loss: 1.1704 | Val Acc: 0.6445\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 38/60\n",
      "----------------------------------------\n",
      "Train Batch 1/12 | Loss: 0.2179\n",
      "Train Batch 11/12 | Loss: 0.3939\n",
      "Train Batch 12/12 | Loss: 0.2499\n",
      "[Epoch 38] Train Loss: 0.3435 | Train Acc: 0.8724\n",
      "[Epoch 38] Val Loss: 1.1392 | Val Acc: 0.6250\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 39/60\n",
      "----------------------------------------\n",
      "Train Batch 1/12 | Loss: 0.3294\n",
      "Train Batch 11/12 | Loss: 0.2737\n",
      "Train Batch 12/12 | Loss: 0.2797\n",
      "[Epoch 39] Train Loss: 0.2899 | Train Acc: 0.8880\n",
      "[Epoch 39] Val Loss: 1.1243 | Val Acc: 0.6406\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 40/60\n",
      "----------------------------------------\n",
      "Train Batch 1/12 | Loss: 0.2346\n",
      "Train Batch 11/12 | Loss: 0.3033\n",
      "Train Batch 12/12 | Loss: 0.1776\n",
      "[Epoch 40] Train Loss: 0.2651 | Train Acc: 0.9049\n",
      "[Epoch 40] Val Loss: 1.2092 | Val Acc: 0.6328\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 41/60\n",
      "----------------------------------------\n",
      "Train Batch 1/12 | Loss: 0.2633\n",
      "Train Batch 11/12 | Loss: 0.3481\n",
      "Train Batch 12/12 | Loss: 0.2214\n",
      "[Epoch 41] Train Loss: 0.2584 | Train Acc: 0.8984\n",
      "[Epoch 41] Val Loss: 1.2983 | Val Acc: 0.6133\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 42/60\n",
      "----------------------------------------\n",
      "Train Batch 1/12 | Loss: 0.2742\n",
      "Train Batch 11/12 | Loss: 0.3647\n",
      "Train Batch 12/12 | Loss: 0.1840\n",
      "[Epoch 42] Train Loss: 0.2315 | Train Acc: 0.9141\n",
      "[Epoch 42] Val Loss: 1.2957 | Val Acc: 0.6406\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 43/60\n",
      "----------------------------------------\n",
      "Train Batch 1/12 | Loss: 0.1239\n",
      "Train Batch 11/12 | Loss: 0.2702\n",
      "Train Batch 12/12 | Loss: 0.1376\n",
      "[Epoch 43] Train Loss: 0.2186 | Train Acc: 0.9193\n",
      "[Epoch 43] Val Loss: 1.3287 | Val Acc: 0.6406\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 44/60\n",
      "----------------------------------------\n",
      "Train Batch 1/12 | Loss: 0.1104\n",
      "Train Batch 11/12 | Loss: 0.2923\n",
      "Train Batch 12/12 | Loss: 0.2730\n",
      "[Epoch 44] Train Loss: 0.2168 | Train Acc: 0.9232\n",
      "[Epoch 44] Val Loss: 1.3993 | Val Acc: 0.6328\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 45/60\n",
      "----------------------------------------\n",
      "Train Batch 1/12 | Loss: 0.1360\n",
      "Train Batch 11/12 | Loss: 0.2192\n",
      "Train Batch 12/12 | Loss: 0.1991\n",
      "[Epoch 45] Train Loss: 0.2048 | Train Acc: 0.9284\n",
      "[Epoch 45] Val Loss: 1.4260 | Val Acc: 0.6406\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 46/60\n",
      "----------------------------------------\n",
      "Train Batch 1/12 | Loss: 0.1441\n",
      "Train Batch 11/12 | Loss: 0.2147\n",
      "Train Batch 12/12 | Loss: 0.1566\n",
      "[Epoch 46] Train Loss: 0.1721 | Train Acc: 0.9401\n",
      "[Epoch 46] Val Loss: 1.4673 | Val Acc: 0.6328\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 47/60\n",
      "----------------------------------------\n",
      "Train Batch 1/12 | Loss: 0.2646\n",
      "Train Batch 11/12 | Loss: 0.1288\n",
      "Train Batch 12/12 | Loss: 0.1739\n",
      "[Epoch 47] Train Loss: 0.1840 | Train Acc: 0.9323\n",
      "[Epoch 47] Val Loss: 1.4588 | Val Acc: 0.6523\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 48/60\n",
      "----------------------------------------\n",
      "Train Batch 1/12 | Loss: 0.1209\n",
      "Train Batch 11/12 | Loss: 0.1616\n",
      "Train Batch 12/12 | Loss: 0.2876\n",
      "[Epoch 48] Train Loss: 0.1701 | Train Acc: 0.9375\n",
      "[Epoch 48] Val Loss: 1.4753 | Val Acc: 0.6367\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 49/60\n",
      "----------------------------------------\n",
      "Train Batch 1/12 | Loss: 0.0835\n",
      "Train Batch 11/12 | Loss: 0.1470\n",
      "Train Batch 12/12 | Loss: 0.1815\n",
      "[Epoch 49] Train Loss: 0.1535 | Train Acc: 0.9505\n",
      "[Epoch 49] Val Loss: 1.5032 | Val Acc: 0.6289\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 50/60\n",
      "----------------------------------------\n",
      "Train Batch 1/12 | Loss: 0.1424\n",
      "Train Batch 11/12 | Loss: 0.2147\n",
      "Train Batch 12/12 | Loss: 0.1775\n",
      "[Epoch 50] Train Loss: 0.1675 | Train Acc: 0.9271\n",
      "[Epoch 50] Val Loss: 1.5300 | Val Acc: 0.6289\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 51/60\n",
      "----------------------------------------\n",
      "Train Batch 1/12 | Loss: 0.1482\n",
      "Train Batch 11/12 | Loss: 0.2144\n",
      "Train Batch 12/12 | Loss: 0.1375\n",
      "[Epoch 51] Train Loss: 0.1467 | Train Acc: 0.9531\n",
      "[Epoch 51] Val Loss: 1.5675 | Val Acc: 0.6484\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 52/60\n",
      "----------------------------------------\n",
      "Train Batch 1/12 | Loss: 0.2186\n",
      "Train Batch 11/12 | Loss: 0.2094\n",
      "Train Batch 12/12 | Loss: 0.1375\n",
      "[Epoch 52] Train Loss: 0.1603 | Train Acc: 0.9388\n",
      "[Epoch 52] Val Loss: 1.5122 | Val Acc: 0.6289\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 53/60\n",
      "----------------------------------------\n",
      "Train Batch 1/12 | Loss: 0.1709\n",
      "Train Batch 11/12 | Loss: 0.1176\n",
      "Train Batch 12/12 | Loss: 0.1362\n",
      "[Epoch 53] Train Loss: 0.1198 | Train Acc: 0.9609\n",
      "[Epoch 53] Val Loss: 1.5792 | Val Acc: 0.6445\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 54/60\n",
      "----------------------------------------\n",
      "Train Batch 1/12 | Loss: 0.1145\n",
      "Train Batch 11/12 | Loss: 0.2058\n",
      "Train Batch 12/12 | Loss: 0.1962\n",
      "[Epoch 54] Train Loss: 0.1540 | Train Acc: 0.9440\n",
      "[Epoch 54] Val Loss: 1.6156 | Val Acc: 0.6445\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 55/60\n",
      "----------------------------------------\n",
      "Train Batch 1/12 | Loss: 0.1555\n",
      "Train Batch 11/12 | Loss: 0.1282\n",
      "Train Batch 12/12 | Loss: 0.1241\n",
      "[Epoch 55] Train Loss: 0.1100 | Train Acc: 0.9596\n",
      "[Epoch 55] Val Loss: 1.6204 | Val Acc: 0.6484\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 56/60\n",
      "----------------------------------------\n",
      "Train Batch 1/12 | Loss: 0.0829\n",
      "Train Batch 11/12 | Loss: 0.1524\n",
      "Train Batch 12/12 | Loss: 0.1134\n",
      "[Epoch 56] Train Loss: 0.1265 | Train Acc: 0.9518\n",
      "[Epoch 56] Val Loss: 1.5752 | Val Acc: 0.6484\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 57/60\n",
      "----------------------------------------\n",
      "Train Batch 1/12 | Loss: 0.0806\n",
      "Train Batch 11/12 | Loss: 0.1781\n",
      "Train Batch 12/12 | Loss: 0.1087\n",
      "[Epoch 57] Train Loss: 0.1234 | Train Acc: 0.9544\n",
      "[Epoch 57] Val Loss: 1.5833 | Val Acc: 0.6602\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 58/60\n",
      "----------------------------------------\n",
      "Train Batch 1/12 | Loss: 0.0931\n",
      "Train Batch 11/12 | Loss: 0.1936\n",
      "Train Batch 12/12 | Loss: 0.0199\n",
      "[Epoch 58] Train Loss: 0.1105 | Train Acc: 0.9596\n",
      "[Epoch 58] Val Loss: 1.6436 | Val Acc: 0.6484\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 59/60\n",
      "----------------------------------------\n",
      "Train Batch 1/12 | Loss: 0.1088\n",
      "Train Batch 11/12 | Loss: 0.1436\n",
      "Train Batch 12/12 | Loss: 0.0722\n",
      "[Epoch 59] Train Loss: 0.1089 | Train Acc: 0.9583\n",
      "[Epoch 59] Val Loss: 1.6501 | Val Acc: 0.6523\n",
      "----------------------------------------\n",
      "\n",
      "Epoch 60/60\n",
      "----------------------------------------\n",
      "Train Batch 1/12 | Loss: 0.1127\n",
      "Train Batch 11/12 | Loss: 0.0637\n",
      "Train Batch 12/12 | Loss: 0.0716\n",
      "[Epoch 60] Train Loss: 0.1191 | Train Acc: 0.9544\n",
      "[Epoch 60] Val Loss: 1.7414 | Val Acc: 0.6367\n",
      "----------------------------------------\n",
      "\n",
      "Training complete\n"
     ]
    }
   ],
   "source": [
    "criterion =  nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(transformer.parameters(), lr=LEARNING_RATE, betas=BETAS, eps=EPSILON)\n",
    "\n",
    "results = train_model(\n",
    "    train_dataset_loader, validation_dataset_loader, transformer, criterion, \n",
    "    optimizer, max_epochs=MAX_EPOCHS, device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917a1fa4",
   "metadata": {},
   "source": [
    "### 3. Test Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d144441",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = test_model(test_dataset_loader, transformer, criterion, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28dac7b",
   "metadata": {},
   "source": [
    "## Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b590910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inference(model: Transformer, source_sequence, start_tokens):\n",
    "    model.eval()\n",
    "    generated = start_tokens\n",
    "    \n",
    "    with T.no_grad():\n",
    "        logits = model(source_sequence, generated)\n",
    "    \n",
    "    next_token = T.argmax(logits[:, -1, :], dim=-1, keepdim=True)\n",
    "\n",
    "    generated = T.cat([generated, next_token], dim=1)\n",
    "\n",
    "    return generated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f458678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Actual Trials\n",
      "[array(['green', 'cross', '4'], dtype='<U6'), array(['blue', 'cross', '2'], dtype='<U6'), array(['red', 'star', '4'], dtype='<U6'), array(['yellow', 'circle', '4'], dtype='<U6'), array(['yellow', 'cross', '2'], dtype='<U6'), 'SEP', 'C4', 'EOS', array(['red', 'square', '2'], dtype='<U6'), 'SEP', 'C3']\n",
      "[array(['green', 'square', '1'], dtype='<U6'), array(['red', 'star', '2'], dtype='<U6'), array(['blue', 'star', '4'], dtype='<U6'), array(['yellow', 'star', '4'], dtype='<U6'), array(['blue', 'cross', '3'], dtype='<U6'), 'SEP', 'C3', 'EOS', array(['green', 'cross', '3'], dtype='<U6'), 'SEP', 'C1']\n",
      "[array(['red', 'star', '4'], dtype='<U6'), array(['blue', 'star', '3'], dtype='<U6'), array(['yellow', 'circle', '4'], dtype='<U6'), array(['green', 'circle', '1'], dtype='<U6'), array(['yellow', 'circle', '2'], dtype='<U6'), 'SEP', 'C3', 'EOS', array(['yellow', 'cross', '1'], dtype='<U6'), 'SEP', 'C3']\n",
      "[array(['blue', 'square', '2'], dtype='<U6'), array(['green', 'star', '1'], dtype='<U6'), array(['red', 'cross', '2'], dtype='<U6'), array(['yellow', 'cross', '1'], dtype='<U6'), array(['red', 'cross', '3'], dtype='<U6'), 'SEP', 'C3', 'EOS', array(['green', 'star', '3'], dtype='<U6'), 'SEP', 'C2']\n",
      "[array(['red', 'circle', '4'], dtype='<U6'), array(['yellow', 'cross', '3'], dtype='<U6'), array(['green', 'square', '3'], dtype='<U6'), array(['blue', 'circle', '1'], dtype='<U6'), array(['yellow', 'circle', '4'], dtype='<U6'), 'SEP', 'C2', 'EOS', array(['green', 'square', '2'], dtype='<U6'), 'SEP', 'C3']\n",
      "Feature for Classification:  2 \n",
      "\n",
      "# Predicted Trials\n",
      "[array(['yellow', 'square', '3'], dtype='<U6'), array(['green', 'cross', '2'], dtype='<U6'), array(['blue', 'cross', '1'], dtype='<U6'), array(['red', 'square', '1'], dtype='<U6'), array(['yellow', 'star', '4'], dtype='<U6'), 'SEP', 'C1', 'EOS', array(['red', 'cross', '3'], dtype='<U6'), 'SEP', 'C4']\n",
      "[array(['blue', 'square', '2'], dtype='<U6'), array(['green', 'star', '4'], dtype='<U6'), array(['red', 'cross', '4'], dtype='<U6'), array(['yellow', 'square', '2'], dtype='<U6'), array(['blue', 'square', '1'], dtype='<U6'), 'SEP', 'C1', 'EOS', array(['green', 'circle', '3'], dtype='<U6'), 'SEP', 'C2']\n",
      "[array(['red', 'square', '1'], dtype='<U6'), array(['green', 'star', '4'], dtype='<U6'), array(['blue', 'circle', '2'], dtype='<U6'), array(['yellow', 'star', '3'], dtype='<U6'), array(['blue', 'square', '3'], dtype='<U6'), 'SEP', 'C3', 'EOS', array(['yellow', 'circle', '3'], dtype='<U6'), 'SEP', 'C4']\n",
      "[array(['blue', 'cross', '1'], dtype='<U6'), array(['red', 'square', '3'], dtype='<U6'), array(['green', 'circle', '4'], dtype='<U6'), array(['yellow', 'cross', '3'], dtype='<U6'), array(['blue', 'cross', '2'], dtype='<U6'), 'SEP', 'C1', 'EOS', array(['green', 'circle', '2'], dtype='<U6'), 'SEP', 'C3']\n",
      "[array(['green', 'cross', '4'], dtype='<U6'), array(['yellow', 'cross', '4'], dtype='<U6'), array(['blue', 'circle', '2'], dtype='<U6'), array(['red', 'cross', '1'], dtype='<U6'), array(['red', 'circle', '3'], dtype='<U6'), 'SEP', 'C4', 'EOS', array(['yellow', 'circle', '3'], dtype='<U6'), 'SEP', 'C2']\n",
      "Feature for Classification:  2 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "x, target = train_dataset[:5]\n",
    "prediction = model_inference(transformer, x, target[:, : -1])\n",
    "\n",
    "print(\"# Actual Trials\")\n",
    "test_batch = [np.asarray(item.cpu()) for item in test_dataset[:5]]\n",
    "output = wcst.visualise_batch(test_batch)\n",
    "\n",
    "print(\"# Predicted Trials\")\n",
    "prediction_batch = [np.asarray(item.cpu()) for item in [x, prediction]]\n",
    "output = wcst.visualise_batch(prediction_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6267503e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "npl_project_2025 (3.12.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
